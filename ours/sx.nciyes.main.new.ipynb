{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23142276",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color=red>Attention!</font> You're in <font color=red>BETABIND</font>\n",
    "# sx.nciyes.main.<font color=red>new</font>.ipynb\n",
    "\n",
    "Notebook for training and inference with updated NCI data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a9fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07:51:34'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec45038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration and Initilization\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf3ac7",
   "metadata": {},
   "source": [
    "1st run:\n",
    "- set \"cfg_save_files = True\" and \"cfg_use_saved_files = False\" to process data and saved them.\n",
    "\n",
    "otherwise:\n",
    "- set \"cfg_save_files = False\" and \"cfg_use_saved_files = True\" to skip processing and load saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c21caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to change settings below.\n",
    "cfg_use_saved_files = True # If you have already generated all the data required.\n",
    "cfg_save_files = False # If you want to save generated data.\n",
    "cfg_frag = False\n",
    "cfg_checkdata = True\n",
    "cfg_split_strategy = (0.5,0.25,0.25)\n",
    "mode_ls = [\"tankbind\", \"nciyes\", \"frag\"]\n",
    "\n",
    "cfg_mode = \"nciyes\" # \"frag\n",
    "cfg_data_version = \"v9.8-only2\" #\"v9.1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cfg_timesplit = True # If timesplit is applicated for splitting the dataset.\n",
    "\n",
    "cfg_custom_dir_name = \"Demo\" # Prefix in the name of the output folder.\n",
    "cfg_train = True # If you want to train the model.\n",
    "\n",
    "cfg_running_mode = [\"train\", \"inference\"]\n",
    "cfg_running_mode = \"inference\"\n",
    "\n",
    "cfg_distinguish_by_timestamp = True # If true, a timestamp is added to your output dir name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476eabd",
   "metadata": {},
   "source": [
    "### Other Configuration and Initilization\n",
    "Under normal condition, you needn't modify this chapiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6443014-d883-4654-8db5-17a42526f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under normal condition, you should not modify the codes below.\n",
    "input_path = f\"./Inputs/{cfg_mode}/{cfg_data_version}/\"\n",
    "output_path = f\"./Outputs/{cfg_mode}/\"\n",
    "save_files_path = f\"./Inputs/Savedfiles/{cfg_mode}.{cfg_data_version}/\"\n",
    "p2rank_save_path = f\"./Inputs/Savedfiles/p2rank/\"\n",
    "p2rank_path = \"../p2rank_2.3/prank\"\n",
    "ds_path = \"../../../\" # Related path used for ds files.\n",
    "save_model_path = f\"./Inputs/Savedfiles/{cfg_mode}.model/\" #TODO: write your paths here or just keep this.\n",
    "pdb_df_fname = f\"Data.{cfg_data_version}.PDBs.csv\"\n",
    "ligand_df_fname = f\"Data.{cfg_data_version}.Ligands.csv\"\n",
    "pdb_df_fpath = f\"{input_path}{pdb_df_fname}\"\n",
    "ligand_df_fpath = f\"{input_path}{ligand_df_fname}\"\n",
    "datainfo = f\"{input_path}Datainfo.txt\"\n",
    "p2rank = f\"bash {p2rank_path}\"\n",
    "\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_fname = f\"Data.{cfg_data_version}.NCIs.csv\"\n",
    "    nci_df_fpath = f\"{input_path}{nci_fname}\"\n",
    "    \n",
    "# Under normal condition, you should not modify the codes below.\n",
    "cfg_jupyter = True # If you're using jupyter notebook. # old\n",
    "cfg_right_pocket_by_minor_distance = True # If the right pocket is chosen by calculating distance between ligand center and pocket center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e6c48-ae07-4a47-b907-e30fb91e79ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9110acb0-dd78-451e-811b-4a7a134ac29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Inputs/nciyes/v9.8-only2/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bd5d4c-1b4f-4264-b511-12f5e12ea65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to \u001b[1;34m./Outputs/nciyes/Demo_22-09-09_0751/\u001b[0m.\n",
      "\n",
      "Data version: v9.8-only2\n",
      "PDBs: 30, Ligands: 30, NCIs: 76583\n",
      "\n",
      "\n",
      "Loaded \u001b[1;31mpdb_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v9.8-only2/Data.v9.8-only2.PDBs.csv\u001b[0m with length 30\n",
      "Loaded \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v9.8-only2/Data.v9.8-only2.Ligands.csv\u001b[0m with length 30\n",
      "Loaded \u001b[1;31mnci_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v9.8-only2/Data.v9.8-only2.NCIs.csv\u001b[0m with length 76583\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "sys.path.insert(0, \"../tankbind/\")\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    pass\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_feature_utils import sx_get_protein_feature, get_clean_res_list\n",
    "    from feature_utils import get_canonical_smiles\n",
    "elif cfg_mode == \"frag\":\n",
    "    from feature_utils import get_clean_res_list, get_protein_feature_qsar\n",
    "    from feature_utils import get_canonical_smiles\n",
    "\n",
    "if cfg_jupyter:\n",
    "    from tqdm.notebook import tqdm\n",
    "    R, B, S = \"\\033[1;31m\", \"\\033[1;34m\", \"\\033[0m\" \n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    R, B, S = \"\", \"\", \"\" \n",
    "\n",
    "_dirname = \"\"\n",
    "_dirname = (_dirname + cfg_custom_dir_name) if cfg_custom_dir_name else (time.strftime(\"%y-%m-%d_%H%M\"))\n",
    "_dirname = (_dirname + \"_\" + time.strftime(\"%y-%m-%d_%H%M\")) if (cfg_custom_dir_name and cfg_distinguish_by_timestamp) else _dirname\n",
    "main_path = f\"{output_path}{_dirname}/\"\n",
    "log_fpath = f\"{main_path}log.txt\"\n",
    "if os.path.exists(log_fpath):\n",
    "    os.system(f\"rm -r {log_fpath}\")\n",
    "if os.path.exists(main_path):\n",
    "    os.system(f\"rm -r {main_path}\")\n",
    "for _path in [save_files_path, save_model_path, main_path]:\n",
    "    os.system(f\"mkdir -p {_path}\")\n",
    "\n",
    "def qrint(target, jupyter= cfg_jupyter, log=log_fpath, R=R, B=B, S=S, r=True):\n",
    "    if r:\n",
    "        print(target)\n",
    "    if jupyter:\n",
    "        target = target.replace(R,\"\").replace(B,\"\").replace(S,\"\")\n",
    "    with open(log_fpath, \"a\") as f:\n",
    "        if target != \"\\n\":\n",
    "            f.write(time.strftime(\"[%m-%d %H:%M:%S] \")+target.replace(\"\\n\", \"                 \\n\")+\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "if cfg_mode == \"nciyes\":\n",
    "    def get_full_id_old(full_id_ls: list, resname):\n",
    "        chain_id = full_id_ls[2]\n",
    "        res_id = full_id_ls[3][1]\n",
    "        return chain_id + \"_\" + str(res_id)+\"_\"+resname\n",
    "\n",
    "def saveconfig(cfg_use_saved_files, cfg_save_files, B=B, R=R, S=S):\n",
    "    qrint(f\"{R}cfg_use_saved_files{S}={B}{cfg_use_saved_files}{S}, {R}cfg_save_files{S}={B}{cfg_save_files}{S}.\")\n",
    "    if cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "    elif not cfg_use_saved_files and cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes and {B}save{S} results.\")\n",
    "    elif not cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes but results {B}won't be saved{S}.\")\n",
    "    else:\n",
    "        return(f\"{R}CONFIG WARNING{R}: {B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "        \n",
    "qrint(f\"Results will be saved to {B}{main_path}{S}.\\n\")\n",
    "\n",
    "if os.path.exists(datainfo):\n",
    "    with open(datainfo,\"r\") as f:\n",
    "        _ls = [k.replace(\"\\n\",\"\") for k in f.readlines()]\n",
    "        for _l in _ls:\n",
    "            qrint(_l)\n",
    "        qrint(\"\\n\")\n",
    "    \n",
    "pdb_df = pd.read_csv(pdb_df_fpath, index_col=0)\n",
    "ligand_df = pd.read_csv(ligand_df_fpath, index_col=0)\n",
    "pdb_code_list = list(pdb_df[\"pdb_code\"])\n",
    "pdb_fpath_list = list(pdb_df[\"pdb_fpath\"])\n",
    "qrint(f\"Loaded {R}pdb_df{S} from {B}{pdb_df_fpath}{S} with length {len(pdb_df)}\")\n",
    "qrint(f\"Loaded {R}ligand_df{S} from {B}{ligand_df_fpath}{S} with length {len(ligand_df)}\")\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_df = pd.read_csv(nci_df_fpath, index_col=0)\n",
    "    qrint(f\"Loaded {R}nci_df{S} from {B}{nci_df_fpath}{S} with length {len(nci_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1fbe0d-620c-4493-80ae-f1858abb101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f\"{main_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6e127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62774b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get protein features: <font color=\"red\">protein_dict</font> (& <font color=\"red\">protein_res_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e6c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/protein_dicts/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd37a5c10754859960d110aff444ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m.\n",
      "CPU times: user 55.2 ms, sys: 34.9 ms, total: 90.2 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}protein_dict{S} and {R}protein_res_id_dict{S}\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "protein_dict = {}\n",
    "protein_res_id_dict = {} if cfg_mode==\"nciyes\" else None\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Processing {R}protein_dict{S}\")\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protein_dicts = [{} for n in range(11)] # 0,1,2,3,4,5,6,7,8,9,x\n",
    "    #protein_res_id_dict = [{} for n in range(11)]  #0,1,2,3,4,5,6,7,8,9,x\n",
    "    \n",
    "    for i, (_pname, _fpath) in tqdm(enumerate(zip(pdb_code_list, pdb_fpath_list)), total=len(pdb_fpath_list)):\n",
    "        s = parser.get_structure(_pname, _fpath)\n",
    "        res_list = list(s.get_residues())\n",
    "        clean_res_list = get_clean_res_list(res_list, ensure_ca_exist=True)\n",
    "        clean_res_full_id_list = [get_full_id_old(x.full_id, x.get_resname()) for x in clean_res_list] if (cfg_mode == \"nciyes\") else None\n",
    "\n",
    "        if (cfg_mode == \"tankbind\"):\n",
    "            _protein_dict = get_protein_feature(clean_res_list)\n",
    "        elif (cfg_mode == \"nciyes\"):\n",
    "            _protein_dict, _protein_res_id_dict = sx_get_protein_feature(clean_res_list, clean_res_full_id_list)\n",
    "        elif (cfg_mode == \"frag\"):\n",
    "            #TODO:write your function here\n",
    "            try:\n",
    "                _protein_dict = get_protein_feature_qsar(clean_res_list)\n",
    "            except Exception as e:\n",
    "                print(_pname+\" ERROR_dim : \"+str(e))\n",
    "            \n",
    "        ind = _pname[0]\n",
    "        ind = int(ind) if str.isdigit(ind) else 10\n",
    "\n",
    "        protein_dicts[ind][_pname] = _protein_dict\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            protein_res_id_dict[_pname] = _protein_res_id_dict\n",
    "            \n",
    "    for _d in protein_dicts:\n",
    "        protein_dict.update(_d)\n",
    "\n",
    "    \n",
    "    \n",
    "    if cfg_save_files:\n",
    "        os.system(f\"mkdir -p {save_files_path}protein_dicts/\")\n",
    "        qrint(f\"Saving {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Saving {R}protein_dict{S}:\")\n",
    "        for i in tqdm(range(11), total=11):\n",
    "            with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_dicts[i], f)\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_res_id_dict, f)\n",
    "    qrint(f\"Successfully processed and saved {R}protein_dict{S} and {R}protein_res_id_dict{S} to {B}{save_files_path}protein_dicts/{S}.\" if (cfg_mode == \"nciyes\") \n",
    "          else f\"Successfully processed and saved {R}protein_dict{S} to {B}{save_files_path}protein_dicts/{S}.\")\n",
    "    \n",
    "else:\n",
    "    qrint(f\"Loading {R}protein_dict{S} and {R}protein_res_id_dict{S} from {B}{save_files_path}protein_dicts/{S}\" if (cfg_mode == \"nciyes\") \n",
    "        else f\"Loading {R}protein_dict{S} from {B}{save_files_path}protein_dicts/{S}\")\n",
    "    for i in tqdm(range(11), total=11):\n",
    "        with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"rb\") as f:\n",
    "            protein_dict.update(pickle.load(f))\n",
    "    with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"rb\") as f:\n",
    "        protein_res_id_dict.update(pickle.load(f))\n",
    "    qrint(f\"Successfully loaded {R}protein_dict{S} and {R}protein_res_id_dict{S}.\" if (cfg_mode == \"nciyes\")\n",
    "        else f\"Successfully loaded {R}protein_dict{S}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ef2ed",
   "metadata": {},
   "source": [
    "### Segmentation of proteins by <font color=red>p2rank</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a6ff8-f905-4a7f-989f-3c796ba8b254",
   "metadata": {},
   "source": [
    "#### Generate or load <font color=\"red\">.ds</font> file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efdf01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34m.ds\u001b[0m file generation\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mFalse\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mTrue\u001b[0m.\n",
      "\u001b[1;34mRun\u001b[0m codes and \u001b[1;34msave\u001b[0m results.\n",
      "Processing \u001b[1;31mprotein_list.ds\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './Outputs/nciyes/Demo_22-09-09_0751/protein_list.ds': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbf57957bc44012a904d346f9cbb1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed \u001b[1;31mprotein_list.ds\u001b[0m at \u001b[1;34m./Outputs/nciyes/Demo_22-09-09_0751/protein_list.ds\u001b[0m\n",
      "Successfully saved \u001b[1;31mprotein_list.ds\u001b[0m to \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/protein_list.ds\u001b[0m\n",
      "CPU times: user 33.4 ms, sys: 2.71 ms, total: 36.1 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = False\n",
    "cfg_save_files = True\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}.ds{S} file generation\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    import shutil\n",
    "    qrint(f\"Processing {R}protein_list.ds{S}:\")\n",
    "    ds = f\"{main_path}protein_list.ds\"\n",
    "    os.system(f\"rm {ds}\")\n",
    "    with open(ds, \"w\") as out:\n",
    "        for _fpath in tqdm(pdb_fpath_list, total=len(pdb_fpath_list)):\n",
    "            out.write(f\"{ds_path}{_fpath}\\n\")\n",
    "    qrint(f\"Successfully processed {R}protein_list.ds{S} at {B}{main_path}protein_list.ds{S}\")\n",
    "    if cfg_save_files:\n",
    "        shutil.copy(ds, f\"{save_files_path}protein_list.ds\")\n",
    "        qrint(f\"Successfully saved {R}protein_list.ds{S} to {B}{save_files_path}protein_list.ds{S}\")\n",
    "else:\n",
    "    qrint(f\"Using existing {R}protein_list.ds{S} file at {B}{save_files_path}protein_list.ds{S}.\")\n",
    "    ds = f\"{save_files_path}protein_list.ds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633ba3-4336-40e1-a238-679468c231d5",
   "metadata": {},
   "source": [
    "#### Generate or check <font color=\"red\">p2rank results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2054fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34mp2rank\u001b[0m file generation or check\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Checking existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/p2rank/\u001b[0m:\n",
      "Existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/p2rank/\u001b[0m will be used.\n",
      "CPU times: user 781 µs, sys: 2.96 ms, total: 3.74 ms\n",
      "Wall time: 22.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = True\n",
    "cfg_save_files = False\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}p2rank{S} file generation or check\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "_drop_p2rank = set()\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    if cfg_save_files:\n",
    "        #os.mkdir(f\"{save_files_path}p2rank\")\n",
    "        print(f\"Running p2rank with output dir {B}{p2rank_save_path}{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {p2rank_save_path} -threads 8\"\n",
    "    else:\n",
    "        os.system(f\"mkdir -p {main_path}p2rank/\")\n",
    "        print(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {main_path}p2rank/ -threads 8\"\n",
    "    os.system(cmd)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f\"Checking existing p2rank files at {B}{p2rank_save_path}{S}:\")\n",
    "    _p2rank_dirset = set(os.listdir(f\"{p2rank_save_path}\"))\n",
    "    _log = []\n",
    "    for _pdb in pdb_code_list:\n",
    "        if _pdb+\"_protein.pdb_predictions.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, prediction\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "        if _pdb+\"_protein.pdb_residues.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, residues\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "    if len(_log) == 0:\n",
    "        qrint(f\"Existing p2rank files at {B}{save_files_path}p2rank/{S} will be used.\")\n",
    "        \n",
    "    else:\n",
    "        with open(f\"{main_path}log_absent_p2rank_pdbs.txt\", \"w\") as f:\n",
    "            f.writelines(_log)\n",
    "        qrint(f\"{len(_log)} files related to {len(_drop_p2rank)} proteins not found in {B}{save_files_path}p2rank/{S}.\")\n",
    "        qrint(f\"Information of abasent files saved to {B}{main_path}log_absent_p2rank_pdbs.txt{S}.\")\n",
    "        qrint(f\"Reprocessing {R}protein_list_absent.ds{S} for absent files:\")\n",
    "        ds2 = f\"{main_path}log_absent_p2rank_pdbs.txt\"\n",
    "        with open(ds, \"r\") as infile:\n",
    "            with open(ds2, \"w\") as out:\n",
    "                for _line in infile.readlines():\n",
    "                    if re.match(r\".+\\/(....)_protein.pdb\", _line).groups()[0] in _drop_p2rank:\n",
    "                        out.write(_line)\n",
    "        qrint(f\"Successfully processed {R}protein_list_absent.ds{S} at {B}{main_path}protein_list_absent.ds{S}\")\n",
    "        \n",
    "        qrint(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds2} -o {main_path}p2rank/ -threads 8\"\n",
    "        os.system(f\"mkdir -p {main_path}p2rank\")\n",
    "        os.system(cmd)\n",
    "        import shutil\n",
    "        qrint(f\"Copying generated files to {B}{save_files_path}p2rank/{S}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/\"):\n",
    "            if _f != \"params.txt\" and _f != \"visualizations\" and _f != \"run.log\":\n",
    "                shutil.copy(f\"{main_path}p2rank/{_f}\", f\"{p2rank_save_path}{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/\"):\n",
    "            if _f != \"data\":\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/{_f}\", f\"{p2rank_save_path}visualizations/{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/data\"):\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/data/{_f}\", f\"{p2rank_save_path}visualizations/data/{_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce0876",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get ligand infomation: <font color=\"red\">ligand_df</font> (& <font color=\"red\">ligand_atom_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d67dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mligand_df\u001b[0m & \u001b[1;31mligand_atom_id_dict\u001b[0m processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/ligand_df.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_df\u001b[0m.\n",
      "Loading \u001b[1;31mligand_atom_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/ligand_atom_id_dict.pkl\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_atom_id_dict\u001b[0m.\n",
      "CPU times: user 7.55 ms, sys: 1.54 ms, total: 9.1 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}ligand_df{S} & {R}ligand_atom_id_dict{S} processing\" if (cfg_mode==\"nciyes\") else f\"{R}ligand_df{S} processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "\n",
    "ligand_atom_id_dict = {} if (cfg_mode==\"nciyes\") else None\n",
    "    \n",
    "parser = PDBParser()\n",
    "if not cfg_use_saved_files:\n",
    "    print(f\"Processing {R}ligand_df{S}:\")\n",
    "    if cfg_mode == \"nciyes\":\n",
    "        ligand_name_list = list(ligand_df[\"ligand_name_file\"])\n",
    "    ligand_pdb_list = list(ligand_df[\"pdb_code\"])\n",
    "    ligand_fpath_list = list(ligand_df[\"ligand_fpath\"])\n",
    "    canonique_smiles = []\n",
    "    error_list = []\n",
    "    for _fpath, _pdb in tqdm(zip(ligand_fpath_list, ligand_pdb_list), total=len(ligand_fpath_list)):\n",
    "        if \".sdf\" in _fpath:\n",
    "            try:\n",
    "                canonique_smiles.append(get_canonical_smiles(Chem.MolToSmiles(Chem.MolFromMolFile(_fpath))))\n",
    "            except Exception as e:\n",
    "                canonique_smiles.append(f\"ERROR - {str(e)}\")\n",
    "                error_list.append(_fpath)\n",
    "                continue            \n",
    "        elif \".pdb\" in _fpath:\n",
    "            try:\n",
    "                canonique_smiles.append(get_canonical_smiles(Chem.MolToSmiles(Chem.MolFromPDBFile(_fpath))))\n",
    "            except Exception as e:\n",
    "                canonique_smiles.append(f\"ERROR - {str(e)}\")\n",
    "                error_list.append(_fpath)\n",
    "                continue\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            structure = parser.get_structure(\"pdb\", _fpath)[0]\n",
    "            _atom_ids = []\n",
    "            for chain in structure:\n",
    "                for residue in chain:\n",
    "                    for atom in residue.get_atoms():\n",
    "                        atom_id = atom.get_name()\n",
    "                        _atom_ids.append(atom_id)\n",
    "            ligand_atom_id_dict[_pdb] = {_atom:i for (_atom, i) in zip(_atom_ids, range(len(_atom_ids)))}\n",
    "    ligand_df[\"canonique_smiles\"] = canonique_smiles\n",
    "    qrint(f\"Successfully processed {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_save_files:\n",
    "        # Saving ligand_df\n",
    "        ligand_df.to_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "        qrint(f\"Successfully saved {R}ligand_df{S} to {B}{save_files_path}ligand_df.csv{S}.\")\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ligand_atom_id_dict, f)\n",
    "            qrint(f\"Successfully saved {R}ligand_atom_id_dict{S} to {B}{save_files_path}ligand_atom_id_dict.pkl{S}.\")\n",
    "\n",
    "else: # cfg_use_saved_files = True\n",
    "    qrint(f\"Loading {R}ligand_df{S} from {B}{save_files_path}ligand_df.csv{S}:\")\n",
    "    ligand_df = pd.read_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "    qrint(f\"Successfully loaded {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_mode == \"nciyes\":\n",
    "        qrint(f\"Loading {R}ligand_atom_id_dict{S} from {B}{save_files_path}ligand_atom_id_dict.pkl{S}:\")\n",
    "        with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"rb\") as f:\n",
    "            ligand_atom_id_dict = pickle.load(f)\n",
    "        qrint(f\"Successfully loaded {R}ligand_atom_id_dict{S}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f99137",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_df_without_smiles = ligand_df[ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_without_smiles.to_csv(f\"{save_files_path}MDropped_1.Ligands_{len(ligand_df_without_smiles)} - SMILES Generation Error.csv\")\n",
    "ligand_df_with_smiles = ligand_df[~ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_with_smiles.to_csv(f\"{save_files_path}ligand_df_with_SMILES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02da2fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pdb_code</th>\n",
       "      <th>ligand_name_file</th>\n",
       "      <th>ligand_name_aff</th>\n",
       "      <th>ligand_name_nci</th>\n",
       "      <th>ligand_fpath</th>\n",
       "      <th>affinity</th>\n",
       "      <th>datagroup</th>\n",
       "      <th>release_year</th>\n",
       "      <th>canonique_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>MNP</td>\n",
       "      <td>MNP</td>\n",
       "      <td>MNP</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>refined</td>\n",
       "      <td>1997</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1thz</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1thz/1thz_ligand...</td>\n",
       "      <td>5.15</td>\n",
       "      <td>refined</td>\n",
       "      <td>2004</td>\n",
       "      <td>Cc1cc(S(=O)(=O)O)ccc1-n1nc(C)c(N=Nc2cc(S(=O)(=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4xiq</td>\n",
       "      <td>40Y</td>\n",
       "      <td>40Y</td>\n",
       "      <td>40Y</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4xiq/4xiq_ligand...</td>\n",
       "      <td>7.07</td>\n",
       "      <td>refined</td>\n",
       "      <td>2015</td>\n",
       "      <td>Cc1c2c(n3c1CCOc1cc(C(N)=O)ccc1-3)CC(C)(C)CC2=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3eqr</td>\n",
       "      <td>T74</td>\n",
       "      <td>T74</td>\n",
       "      <td>T74</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/3eqr/3eqr_ligand...</td>\n",
       "      <td>8.70</td>\n",
       "      <td>refined</td>\n",
       "      <td>2008</td>\n",
       "      <td>COC(C)(C)CCn1nc(Nc2c(C)cccc2C)c2cnc(Nc3ccc(N4C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6d2o</td>\n",
       "      <td>4MZ</td>\n",
       "      <td>4MZ</td>\n",
       "      <td>4MZ</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/6d2o/6d2o_ligand...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>refined</td>\n",
       "      <td>2018</td>\n",
       "      <td>Cc1c[nH]cn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4pvy</td>\n",
       "      <td>JD1</td>\n",
       "      <td>JD1</td>\n",
       "      <td>JD1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4pvy/4pvy_ligand...</td>\n",
       "      <td>6.71</td>\n",
       "      <td>refined</td>\n",
       "      <td>2015</td>\n",
       "      <td>CC(C)Oc1ccc(-c2cncc(NC(P(=O)(O)O)P(=O)(O)O)c2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5m23</td>\n",
       "      <td>7DC</td>\n",
       "      <td>7DC</td>\n",
       "      <td>7DC</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5m23/5m23_ligand...</td>\n",
       "      <td>8.90</td>\n",
       "      <td>refined</td>\n",
       "      <td>2017</td>\n",
       "      <td>CC(C)[C@@H](C=O)NC(=O)c1ccc(N=Nc2ccc(CNC(=O)[C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3own</td>\n",
       "      <td>3OX</td>\n",
       "      <td>3OX</td>\n",
       "      <td>3OX</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/3own/3own_ligand...</td>\n",
       "      <td>7.47</td>\n",
       "      <td>refined</td>\n",
       "      <td>2010</td>\n",
       "      <td>CC(C)CNC(=O)[C@@H](C[C@H](O)[C@@H]1COCc2cccc(c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4lhv</td>\n",
       "      <td>GDP</td>\n",
       "      <td>GDP</td>\n",
       "      <td>GDP</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4lhv/4lhv_ligand...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>refined</td>\n",
       "      <td>2013</td>\n",
       "      <td>Nc1nc2c(ncn2[C@@H]2O[C@H](CO[P@@](=O)(O)OP(=O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4r4t</td>\n",
       "      <td>3J0</td>\n",
       "      <td>3J0</td>\n",
       "      <td>3J0</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4r4t/4r4t_ligand...</td>\n",
       "      <td>6.26</td>\n",
       "      <td>refined</td>\n",
       "      <td>2014</td>\n",
       "      <td>O=C(O)c1cc(-c2ccc(CNC(=S)c3ccc(-c4ccc(C(=O)O)o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1i7z</td>\n",
       "      <td>COC</td>\n",
       "      <td>COC</td>\n",
       "      <td>COC</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1i7z/1i7z_ligand...</td>\n",
       "      <td>6.40</td>\n",
       "      <td>refined</td>\n",
       "      <td>2001</td>\n",
       "      <td>COC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1zdp</td>\n",
       "      <td>TIO</td>\n",
       "      <td>TIO</td>\n",
       "      <td>TIO</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1zdp/1zdp_ligand...</td>\n",
       "      <td>5.74</td>\n",
       "      <td>refined</td>\n",
       "      <td>2005</td>\n",
       "      <td>O=C(O)CNC(=O)[C@@H](CS)Cc1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>6czc</td>\n",
       "      <td>TTP</td>\n",
       "      <td>TTP</td>\n",
       "      <td>TTP</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/6czc/6czc_ligand...</td>\n",
       "      <td>4.78</td>\n",
       "      <td>refined</td>\n",
       "      <td>2018</td>\n",
       "      <td>Cc1cn([C@H]2C[C@H](O)[C@@H](CO[P@@](=O)(O)O[P@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3ujd</td>\n",
       "      <td>PC</td>\n",
       "      <td>PC</td>\n",
       "      <td>PC</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/3ujd/3ujd_ligand...</td>\n",
       "      <td>4.69</td>\n",
       "      <td>refined</td>\n",
       "      <td>2011</td>\n",
       "      <td>C[N+](C)(C)CCOP(=O)(O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2jg0</td>\n",
       "      <td>TTZ</td>\n",
       "      <td>TTZ</td>\n",
       "      <td>TTZ</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/2jg0/2jg0_ligand...</td>\n",
       "      <td>7.82</td>\n",
       "      <td>refined</td>\n",
       "      <td>2007</td>\n",
       "      <td>OC[C@H]1O[C@H](NC2=N[C@@H]3[C@H](S2)[C@@H](O)[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2qtg</td>\n",
       "      <td>MTH</td>\n",
       "      <td>MTH</td>\n",
       "      <td>MTH</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/2qtg/2qtg_ligand...</td>\n",
       "      <td>5.08</td>\n",
       "      <td>refined</td>\n",
       "      <td>2008</td>\n",
       "      <td>CSC[C@H]1O[C@@H](n2ccc3c(N)ncnc32)[C@H](O)[C@@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1jjt</td>\n",
       "      <td>BDS</td>\n",
       "      <td>BDS</td>\n",
       "      <td>BDS</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1jjt/1jjt_ligand...</td>\n",
       "      <td>8.05</td>\n",
       "      <td>general</td>\n",
       "      <td>2001</td>\n",
       "      <td>O=C(O)[C@@H](Cc1ccc2c(c1)OCO2)[C@H](Cc1ccc2c(c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3dsu</td>\n",
       "      <td>FPP</td>\n",
       "      <td>FPP</td>\n",
       "      <td>FPP</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/3dsu/3dsu_ligand...</td>\n",
       "      <td>7.03</td>\n",
       "      <td>general</td>\n",
       "      <td>2008</td>\n",
       "      <td>CC(C)=CCCC(C)=CCCC(C)=CCO[P@](=O)(O)OP(=O)(O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2p0d</td>\n",
       "      <td>I3P</td>\n",
       "      <td>I3P</td>\n",
       "      <td>I3P</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/2p0d/2p0d_ligand...</td>\n",
       "      <td>7.00</td>\n",
       "      <td>general</td>\n",
       "      <td>2007</td>\n",
       "      <td>O=P(O)(O)O[C@H]1[C@H](O)[C@@H](OP(=O)(O)O)[C@H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>5h17</td>\n",
       "      <td>LQE</td>\n",
       "      <td>LQE</td>\n",
       "      <td>LQE</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5h17/5h17_ligand...</td>\n",
       "      <td>6.05</td>\n",
       "      <td>general</td>\n",
       "      <td>2017</td>\n",
       "      <td>COc1cccc(C[C@H]2C[C@H]3Cc4c(cccc4OC)C[C@@H]3N(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>4l31</td>\n",
       "      <td>F08</td>\n",
       "      <td>F08</td>\n",
       "      <td>F08</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4l31/4l31_ligand...</td>\n",
       "      <td>6.79</td>\n",
       "      <td>general</td>\n",
       "      <td>2013</td>\n",
       "      <td>COC(=O)c1ccc(-c2cc(=O)c3ccccc3o2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>4xg7</td>\n",
       "      <td>X7G</td>\n",
       "      <td>X7G</td>\n",
       "      <td>X7G</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/4xg7/4xg7_ligand...</td>\n",
       "      <td>7.49</td>\n",
       "      <td>general</td>\n",
       "      <td>2015</td>\n",
       "      <td>Cc1nn(-c2ccnc(Nc3ccc4c(cnn4C)c3)n2)cc1CN1CC(O)C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2c5y</td>\n",
       "      <td>MTW</td>\n",
       "      <td>MTW</td>\n",
       "      <td>MTW</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/2c5y/2c5y_ligand...</td>\n",
       "      <td>6.60</td>\n",
       "      <td>general</td>\n",
       "      <td>2006</td>\n",
       "      <td>O=[N+]([O-])c1cccc(N=C2N=CCC(c3cccc(Cn4cncn4)c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>3unk</td>\n",
       "      <td>0BY</td>\n",
       "      <td>0BY</td>\n",
       "      <td>0BY</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/3unk/3unk_ligand...</td>\n",
       "      <td>4.82</td>\n",
       "      <td>general</td>\n",
       "      <td>2012</td>\n",
       "      <td>O=C(O)c1ccc(Nc2nccc(Nc3ccccc3Cl)n2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>5oq5</td>\n",
       "      <td>4K4</td>\n",
       "      <td>4K4</td>\n",
       "      <td>4K4</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5oq5/5oq5_ligand...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>general</td>\n",
       "      <td>2017</td>\n",
       "      <td>COc1cc(C(=O)N2CCC(N3CCN(C)CC3)CC2)ccc1Nc1ncc2c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>5alt</td>\n",
       "      <td>9XZ</td>\n",
       "      <td>9XZ</td>\n",
       "      <td>9XZ</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5alt/5alt_ligand...</td>\n",
       "      <td>4.88</td>\n",
       "      <td>general</td>\n",
       "      <td>2015</td>\n",
       "      <td>Cn1nnc(-c2cc(-c3ccsc3)cnc2N)n1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2nng</td>\n",
       "      <td>ZYX</td>\n",
       "      <td>ZYX</td>\n",
       "      <td>ZYX</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/2nng/2nng_ligand...</td>\n",
       "      <td>5.36</td>\n",
       "      <td>general</td>\n",
       "      <td>2007</td>\n",
       "      <td>NCCc1ccc(S(N)(=O)=O)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>5mnb</td>\n",
       "      <td>2AP</td>\n",
       "      <td>2AP</td>\n",
       "      <td>2AP</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...</td>\n",
       "      <td>2.48</td>\n",
       "      <td>general</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nc1cccc[nH+]1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>6e91</td>\n",
       "      <td>J04</td>\n",
       "      <td>J04</td>\n",
       "      <td>J04</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/6e91/6e91_ligand...</td>\n",
       "      <td>6.62</td>\n",
       "      <td>general</td>\n",
       "      <td>2019</td>\n",
       "      <td>COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 pdb_code ligand_name_file ligand_name_aff ligand_name_nci  \\\n",
       "0            0     1ai5              MNP             MNP             MNP   \n",
       "1            1     1thz              326             326             326   \n",
       "2            2     4xiq              40Y             40Y             40Y   \n",
       "3            3     3eqr              T74             T74             T74   \n",
       "4            4     6d2o              4MZ             4MZ             4MZ   \n",
       "5            5     4pvy              JD1             JD1             JD1   \n",
       "6            6     5m23              7DC             7DC             7DC   \n",
       "7            7     3own              3OX             3OX             3OX   \n",
       "8            8     4lhv              GDP             GDP             GDP   \n",
       "9            9     4r4t              3J0             3J0             3J0   \n",
       "10          10     1i7z              COC             COC             COC   \n",
       "11          11     1zdp              TIO             TIO             TIO   \n",
       "12          12     6czc              TTP             TTP             TTP   \n",
       "13          13     3ujd               PC              PC              PC   \n",
       "14          14     2jg0              TTZ             TTZ             TTZ   \n",
       "15          15     2qtg              MTH             MTH             MTH   \n",
       "16          16     1jjt              BDS             BDS             BDS   \n",
       "17          17     3dsu              FPP             FPP             FPP   \n",
       "18          18     2p0d              I3P             I3P             I3P   \n",
       "19          19     5h17              LQE             LQE             LQE   \n",
       "20          20     4l31              F08             F08             F08   \n",
       "21          21     4xg7              X7G             X7G             X7G   \n",
       "23          23     2c5y              MTW             MTW             MTW   \n",
       "24          24     3unk              0BY             0BY             0BY   \n",
       "25          25     5oq5              4K4             4K4             4K4   \n",
       "26          26     5alt              9XZ             9XZ             9XZ   \n",
       "27          27     2nng              ZYX             ZYX             ZYX   \n",
       "28          28     5mnb              2AP             2AP             2AP   \n",
       "29          29     6e91              J04             J04             J04   \n",
       "\n",
       "                                         ligand_fpath  affinity datagroup  \\\n",
       "0   ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...      3.72   refined   \n",
       "1   ../../dataspace/pdb_bind_2020/1thz/1thz_ligand...      5.15   refined   \n",
       "2   ../../dataspace/pdb_bind_2020/4xiq/4xiq_ligand...      7.07   refined   \n",
       "3   ../../dataspace/pdb_bind_2020/3eqr/3eqr_ligand...      8.70   refined   \n",
       "4   ../../dataspace/pdb_bind_2020/6d2o/6d2o_ligand...      5.30   refined   \n",
       "5   ../../dataspace/pdb_bind_2020/4pvy/4pvy_ligand...      6.71   refined   \n",
       "6   ../../dataspace/pdb_bind_2020/5m23/5m23_ligand...      8.90   refined   \n",
       "7   ../../dataspace/pdb_bind_2020/3own/3own_ligand...      7.47   refined   \n",
       "8   ../../dataspace/pdb_bind_2020/4lhv/4lhv_ligand...     10.00   refined   \n",
       "9   ../../dataspace/pdb_bind_2020/4r4t/4r4t_ligand...      6.26   refined   \n",
       "10  ../../dataspace/pdb_bind_2020/1i7z/1i7z_ligand...      6.40   refined   \n",
       "11  ../../dataspace/pdb_bind_2020/1zdp/1zdp_ligand...      5.74   refined   \n",
       "12  ../../dataspace/pdb_bind_2020/6czc/6czc_ligand...      4.78   refined   \n",
       "13  ../../dataspace/pdb_bind_2020/3ujd/3ujd_ligand...      4.69   refined   \n",
       "14  ../../dataspace/pdb_bind_2020/2jg0/2jg0_ligand...      7.82   refined   \n",
       "15  ../../dataspace/pdb_bind_2020/2qtg/2qtg_ligand...      5.08   refined   \n",
       "16  ../../dataspace/pdb_bind_2020/1jjt/1jjt_ligand...      8.05   general   \n",
       "17  ../../dataspace/pdb_bind_2020/3dsu/3dsu_ligand...      7.03   general   \n",
       "18  ../../dataspace/pdb_bind_2020/2p0d/2p0d_ligand...      7.00   general   \n",
       "19  ../../dataspace/pdb_bind_2020/5h17/5h17_ligand...      6.05   general   \n",
       "20  ../../dataspace/pdb_bind_2020/4l31/4l31_ligand...      6.79   general   \n",
       "21  ../../dataspace/pdb_bind_2020/4xg7/4xg7_ligand...      7.49   general   \n",
       "23  ../../dataspace/pdb_bind_2020/2c5y/2c5y_ligand...      6.60   general   \n",
       "24  ../../dataspace/pdb_bind_2020/3unk/3unk_ligand...      4.82   general   \n",
       "25  ../../dataspace/pdb_bind_2020/5oq5/5oq5_ligand...      7.70   general   \n",
       "26  ../../dataspace/pdb_bind_2020/5alt/5alt_ligand...      4.88   general   \n",
       "27  ../../dataspace/pdb_bind_2020/2nng/2nng_ligand...      5.36   general   \n",
       "28  ../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...      2.48   general   \n",
       "29  ../../dataspace/pdb_bind_2020/6e91/6e91_ligand...      6.62   general   \n",
       "\n",
       "    release_year                                   canonique_smiles  \n",
       "0           1997                      O=C(O)Cc1cccc([N+](=O)[O-])c1  \n",
       "1           2004  Cc1cc(S(=O)(=O)O)ccc1-n1nc(C)c(N=Nc2cc(S(=O)(=...  \n",
       "2           2015     Cc1c2c(n3c1CCOc1cc(C(N)=O)ccc1-3)CC(C)(C)CC2=O  \n",
       "3           2008  COC(C)(C)CCn1nc(Nc2c(C)cccc2C)c2cnc(Nc3ccc(N4C...  \n",
       "4           2018                                        Cc1c[nH]cn1  \n",
       "5           2015  CC(C)Oc1ccc(-c2cncc(NC(P(=O)(O)O)P(=O)(O)O)c2)cc1  \n",
       "6           2017  CC(C)[C@@H](C=O)NC(=O)c1ccc(N=Nc2ccc(CNC(=O)[C...  \n",
       "7           2010  CC(C)CNC(=O)[C@@H](C[C@H](O)[C@@H]1COCc2cccc(c...  \n",
       "8           2013  Nc1nc2c(ncn2[C@@H]2O[C@H](CO[P@@](=O)(O)OP(=O)...  \n",
       "9           2014  O=C(O)c1cc(-c2ccc(CNC(=S)c3ccc(-c4ccc(C(=O)O)o...  \n",
       "10          2001  COC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[...  \n",
       "11          2005                   O=C(O)CNC(=O)[C@@H](CS)Cc1ccccc1  \n",
       "12          2018  Cc1cn([C@H]2C[C@H](O)[C@@H](CO[P@@](=O)(O)O[P@...  \n",
       "13          2011                            C[N+](C)(C)CCOP(=O)(O)O  \n",
       "14          2007  OC[C@H]1O[C@H](NC2=N[C@@H]3[C@H](S2)[C@@H](O)[...  \n",
       "15          2008  CSC[C@H]1O[C@@H](n2ccc3c(N)ncnc32)[C@H](O)[C@@...  \n",
       "16          2001  O=C(O)[C@@H](Cc1ccc2c(c1)OCO2)[C@H](Cc1ccc2c(c...  \n",
       "17          2008     CC(C)=CCCC(C)=CCCC(C)=CCO[P@](=O)(O)OP(=O)(O)O  \n",
       "18          2007  O=P(O)(O)O[C@H]1[C@H](O)[C@@H](OP(=O)(O)O)[C@H...  \n",
       "19          2017  COc1cccc(C[C@H]2C[C@H]3Cc4c(cccc4OC)C[C@@H]3N(...  \n",
       "20          2013               COC(=O)c1ccc(-c2cc(=O)c3ccccc3o2)cc1  \n",
       "21          2015   Cc1nn(-c2ccnc(Nc3ccc4c(cnn4C)c3)n2)cc1CN1CC(O)C1  \n",
       "23          2006  O=[N+]([O-])c1cccc(N=C2N=CCC(c3cccc(Cn4cncn4)c...  \n",
       "24          2012             O=C(O)c1ccc(Nc2nccc(Nc3ccccc3Cl)n2)cc1  \n",
       "25          2017  COc1cc(C(=O)N2CCC(N3CCN(C)CC3)CC2)ccc1Nc1ncc2c...  \n",
       "26          2015                     Cn1nnc(-c2cc(-c3ccsc3)cnc2N)n1  \n",
       "27          2007                            NCCc1ccc(S(N)(=O)=O)cc1  \n",
       "28          2017                                      Nc1cccc[nH+]1  \n",
       "29          2019  COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ligand_df_with_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ae143-5c65-4397-844c-1b92189d6224",
   "metadata": {},
   "source": [
    "### Get <font color = \"red\"> info </font> for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5671fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31minfo\u001b[0m for dataset processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31minfo\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v9.8-only2/info.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31minfo\u001b[0m. Length: \u001b[1;34m238\u001b[0m\n",
      "CPU times: user 8.44 ms, sys: 1.09 ms, total: 9.52 ms\n",
      "Wall time: 30.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info{S} for dataset processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}info{S}:\")\n",
    "    info = []\n",
    "    error_info = []\n",
    "    for i,line in tqdm(ligand_df_with_smiles.iterrows(), total=ligand_df_with_smiles.shape[0]):\n",
    "        ligand_name = line[\"ligand_name_file\"]\n",
    "        pdb_code = line[\"pdb_code\"]\n",
    "        pdb_fname = line[\"pdb_code\"] + \"_protein\"\n",
    "        canonique_smiles = line[\"canonique_smiles\"]\n",
    "        ligand_fpath = line[\"ligand_fpath\"]\n",
    "        ligand_ftype = os.path.splitext(os.path.split(ligand_fpath)[1])[1]\n",
    "        release_year = line[\"release_year\"]\n",
    "        affinity = line[\"affinity\"]\n",
    "        if cfg_save_files or cfg_use_saved_files:\n",
    "            p2rankFile = f\"{p2rank_save_path}{pdb_fname}.pdb_predictions.csv\"\n",
    "        else:\n",
    "            p2rankFile = f\"{main_path}p2rank/{pdb_fname}.pdb_predictions.csv\"\n",
    "                    \n",
    "        pocket_df = pd.read_csv(p2rankFile)\n",
    "        pocket_df.columns = pocket_df.columns.str.strip()\n",
    "        pocket_coms = pocket_df[[\"center_x\", \"center_y\", \"center_z\"]].values\n",
    "        if len(pocket_coms) == 0:\n",
    "            error_info.append([pdb_code, \"p2rank file error\", ligand_name, ligand_fpath, release_year])\n",
    "            continue\n",
    "        if cfg_right_pocket_by_minor_distance:\n",
    "            if ligand_ftype == \".sdf\":\n",
    "                coord = Chem.MolFromMolFile(ligand_fpath).GetConformer().GetPositions()\n",
    "            elif ligand_ftype == \".pdb\":\n",
    "                coord = Chem.MolFromPDBFile(ligand_fpath).GetConformer().GetPositions()\n",
    "            coord = coord.sum(axis=0)/len(coord)\n",
    "            right_ith_pocket = ((pocket_coms-coord)**2).sum(axis=1).argmin()\n",
    "        for ith_pocket, _pocket_com in enumerate(pocket_coms):\n",
    "            _pocket_com = \",\".join([str(a.round(3)) for a in _pocket_com])\n",
    "            if cfg_right_pocket_by_minor_distance:\n",
    "                right_pocket_by_distance = (ith_pocket == right_ith_pocket)\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, affinity, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year, right_pocket_by_distance])\n",
    "            else:\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year])\n",
    "\n",
    "    info = pd.DataFrame(info, columns = [\"ligand_name\", \"pdb_code\", \"canonique_smiles\", \n",
    "                                        \"ligand_fpath\", \"ligand_ftype\", \"affinity\", \"pocket_name\", \"pocket_com\", \"release_year\", \"right_pocket_by_distance\"])\n",
    "    error_info = pd.DataFrame(error_info, columns = [\"pdb_code\", \"info\", \"ligand_name\", \"ligand_fpath\", \"release_year\"])\n",
    "    qrint(f\"Successfully processed {R}info{S}.\")\n",
    "    \n",
    "    # Remove recorded bad info\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "    info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    if cfg_save_files:\n",
    "        info.to_csv(f\"{save_files_path}info.csv\")\n",
    "        qrint(f\"Successfully saved {R}info{S} to {B}{save_files_path}info.csv{S}.\")\n",
    "        error_info.to_csv(f\"{save_files_path}MDropped_2.PDBs - Info Generation Error.csv\")\n",
    "        qrint(f\"Successfully saved {R}error_info{S} to {B}{save_files_path}MDropped_2.PDBs - Info Generation Error.csv{S}.\")\n",
    "\n",
    "else:\n",
    "    qrint(f\"Loading {R}info{S} from {B}{save_files_path}info.csv{S}:\")\n",
    "    info = pd.read_csv(f\"{save_files_path}info.csv\", index_col = 0)\n",
    "    \n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_pocket.csv{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_protein.txt{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\"):    \n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv{S}.\")\n",
    "    \n",
    "    info = info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    \n",
    "    qrint(f\"Successfully loaded {R}info{S}. Length: {B}{len(info)}{S}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2617bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ligand_name</th>\n",
       "      <th>pdb_code</th>\n",
       "      <th>canonique_smiles</th>\n",
       "      <th>ligand_fpath</th>\n",
       "      <th>ligand_ftype</th>\n",
       "      <th>affinity</th>\n",
       "      <th>pocket_name</th>\n",
       "      <th>pocket_com</th>\n",
       "      <th>release_year</th>\n",
       "      <th>right_pocket_by_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNP</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>3.72</td>\n",
       "      <td>pocket_1</td>\n",
       "      <td>13.744,38.358,37.723</td>\n",
       "      <td>1997</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNP</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>3.72</td>\n",
       "      <td>pocket_2</td>\n",
       "      <td>23.32,22.373,12.148</td>\n",
       "      <td>1997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNP</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>3.72</td>\n",
       "      <td>pocket_3</td>\n",
       "      <td>27.917,52.005,37.488</td>\n",
       "      <td>1997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNP</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>3.72</td>\n",
       "      <td>pocket_4</td>\n",
       "      <td>12.822,14.997,60.244</td>\n",
       "      <td>1997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNP</td>\n",
       "      <td>1ai5</td>\n",
       "      <td>O=C(O)Cc1cccc([N+](=O)[O-])c1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>3.72</td>\n",
       "      <td>pocket_5</td>\n",
       "      <td>8.304,48.622,11.532</td>\n",
       "      <td>1997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2AP</td>\n",
       "      <td>5mnb</td>\n",
       "      <td>Nc1cccc[nH+]1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>2.48</td>\n",
       "      <td>pocket_5</td>\n",
       "      <td>-0.855,-33.342,-11.244</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2AP</td>\n",
       "      <td>5mnb</td>\n",
       "      <td>Nc1cccc[nH+]1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>2.48</td>\n",
       "      <td>pocket_6</td>\n",
       "      <td>-7.494,-18.402,1.966</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2AP</td>\n",
       "      <td>5mnb</td>\n",
       "      <td>Nc1cccc[nH+]1</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>2.48</td>\n",
       "      <td>pocket_7</td>\n",
       "      <td>20.325,-19.721,-11.178</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>J04</td>\n",
       "      <td>6e91</td>\n",
       "      <td>COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/6e91/6e91_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>6.62</td>\n",
       "      <td>pocket_1</td>\n",
       "      <td>-4.452,3.537,14.167</td>\n",
       "      <td>2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>J04</td>\n",
       "      <td>6e91</td>\n",
       "      <td>COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...</td>\n",
       "      <td>../../dataspace/pdb_bind_2020/6e91/6e91_ligand...</td>\n",
       "      <td>.pdb</td>\n",
       "      <td>6.62</td>\n",
       "      <td>pocket_2</td>\n",
       "      <td>5.221,-5.387,6.779</td>\n",
       "      <td>2019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ligand_name pdb_code                                   canonique_smiles  \\\n",
       "0           MNP     1ai5                      O=C(O)Cc1cccc([N+](=O)[O-])c1   \n",
       "1           MNP     1ai5                      O=C(O)Cc1cccc([N+](=O)[O-])c1   \n",
       "2           MNP     1ai5                      O=C(O)Cc1cccc([N+](=O)[O-])c1   \n",
       "3           MNP     1ai5                      O=C(O)Cc1cccc([N+](=O)[O-])c1   \n",
       "4           MNP     1ai5                      O=C(O)Cc1cccc([N+](=O)[O-])c1   \n",
       "..          ...      ...                                                ...   \n",
       "233         2AP     5mnb                                      Nc1cccc[nH+]1   \n",
       "234         2AP     5mnb                                      Nc1cccc[nH+]1   \n",
       "235         2AP     5mnb                                      Nc1cccc[nH+]1   \n",
       "236         J04     6e91  COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...   \n",
       "237         J04     6e91  COc1cc2c[n+](Cc3cc(OC)c(OC)c(OC)c3)ccc2cc1OS(N...   \n",
       "\n",
       "                                          ligand_fpath ligand_ftype  affinity  \\\n",
       "0    ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...         .pdb      3.72   \n",
       "1    ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...         .pdb      3.72   \n",
       "2    ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...         .pdb      3.72   \n",
       "3    ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...         .pdb      3.72   \n",
       "4    ../../dataspace/pdb_bind_2020/1ai5/1ai5_ligand...         .pdb      3.72   \n",
       "..                                                 ...          ...       ...   \n",
       "233  ../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...         .pdb      2.48   \n",
       "234  ../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...         .pdb      2.48   \n",
       "235  ../../dataspace/pdb_bind_2020/5mnb/5mnb_ligand...         .pdb      2.48   \n",
       "236  ../../dataspace/pdb_bind_2020/6e91/6e91_ligand...         .pdb      6.62   \n",
       "237  ../../dataspace/pdb_bind_2020/6e91/6e91_ligand...         .pdb      6.62   \n",
       "\n",
       "    pocket_name              pocket_com  release_year  \\\n",
       "0      pocket_1    13.744,38.358,37.723          1997   \n",
       "1      pocket_2     23.32,22.373,12.148          1997   \n",
       "2      pocket_3    27.917,52.005,37.488          1997   \n",
       "3      pocket_4    12.822,14.997,60.244          1997   \n",
       "4      pocket_5     8.304,48.622,11.532          1997   \n",
       "..          ...                     ...           ...   \n",
       "233    pocket_5  -0.855,-33.342,-11.244          2017   \n",
       "234    pocket_6    -7.494,-18.402,1.966          2017   \n",
       "235    pocket_7  20.325,-19.721,-11.178          2017   \n",
       "236    pocket_1     -4.452,3.537,14.167          2019   \n",
       "237    pocket_2      5.221,-5.387,6.779          2019   \n",
       "\n",
       "     right_pocket_by_distance  \n",
       "0                        True  \n",
       "1                       False  \n",
       "2                       False  \n",
       "3                       False  \n",
       "4                       False  \n",
       "..                        ...  \n",
       "233                     False  \n",
       "234                     False  \n",
       "235                     False  \n",
       "236                      True  \n",
       "237                     False  \n",
       "\n",
       "[238 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d21cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3883313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch_geometric.data import Dataset\n",
    "import rdkit.Chem as Chem    # conda install rdkit -c rdkit if import failure.\n",
    "from feature_utils import extract_torchdrug_feature_from_mol\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    from utils import construct_data_from_graph_gvp\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_utils import sx_construct_data_from_graph_gvp\n",
    "    from sx_feature_utils import sx_extract_torchdrug_feature_from_mol\n",
    "    from sx_new_utils import sx_ligand_dedocking\n",
    "    from sx_new_utils import sx_get_nci_matrix_by_dict\n",
    "elif cfg_mode == \"frag\":\n",
    "    from utils import construct_data_from_graph_gvp\n",
    "    nci_df = None\n",
    "\n",
    "\n",
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, proteinMode=0, compoundMode=1,\n",
    "                pocket_radius=20, shake_nodes=None, \n",
    "                transform=None, pre_transform=None, pre_filter=None, generate_3D_conf = False,\n",
    "                protein_res_id_dict=None, nci_df=None, ligand_atom_id_dict=None, cfg_mode=None,\n",
    "                right_pocket_by_distance=True,\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        print(self.processed_paths)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.nci_df=nci_df\n",
    "        self.protein_res_id_dict = protein_res_id_dict\n",
    "        self.ligand_atom_id_dict = ligand_atom_id_dict\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "        self.generate_3D_conf = generate_3D_conf\n",
    "        self.cfg_mode = cfg_mode if cfg_mode else \"tankbind\"\n",
    "        self.right_pocket_by_distance=right_pocket_by_distance\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt', 'protein.pt']\n",
    "\n",
    "    def process(self):\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        canonique_smiles = line['canonique_smiles']\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if type(pocket_com) == str else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line['use_whole_protein'] if \"use_whole_protein\" in line.index else False\n",
    "        protein_name = line['pdb_code']\n",
    "        pocket_name = line['pocket_name']\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = self.protein_dict[protein_name]\n",
    "        ligand_fpath = line['ligand_fpath']\n",
    "        ligand_ftype = line['ligand_ftype']\n",
    "        ligand_name = line['ligand_name']\n",
    "        affinity = line['affinity']\n",
    "        \n",
    "        if self.right_pocket_by_distance:\n",
    "            right_pocket_by_distance = line['right_pocket_by_distance']\n",
    "        \n",
    "        if ligand_ftype == \".pdb\":\n",
    "            mol = Chem.MolFromPDBFile(ligand_fpath)\n",
    "        elif ligand_ftype == \".sdf\":\n",
    "            mol = Chem.MolFromMolFile(ligand_fpath)\n",
    "        mol.Compute2DCoords()  \n",
    "        \n",
    "        if self.cfg_mode == \"tankbind\":\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "            try:\n",
    "                data, input_node_list, keepNode = construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                      protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                      coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                      pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                      use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            try:\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"nciyes\":\n",
    "            protein_res_ids = self.protein_res_id_dict[protein_name]\n",
    "            if (len(protein_res_ids.keys())-1) != protein_res_ids[list(protein_res_ids.keys())[-1]]:\n",
    "                return protein_name+\" ERROR_I : protein_res_ids length error.\"\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = sx_extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True, generate_3D_conf=False)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "                # y is distance map, instead of contact map.\n",
    "            try:\n",
    "                data, input_node_list, keepNode = sx_construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                    protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                    pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                    use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            kept_res_ids = [_id for (_id, _keep) in zip(protein_res_ids, keepNode) if _keep]\n",
    "\n",
    "            try:\n",
    "                atom_ids = self.ligand_atom_id_dict[protein_name]\n",
    "                #data.nci_sequence = torch.Tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, res_full_id, atom_ids, self.nci_df).flatten())\n",
    "                data.nci_sequence = torch.tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, kept_res_ids, atom_ids, self.nci_df).flatten())\n",
    "                data.pair_shape = (len(kept_res_ids), len(atom_ids))\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"frag\":\n",
    "            pass\n",
    "            #TODO: write your codes here. Refer to \"if self.cfg_mode==\"tankband\" above.\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e6972-8dca-4750-9ca3-f1cae581c58e",
   "metadata": {},
   "source": [
    "#### data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b384d-6b29-40a5-baf7-8ea893682cde",
   "metadata": {},
   "source": [
    "### datasets generation or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66002d0-9eb2-486c-877a-1dd368d3f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_use_saved_files = False\n",
    "cfg_save_files=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f329c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mdataset\u001b[0m generation or load\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mFalse\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mRun\u001b[0m codes but results \u001b[1;34mwon't be saved\u001b[0m.\n",
      "\u001b[1;31minfo split\u001b[0m for dataset processing\n",
      "\u001b[1;31mTimesplit\u001b[0m: for dataset processing, the results will always be saved.\n",
      "Checking timesplit files：\n",
      "Timesplit files not found. \u001b[1;31mLength split strategy\u001b[0m will be applied.\n",
      "\u001b[1;31mLength% split strategy\u001b[0m: 119 - 59 - 60. For dataset processing, the results will always be saved.\n",
      "Processing \u001b[1;31mdataset\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inputs/Savedfiles/nciyes.v9.8-only2/dataset/train/processed/data.pt', 'Inputs/Savedfiles/nciyes.v9.8-only2/dataset/train/processed/protein.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inputs/Savedfiles/nciyes.v9.8-only2/dataset/val/processed/data.pt', 'Inputs/Savedfiles/nciyes.v9.8-only2/dataset/val/processed/protein.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inputs/Savedfiles/nciyes.v9.8-only2/dataset/test/processed/data.pt', 'Inputs/Savedfiles/nciyes.v9.8-only2/dataset/test/processed/protein.pt']\n",
      "Successfully processed \u001b[1;31mdataset\u001b[0ms.\n",
      "-- \u001b[1;31mtrain set\u001b[0m : 119\n",
      "-- \u001b[1;31mval set\u001b[0m   : 59\n",
      "-- \u001b[1;31mtest set\u001b[0m  : 60\n",
      "-- \u001b[1;31mtest2 set\u001b[0m : 0\n",
      "CPU times: user 398 ms, sys: 493 ms, total: 891 ms\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}dataset{S} generation or load\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info split{S} for dataset processing\")\n",
    "\n",
    "\n",
    "if cfg_timesplit: # cfg_timesplit = True\n",
    "    qrint(f\"{R}Timesplit{S}: for dataset processing, the results will always be saved.\")\n",
    "    qrint(f\"Checking timesplit files：\")\n",
    "    if not os.path.exists(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_test.txt\"):\n",
    "        qrint(f\"Timesplit files not found. {R}Length split strategy{S} will be applied.\")\n",
    "        cfg_timesplit = False\n",
    "    else:\n",
    "        qrint(f\"Found timesplit files. {R}Timesplit{S} will be applied.\")\n",
    "        with open(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _train = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _val = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_test.txt\", \"r\") as f:\n",
    "            _test = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "            \n",
    "        dataset_path = f\"{save_files_path}dataset/\"\n",
    "        qrint(f\"Processing {R}dataset{S}:\")\n",
    "        if not cfg_use_saved_files:\n",
    "            os.system(f\"rm -r {dataset_path}\")\n",
    "            for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "                os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "            info_test2 = info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))]\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=info_test2, protein_dict=protein_dict, \n",
    "                                     protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) if len(info_test2) \\\n",
    "                        else None        \n",
    "        else:\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))], \n",
    "                                     protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) \\\n",
    "                        if os.path.exists(f\"{dataset_path}test2/processed/data.pt\") else None\n",
    "        \n",
    "        train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info[info.pdb_code.isin(_train)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info[info.pdb_code.isin(_val)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info[info.pdb_code.isin(_test)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "            \n",
    "if not cfg_timesplit:\n",
    "    test2_set = None\n",
    "    train_part, val_part, test_part = cfg_split_strategy[0], cfg_split_strategy[1], cfg_split_strategy[2]\n",
    "    train_val_split, val_test_split = int(train_part * len(info)), int((train_part+val_part) * len(info))\n",
    "    qrint(f\"{R}Length% split strategy{S}: {train_val_split} - {val_test_split-train_val_split} - {len(info)-val_test_split}. For dataset processing, the results will always be saved.\")\n",
    "    dataset_path = f\"{save_files_path}dataset/\"\n",
    "    qrint(f\"Processing {R}dataset{S}:\")\n",
    "    if not cfg_use_saved_files:\n",
    "        os.system(f\"rm -r {dataset_path}\")\n",
    "        for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "            os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "    train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info.iloc[0:train_val_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info.iloc[train_val_split:val_test_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info.iloc[val_test_split:], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    \n",
    "    \n",
    "qrint(f\"Successfully processed {R}dataset{S}s.\")\n",
    "qrint(f\"-- {R}train set{S} : {len(train_set)}\")\n",
    "qrint(f\"-- {R}val set{S}   : {len(val_set)}\")\n",
    "qrint(f\"-- {R}test set{S}  : {len(test_set)}\")\n",
    "qrint(f\"-- {R}test2 set{S} : {len(test2_set) if test2_set else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa6733c-1c42-4514-931f-cf06f8121ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdata(data, fpath = \"./trash.txt\", cfg_mode=cfg_mode):\n",
    "    if isinstance(data, str) or isinstance(data, list) or data is None:\n",
    "        with open(fpath, \"a\") as f:\n",
    "            f.write(f\"{data} error!\\n\")        \n",
    "    elif cfg_mode==\"nciyes\":\n",
    "        device = data.y_batch.device\n",
    "        samples = []\n",
    "        shapes = []\n",
    "        ff = True\n",
    "\n",
    "        # check nci record in right_pocket\n",
    "        for i, (_right, _shape) in enumerate(zip(data.right_pocket_by_distance, data.pair_shape)):\n",
    "            if _right == True:\n",
    "                samples.append(i)\n",
    "                shapes.append(_shape)\n",
    "        samples = torch.tensor(samples).to(device)\n",
    "        index = torch.isin(data.y_batch, samples)\n",
    "        ss = data.nci_sequence[index]\n",
    "        if len(samples) and not (ss.sum()):\n",
    "            with open(fpath, \"a\") as f:\n",
    "                f.write(f\"{data.dataname} : its right pocket has no NCI record!\\n\")\n",
    "def checkdata2(data, fpath = \"./trash2.txt\", cfg_mode=cfg_mode):\n",
    "    if isinstance(data, str) or isinstance(data, list):\n",
    "        with open(fpath, \"a\") as f:\n",
    "            f.write(f\"{data} error!\\n\")        \n",
    "    elif cfg_mode==\"nciyes\":\n",
    "        device = data.y_batch.device\n",
    "        samples = []\n",
    "        shapes = []\n",
    "        ff = True\n",
    "\n",
    "        # check nci record in right_pocket\n",
    "        for i, _right in enumerate(data.right_pocket_by_distance):\n",
    "            if _right == True:\n",
    "                samples.append(i)\n",
    "        samples = torch.tensor(samples).to(device)\n",
    "        index = torch.isin(data.y_batch, samples)\n",
    "        ss = data.nci_sequence[index]\n",
    "        if len(samples) and not (ss.sum()):\n",
    "            with open(fpath, \"a\") as f:\n",
    "                f.write(f\"{data.dataname} : its right pocket has no NCI record!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f467456f-53ab-4403-937f-308200f1d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_checkdata=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ce96b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ebf690-95c2-464d-a4d6-db72b6d50e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f66a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:52:25   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:7\"\n",
    "if True:\n",
    "    if cfg_mode == \"tankbind\":\n",
    "        from model import get_model\n",
    "        model = get_model(0, logging, device)\n",
    "        modelFile = \"../saved_models/self_dock.pt\"\n",
    "        model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "\n",
    "    elif cfg_mode == \"nciyes\":\n",
    "        from sx_model import sx_get_model\n",
    "        model = sx_get_model(0, logging, device, nciyes=True, margin=1, margin_weight=1, nci_weight=0, output_classes=2,\n",
    "                             class_weight=torch.tensor([1,1],dtype=torch.float32))\n",
    "        IaBNetFile = \"../saved_models/self_dock.pt\"\n",
    "        model.IaBNet.load_state_dict(torch.load(IaBNetFile, map_location=device))\n",
    "\n",
    "    elif cfg_mode == \"frag\":\n",
    "        from model_frag import get_model\n",
    "        model = get_model(0, logging, device)\n",
    "        modelFile = \"../saved_models/self_dock.pt\" #?\n",
    "        model.load_state_dict(torch.load(modelFile, map_location=device))#?\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    _ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f0a0001-48e1-482f-a39c-304b937cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "if True:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--max_epoch\", type=int, default=20)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=3)\n",
    "    parser.add_argument(\"--compound_name\", \n",
    "            choices=['PDL1','HPK1_3','HPK1_4','KRAS_G12D','MAT2A','SOS1','ALK', 'BRAF', 'BTK', 'CDK4', 'EGFR', 'FGFR1', 'JAK2', 'NTRK1', 'VEGFR2','PRMT5'])\n",
    "    parser.add_argument(\"--data_path\", type=str, default='data')\n",
    "    parser.add_argument(\"--init_model\", type=str, default=\"../saved_models/self_dock.pt\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=save_model_path)\n",
    "    parser.add_argument(\"--embedding_dir\", type=str, default='embedding')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--dropout_rate\", type=float, default=0)\n",
    "    parser.add_argument(\"--iter\", type=int, default=1)\n",
    "    parser.add_argument(\"--model_mode\", choices=['init', 'Halfbind', 'Tankbind'], default='Tankbind')\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d835a42d-d5e9-43d2-894b-26dafab0b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sx_new_class import InstanceDynamicBatchSampler\n",
    "mx = 1000\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "train_data_loader = DataLoader(train_set, batch_sampler = InstanceDynamicBatchSampler(dataset=train_set, max_num=mx, mode=\"node\", shuffle=True), follow_batch=['x', 'y', 'compound_pair'], num_workers=3)\n",
    "val_data_loader = DataLoader(val_set, batch_sampler = InstanceDynamicBatchSampler(dataset=val_set, max_num=mx, mode=\"node\", shuffle=True), follow_batch=['x', 'y', 'compound_pair'], num_workers=3) if len(val_set) else None\n",
    "test_data_loader = DataLoader(test_set, batch_sampler = InstanceDynamicBatchSampler(dataset=test_set, max_num=mx, mode=\"node\", shuffle=True), follow_batch=['x', 'y', 'compound_pair'], num_workers=1) if len(test_set) else None\n",
    "test2_data_loader = DataLoader(test2_set,batch_sampler = InstanceDynamicBatchSampler(dataset=test2_set, max_num=mx, mode=\"node\", shuffle=True), follow_batch=['x', 'y', 'compound_pair'], num_workers=1) if test2_set else None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "#device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "list_val_metric = []\n",
    "\n",
    "#from Collections import defaultdict\n",
    "\n",
    "nci_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d330515-674e-40bc-af35-f5c1d475c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "######## start train\n",
    "if cfg_running_mode == \"train\":\n",
    "    for _epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        _list_loss = []\n",
    "        _list_nci = []\n",
    "        _list_score = []\n",
    "        errors, error_file = [], f\"{main_path}train_{_epoch}_errors.txt\"\n",
    "        for i,data in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "            if cfg_checkdata:\n",
    "                checkdata(data)\n",
    "                if isinstance(data, str) or isinstance(data, list):\n",
    "                    errors.append(data)\n",
    "                    with open(error_file, \"a\") as f:\n",
    "                        f.write(\"\\n\"+str(data)+\"\\n\")    \n",
    "                    continue\n",
    "            data = data.to(device)\n",
    "            if cfg_mode == \"tankbind\": # Results : affinity_pred_dictbb\n",
    "                y_pred, affinity_pred = model(data)\n",
    "            elif cfg_mode == \"nciyes\": # Results : many\n",
    "                data = data.to(device)\n",
    "                y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                            data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                score = model.calculate_aff_score(affinity_pred, y_pred)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _list_loss.append(loss.detach().cpu())\n",
    "                _list_nci.append(nci_pred.detach().cpu())\n",
    "                _list_score.append(score.detach().cpu())\n",
    "        \n",
    "        nci_dict[f\"train_{_epoch}\"] = _list_nci\n",
    "        \n",
    "        losst = torch.mean(torch.cat([torch.tensor([i]) for i in _list_loss]), dim=0)\n",
    "        scoret = torch.mean(torch.cat([torch.tensor([i]) for i in _list_score]), dim=0)\n",
    "        \n",
    "        print('TRAIN----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst.item()))\n",
    "        writer.add_scalar('Loss/train', losst.item(), _epoch)\n",
    "        writer.add_scalar('MSE/train', scoret.item(), _epoch)\n",
    "        model.eval()\n",
    "        \n",
    "        if val_data_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                _list_loss_va = []\n",
    "                _list_nci = []\n",
    "                _list_score_va = []\n",
    "                for data in tqdm(val_data_loader):\n",
    "                    data = data.to(device)\n",
    "                    if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                    elif cfg_mode == \"nciyes\": # Results : many\n",
    "                        y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                        loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                    data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                        score = model.calculate_aff_score(affinity_pred, y_pred)\n",
    "                        _list_loss_va.append(loss)\n",
    "                        _list_nci.append(nci_pred)\n",
    "                        _list_score_va.append(score.detach().cpu())\n",
    "                losst_va = torch.mean(torch.cat([torch.tensor([i]) for i in _list_loss_va]), dim=0)\n",
    "                score_va = torch.mean(torch.cat([torch.tensor([i]) for i in _list_score_va]), dim=0)\n",
    "                \n",
    "                nci_dict[f\"val_{_epoch}\"] = _list_nci\n",
    "                print('VALID----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst_va.item()))\n",
    "                writer.add_scalar('Loss/valid', losst_va.item(), _epoch)\n",
    "                writer.add_scalar('MSE/valid', score_va.item(), _epoch)\n",
    "                state = {'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch': _epoch}\n",
    "                model_dir = args.model_dir + '/%s/lr%s-batchsize%s-%s' % (args.iter, args.lr, args.batch_size, args.model_mode)\n",
    "                if not os.path.exists(model_dir):\n",
    "                    os.system(f\"mkdir -p {model_dir}\")\n",
    "                epoch_model_dir = '%s/epoch-%s' % (model_dir, _epoch + 1)\n",
    "                torch.save(state, epoch_model_dir)\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            ## TODO 定义一个score用来评估\n",
    "\n",
    "            if test_data_loader is not None:\n",
    "                _list_loss_te = []\n",
    "                _list_nci = []\n",
    "                _list_score_te = []\n",
    "                for data in tqdm(test_data_loader):\n",
    "                    data = data.to(device)\n",
    "                    if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                            data = data.to(device)\n",
    "                            y_pred, affinity_pred = model(data)\n",
    "                    elif cfg_mode == \"nciyes\": # Results : many\n",
    "                        y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                        loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                    data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                        score = model.calculate_aff_score(affinity_pred, y_pred)\n",
    "                        _list_loss_te.append(loss)\n",
    "                        _list_nci.append(nci_pred)\n",
    "                        _list_score_te.append(score.detach().cpu())\n",
    "                nci_dict[f\"test_{_epoch}\"] = _list_nci\n",
    "                losst_te = torch.mean(torch.cat([torch.tensor([i]) for i in _list_loss_te]), dim=0)\n",
    "                score_te = torch.mean(torch.cat([torch.tensor([i]) for i in _list_score_te]), dim=0)\n",
    "                print('TEST----> loss: {}' .format(losst_te))\n",
    "                writer.add_scalar('Loss/test', losst_te.item(), _epoch)\n",
    "                writer.add_scalar('MSE/test', score_te.item(), _epoch)\n",
    "        if test2_data_loader is not None:\n",
    "            _list_loss_te_2 = []\n",
    "            for data in tqdm(test2_data_loader):\n",
    "                if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        data = data.to(device)\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                elif cfg_mode == \"nciyes\": # Results : many\n",
    "                    y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                    loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                    _list_loss_te_2.append(loss)\n",
    "            \n",
    "            losst_te_2 = torch.cat([torch.tensor([i]) for i in _list_loss_te_2])\n",
    "            losst_te_2 = torch.mean(losst_te_2, dim=0)\n",
    "            print('TEST2----> loss: {}' .format(losst_te_2))\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c77fcfb6-3ab9-4c07-a510-4f564a965432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd86da8826444f9a03489e1f168c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [1/20], Loss: 2.0073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c615ab04053464bb9fa720386ec5021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [1/20], Loss: 2.4404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7387a83d4a1474dab28b72c6f715ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [2/20], Loss: 2.1063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ba6a69a87b4d4895c6ea00bf8566b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [2/20], Loss: 2.4933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c17f6c79b540d3acee366bf6967a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [3/20], Loss: 2.0708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fa7db40e0e4385a66f282e56aaad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [3/20], Loss: 2.3770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac55f85d6da34e84b709d13f1e549408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [4/20], Loss: 2.1122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e3404720d34db1a2dbecceeb5a7017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [4/20], Loss: 2.4907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d6e8eed0694dc4904fd2d377e685bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [5/20], Loss: 1.9040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabb4fb20e4245bfb6be767c5141507b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [5/20], Loss: 2.4752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84612b100330458ea9a1429d2398f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [6/20], Loss: 1.9992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec2b2b3f6e47e88d12e05b761e3f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [6/20], Loss: 2.1895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cebd23b8a54d509917c9578777c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [7/20], Loss: 2.1012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54d0723e9b4485681a5e2a6d58aab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [7/20], Loss: 2.3886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a6d78f8ea747bda0677036a2a6a0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [8/20], Loss: 2.2665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590244ce2d984d0db2e1bf79829e92e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [8/20], Loss: 2.4286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d975edc556c4331a370daff6e5008e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [9/20], Loss: 2.0319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb2ba6362e450ba095be60054a6d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [9/20], Loss: 2.4538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6a8b1503ed4efeb7e8895f784a804d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [10/20], Loss: 1.8290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8664403a3ee24d7ca5ba7df4efdc2c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [10/20], Loss: 2.0760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ab50b507a4639865e99cb6ffe52e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [11/20], Loss: 1.9981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd124fc93c3b4e75aea11f70689a277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID----> Epoch [11/20], Loss: 2.2160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c16f8576b604f3e9f13d3c4ba53e486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py\", line 17, in __call__\n    elem = batch[0]\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m _list_nci \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m errors, error_file \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_errors.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_data_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data_loader)):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_checkdata:\n\u001b[1;32m     10\u001b[0m         checkdata(data)\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1250\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/envs/betabind_py38/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py\", line 17, in __call__\n    elem = batch[0]\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## start train\n",
    "if cfg_running_mode == \"inference\":\n",
    "    for _epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        _list_loss = []\n",
    "        _list_nci = []\n",
    "        errors, error_file = [], f\"{main_path}train_{_epoch}_errors.txt\"\n",
    "        for i,data in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "            if cfg_checkdata:\n",
    "                checkdata(data)\n",
    "                if isinstance(data, str) or isinstance(data, list):\n",
    "                    errors.append(data)\n",
    "                    with open(error_file, \"a\") as f:\n",
    "                        f.write(\"\\n\"+str(data)+\"\\n\")    \n",
    "                    continue\n",
    "            data = data.to(device)\n",
    "            if cfg_mode == \"tankbind\": # Results : affinity_pred_dictbb\n",
    "                y_pred, affinity_pred = model(data)\n",
    "            elif cfg_mode == \"nciyes\": # Results : many\n",
    "                data = data.to(device)\n",
    "                y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                            data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _list_loss.append(loss.detach().cpu())\n",
    "                _list_nci.append(nci_pred.detach().cpu())\n",
    "        \n",
    "        nci_dict[f\"train_{_epoch}\"] = _list_nci\n",
    "        \n",
    "        losst = torch.cat([torch.tensor([i]) for i in _list_loss])\n",
    "        losst = torch.mean(losst, dim=0)\n",
    "        \n",
    "        print('TRAIN----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst.item()))\n",
    "        model.eval()\n",
    "        \n",
    "        if val_data_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                _list_loss_va = []\n",
    "                _list_nci = []\n",
    "                for data in tqdm(val_data_loader):\n",
    "                    data = data.to(device)\n",
    "                    if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                    elif cfg_mode == \"nciyes\": # Results : many\n",
    "                        y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                        loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                    data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                        _list_loss_va.append(loss)\n",
    "                        _list_nci.append(nci_pred)\n",
    "                losst_va = torch.cat([torch.tensor([i]) for i in _list_loss_va])\n",
    "                losst_va = torch.mean(losst_va, dim=0)\n",
    "                \n",
    "                nci_dict[f\"val_{_epoch}\"] = _list_nci\n",
    "                print('VALID----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst_va.item()))\n",
    "                state = {'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch': _epoch}\n",
    "                model_dir = args.model_dir + '/%s/lr%s-batchsize%s-%s' % (args.iter, args.lr, args.batch_size, args.model_mode)\n",
    "                if not os.path.exists(model_dir):\n",
    "                    os.system(f\"mkdir -p {model_dir}\")\n",
    "                epoch_model_dir = '%s/epoch-%s' % (model_dir, _epoch + 1)\n",
    "                torch.save(state, epoch_model_dir)\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        ## TODO 定义一个score用来评估\n",
    "        \n",
    "        if test_data_loader is not None:\n",
    "            _list_loss_te = []\n",
    "            _list_nci = []\n",
    "            for data in tqdm(test_data_loader):\n",
    "                data = data.to(device)\n",
    "                if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        data = data.to(device)\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                elif cfg_mode == \"nciyes\": # Results : many\n",
    "                    y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                    loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                    _list_loss_te.append(loss)\n",
    "                    _list_nci.append(nci_pred)\n",
    "            \n",
    "            nci_dict[f\"test_{_epoch}\"] = _list_nci\n",
    "            losst_te = torch.cat([torch.tensor([i]) for i in _list_loss_te])\n",
    "            losst_te = torch.mean(losst_te, dim=0)\n",
    "            print('TEST----> loss: {}' .format(losst_te))\n",
    "        \n",
    "        if test2_data_loader is not None:\n",
    "            _list_loss_te_2 = []\n",
    "            for data in tqdm(test2_data_loader):\n",
    "                if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        data = data.to(device)\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                elif cfg_mode == \"nciyes\": # Results : many\n",
    "                    y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                    loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                    _list_loss_te_2.append(loss)\n",
    "            \n",
    "            losst_te_2 = torch.cat([torch.tensor([i]) for i in _list_loss_te_2])\n",
    "            losst_te_2 = torch.mean(losst_te_2, dim=0)\n",
    "            print('TEST2----> loss: {}' .format(losst_te_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de80375",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090a42f-2ebd-46ad-b6e7-53b6bc6dcabb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f481-f689-47e2-b00f-3ad1ddf46d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if cfg_mode == \"tankbind\":\n",
    "    pass\n",
    "elif cfg_mode == \"nciyes\":\n",
    "    with open(f\"{main_path}errors.pkl\", \"wb\") as f:\n",
    "        pickle.dump(errors, f)\n",
    "    with open(f\"{main_path}y_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_true_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_true_dict, f)\n",
    "    with open(f\"{main_path}affinity_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(affinity_pred_dict, f)\n",
    "    with open(f\"{main_path}data_name_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_name_dict, f)    \n",
    "    with open(f\"{main_path}dis_map_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dis_map_dict, f)\n",
    "    with open(f\"{main_path}loss_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(loss_dict, f)\n",
    "    with open(f\"{main_path}right_pocket_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(right_pocket_dict, f)\n",
    "elif cfg_mode == \"frag\":\n",
    "    pass\n",
    "    #TODO: write your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa32fcf-5871-4540-99a4-f617933f24d2",
   "metadata": {},
   "source": [
    "|CODE|EPOCH|TP|TN|FP|FN|Preci.%|Recall%|Notes\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "|NCIYes-test00-08150437|val_0|#25175|4490395|1414547|557|5.58|97.83|weights: 1, 500|\n",
    "|NCIYes-test00-08150549|val_0|9325|2553748|398723|3541|22.85|72.47|weights: 1, 100|\n",
    "|NCIYes-test00-08150616|val_0|0|2952471|0|12866|-|-|weights: 1, 50|\n",
    "|NCIYes-test00-08150700|val_0|10390|2496756|455715|2476|22.29|80.75|weights: 1, 80|\n",
    "|NCIYes-test00-08150700|val_0|18514|5197212|707730|7218|2.54|80.75|weights: 1, 80|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a37cb-2a2d-4ecb-aeb3-66ba560dd83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:betabind_py38]",
   "language": "python",
   "name": "conda-env-betabind_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cdc0ee80d7648aad0ad3f887bd8d80fecc6c84f6e00bb50108c159517dcbac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
