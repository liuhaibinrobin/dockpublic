{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23142276",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sx.nciyes.main.<font color=red>new</font>.ipynb\n",
    "\n",
    "Notebook for training and inference with updated NCI data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a9fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03:06:24'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec45038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration and Initilization\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf3ac7",
   "metadata": {},
   "source": [
    "1st run:\n",
    "- set \"cfg_save_files = True\" and \"cfg_use_saved_files = False\" to process data and saved them.\n",
    "\n",
    "otherwise:\n",
    "- set \"cfg_save_files = False\" and \"cfg_use_saved_files = True\" to skip processing and load saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62c21caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to change settings below.\n",
    "cfg_use_saved_files = True # If you have already generated all the data required.\n",
    "cfg_save_files = False # If you want to save generated data.\n",
    "cfg_frag = False\n",
    "cfg_checkdata = True\n",
    "cfg_split_strategy = (0.5,0.25,0.25)\n",
    "mode_ls = [\"tankbind\", \"nciyes\", \"frag\"]\n",
    "\n",
    "cfg_mode = \"nciyes\" # \"frag\n",
    "cfg_data_version = \"v8.30\" #\"v9.1\"\n",
    "\n",
    "\n",
    "cfg_timesplit = True # If timesplit is applicated for splitting the dataset.\n",
    "\n",
    "cfg_custom_dir_name = \"Demo\" # Prefix in the name of the output folder.\n",
    "cfg_train = True # If you want to train the model.\n",
    "cfg_distinguish_by_timestamp = True # If true, a timestamp is added to your output dir name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476eabd",
   "metadata": {},
   "source": [
    "### Other Configuration and Initilization\n",
    "Under normal condition, you needn't modify this chapiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6443014-d883-4654-8db5-17a42526f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under normal condition, you should not modify the codes below.\n",
    "input_path = f\"./Inputs/{cfg_mode}/{cfg_data_version}/\"\n",
    "output_path = f\"./Outputs/{cfg_mode}/\"\n",
    "save_files_path = f\"./Inputs/Savedfiles/{cfg_mode}.{cfg_data_version}/\"\n",
    "p2rank_save_path = f\"./Inputs/Savedfiles/p2rank/\"\n",
    "p2rank_path = \"../p2rank_2.3/prank\"\n",
    "ds_path = \"../../../\" # Related path used for ds files.\n",
    "save_model_path = f\"./Inputs/Savedfiles/{cfg_mode}.model/\" #TODO: write your paths here or just keep this.\n",
    "pdb_df_fname = f\"Data.{cfg_data_version}.PDBs.csv\"\n",
    "ligand_df_fname = f\"Data.{cfg_data_version}.Ligands.csv\"\n",
    "pdb_df_fpath = f\"{input_path}{pdb_df_fname}\"\n",
    "ligand_df_fpath = f\"{input_path}{ligand_df_fname}\"\n",
    "datainfo = f\"{input_path}Datainfo.txt\"\n",
    "p2rank = f\"bash {p2rank_path}\"\n",
    "\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_fname = f\"Data.{cfg_data_version}.NCIs.csv\"\n",
    "    nci_df_fpath = f\"{input_path}{nci_fname}\"\n",
    "    \n",
    "# Under normal condition, you should not modify the codes below.\n",
    "cfg_jupyter = True # If you're using jupyter notebook. # old\n",
    "cfg_right_pocket_by_minor_distance = True # If the right pocket is chosen by calculating distance between ligand center and pocket center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9110acb0-dd78-451e-811b-4a7a134ac29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Inputs/nciyes/v8.26/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46bd5d4c-1b4f-4264-b511-12f5e12ea65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to \u001b[1;34m./Outputs/nciyes/Demo_22-09-06_0306/\u001b[0m.\n",
      "\n",
      "Data version: v8.26\n",
      "PDBs: 1, Ligands: 1, NCIs: 20\n",
      "\n",
      "\n",
      "Loaded \u001b[1;31mpdb_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.PDBs.csv\u001b[0m with length 1\n",
      "Loaded \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.Ligands.csv\u001b[0m with length 1\n",
      "Loaded \u001b[1;31mnci_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.NCIs.csv\u001b[0m with length 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "sys.path.insert(0, \"../tankbind/\")\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    pass\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_feature_utils import sx_get_protein_feature, get_clean_res_list\n",
    "    from feature_utils import get_canonical_smiles\n",
    "elif cfg_mode == \"frag\":\n",
    "    from feature_utils import get_clean_res_list, get_protein_feature_qsar\n",
    "    from feature_utils import get_canonical_smiles\n",
    "\n",
    "if cfg_jupyter:\n",
    "    from tqdm.notebook import tqdm\n",
    "    R, B, S = \"\\033[1;31m\", \"\\033[1;34m\", \"\\033[0m\" \n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    R, B, S = \"\", \"\", \"\" \n",
    "\n",
    "_dirname = \"\"\n",
    "_dirname = (_dirname + cfg_custom_dir_name) if cfg_custom_dir_name else (time.strftime(\"%y-%m-%d_%H%M\"))\n",
    "_dirname = (_dirname + \"_\" + time.strftime(\"%y-%m-%d_%H%M\")) if (cfg_custom_dir_name and cfg_distinguish_by_timestamp) else _dirname\n",
    "main_path = f\"{output_path}{_dirname}/\"\n",
    "log_fpath = f\"{main_path}log.txt\"\n",
    "if os.path.exists(log_fpath):\n",
    "    os.system(f\"rm -r {log_fpath}\")\n",
    "if os.path.exists(main_path):\n",
    "    os.system(f\"rm -r {main_path}\")\n",
    "for _path in [save_files_path, save_model_path, main_path]:\n",
    "    os.system(f\"mkdir -p {_path}\")\n",
    "\n",
    "def qrint(target, jupyter= cfg_jupyter, log=log_fpath, R=R, B=B, S=S, r=True):\n",
    "    if r:\n",
    "        print(target)\n",
    "    if jupyter:\n",
    "        target = target.replace(R,\"\").replace(B,\"\").replace(S,\"\")\n",
    "    with open(log_fpath, \"a\") as f:\n",
    "        if target != \"\\n\":\n",
    "            f.write(time.strftime(\"[%m-%d %H:%M:%S] \")+target.replace(\"\\n\", \"                 \\n\")+\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "if cfg_mode == \"nci_yes\":\n",
    "    def get_full_id_old(full_id_ls: list, resname):\n",
    "        chain_id = full_id_ls[2]\n",
    "        res_id = full_id_ls[3][1]\n",
    "        return chain_id + \"_\" + str(res_id)+\"_\"+resname\n",
    "\n",
    "def saveconfig(cfg_use_saved_files, cfg_save_files, B=B, R=R, S=S):\n",
    "    qrint(f\"{R}cfg_use_saved_files{S}={B}{cfg_use_saved_files}{S}, {R}cfg_save_files{S}={B}{cfg_save_files}{S}.\")\n",
    "    if cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "    elif not cfg_use_saved_files and cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes and {B}save{S} results.\")\n",
    "    elif not cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes but results {B}won't be saved{S}.\")\n",
    "    else:\n",
    "        return(f\"{R}CONFIG WARNING{R}: {B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "        \n",
    "qrint(f\"Results will be saved to {B}{main_path}{S}.\\n\")\n",
    "\n",
    "with open(datainfo,\"r\") as f:\n",
    "    _ls = [k.replace(\"\\n\",\"\") for k in f.readlines()]\n",
    "    for _l in _ls:\n",
    "        qrint(_l)\n",
    "    qrint(\"\\n\")\n",
    "    \n",
    "pdb_df = pd.read_csv(pdb_df_fpath, index_col=0)\n",
    "ligand_df = pd.read_csv(ligand_df_fpath, index_col=0)\n",
    "pdb_code_list = list(pdb_df[\"pdb_code\"])\n",
    "pdb_fpath_list = list(pdb_df[\"pdb_fpath\"])\n",
    "qrint(f\"Loaded {R}pdb_df{S} from {B}{pdb_df_fpath}{S} with length {len(pdb_df)}\")\n",
    "qrint(f\"Loaded {R}ligand_df{S} from {B}{ligand_df_fpath}{S} with length {len(ligand_df)}\")\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_df = pd.read_csv(nci_df_fpath, index_col=0)\n",
    "    qrint(f\"Loaded {R}nci_df{S} from {B}{nci_df_fpath}{S} with length {len(nci_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62774b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get protein features: <font color=\"red\">protein_dict</font> (& <font color=\"red\">protein_res_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e6c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/protein_dicts/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a510affa04984611a511984dd6b8a663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m.\n",
      "CPU times: user 36 ms, sys: 3.86 ms, total: 39.9 ms\n",
      "Wall time: 77.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}protein_dict{S} and {R}protein_res_id_dict{S}\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "protein_dict = {}\n",
    "protein_res_id_dict = {} if cfg_mode==\"nciyes\" else None\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Processing {R}protein_dict{S}\")\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protein_dicts = [{} for n in range(11)] # 0,1,2,3,4,5,6,7,8,9,x\n",
    "    #protein_res_id_dict = [{} for n in range(11)]  #0,1,2,3,4,5,6,7,8,9,x\n",
    "    \n",
    "    for i, (_pname, _fpath) in tqdm(enumerate(zip(pdb_code_list, pdb_fpath_list)), total=len(pdb_fpath_list)):\n",
    "        s = parser.get_structure(_pname, _fpath)\n",
    "        res_list = list(s.get_residues())\n",
    "        clean_res_list = get_clean_res_list(res_list, ensure_ca_exist=True)\n",
    "        clean_res_full_id_list = [get_full_id_old(x.full_id, x.get_resname()) for x in clean_res_list] if (cfg_mode == \"nciyes\") else None\n",
    "\n",
    "        if (cfg_mode == \"tankbind\"):\n",
    "            _protein_dict = get_protein_feature(clean_res_list)\n",
    "        elif (cfg_mode == \"nciyes\"):\n",
    "            _protein_dict, _protein_res_id_dict = sx_get_protein_feature(clean_res_list, clean_res_full_id_list)\n",
    "        elif (cfg_mode == \"frag\"):\n",
    "            #TODO:write your function here\n",
    "            try:\n",
    "                _protein_dict = get_protein_feature_qsar(clean_res_list)\n",
    "            except Exception as e:\n",
    "                print(_pname+\" ERROR_dim : \"+str(e))\n",
    "            \n",
    "        ind = _pname[0]\n",
    "        ind = int(ind) if str.isdigit(ind) else 10\n",
    "\n",
    "        protein_dicts[ind][_pname] = _protein_dict\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            protein_res_id_dict[_pname] = _protein_res_id_dict\n",
    "            \n",
    "    for _d in protein_dicts:\n",
    "        protein_dict.update(_d)\n",
    "\n",
    "    \n",
    "    \n",
    "    if cfg_save_files:\n",
    "        os.system(f\"mkdir -p {save_files_path}protein_dicts/\")\n",
    "        qrint(f\"Saving {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Saving {R}protein_dict{S}:\")\n",
    "        for i in tqdm(range(11), total=11):\n",
    "            with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_dicts[i], f)\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_res_id_dict, f)\n",
    "    qrint(f\"Successfully processed and saved {R}protein_dict{S} and {R}protein_res_id_dict{S} to {B}{save_files_path}protein_dicts/{S}.\" if (cfg_mode == \"nciyes\") \n",
    "          else f\"Successfully processed and saved {R}protein_dict{S} to {B}{save_files_path}protein_dicts/{S}.\")\n",
    "    \n",
    "else:\n",
    "    qrint(f\"Loading {R}protein_dict{S} and {R}protein_res_id_dict{S} from {B}{save_files_path}protein_dicts/{S}\" if (cfg_mode == \"nciyes\") \n",
    "        else f\"Loading {R}protein_dict{S} from {B}{save_files_path}protein_dicts/{S}\")\n",
    "    for i in tqdm(range(11), total=11):\n",
    "        with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"rb\") as f:\n",
    "            protein_dict.update(pickle.load(f))\n",
    "    with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"rb\") as f:\n",
    "        protein_res_id_dict.update(pickle.load(f))\n",
    "    qrint(f\"Successfully loaded {R}protein_dict{S} and {R}protein_res_id_dict{S}.\" if (cfg_mode == \"nciyes\")\n",
    "        else f\"Successfully loaded {R}protein_dict{S}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ef2ed",
   "metadata": {},
   "source": [
    "### Segmentation of proteins by <font color=red>p2rank</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a6ff8-f905-4a7f-989f-3c796ba8b254",
   "metadata": {},
   "source": [
    "#### Generate or load <font color=\"red\">.ds</font> file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efdf01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34m.ds\u001b[0m file generation\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mFalse\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mTrue\u001b[0m.\n",
      "\u001b[1;34mRun\u001b[0m codes and \u001b[1;34msave\u001b[0m results.\n",
      "Processing \u001b[1;31mprotein_list.ds\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './Outputs/nciyes/Demo_22-09-01_1000/protein_list.ds': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13986dc58524f68add78483919408fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed \u001b[1;31mprotein_list.ds\u001b[0m at \u001b[1;34m./Outputs/nciyes/Demo_22-09-01_1000/protein_list.ds\u001b[0m\n",
      "Successfully saved \u001b[1;31mprotein_list.ds\u001b[0m to \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/protein_list.ds\u001b[0m\n",
      "CPU times: user 35.3 ms, sys: 5.01 ms, total: 40.3 ms\n",
      "Wall time: 93.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = False\n",
    "cfg_save_files = True\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}.ds{S} file generation\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    import shutil\n",
    "    qrint(f\"Processing {R}protein_list.ds{S}:\")\n",
    "    ds = f\"{main_path}protein_list.ds\"\n",
    "    os.system(f\"rm {ds}\")\n",
    "    with open(ds, \"w\") as out:\n",
    "        for _fpath in tqdm(pdb_fpath_list, total=len(pdb_fpath_list)):\n",
    "            out.write(f\"{ds_path}{_fpath}\\n\")\n",
    "    qrint(f\"Successfully processed {R}protein_list.ds{S} at {B}{main_path}protein_list.ds{S}\")\n",
    "    if cfg_save_files:\n",
    "        shutil.copy(ds, f\"{save_files_path}protein_list.ds\")\n",
    "        qrint(f\"Successfully saved {R}protein_list.ds{S} to {B}{save_files_path}protein_list.ds{S}\")\n",
    "else:\n",
    "    qrint(f\"Using existing {R}protein_list.ds{S} file at {B}{save_files_path}protein_list.ds{S}.\")\n",
    "    ds = f\"{save_files_path}protein_list.ds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633ba3-4336-40e1-a238-679468c231d5",
   "metadata": {},
   "source": [
    "#### Generate or check <font color=\"red\">p2rank results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2054fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34mp2rank\u001b[0m file generation or check\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Checking existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/p2rank/\u001b[0m:\n",
      "Existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/p2rank/\u001b[0m will be used.\n",
      "CPU times: user 6.47 ms, sys: 10.1 ms, total: 16.6 ms\n",
      "Wall time: 39.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = True\n",
    "cfg_save_files = False\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}p2rank{S} file generation or check\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "_drop_p2rank = set()\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    if cfg_save_files:\n",
    "        #os.mkdir(f\"{save_files_path}p2rank\")\n",
    "        print(f\"Running p2rank with output dir {B}{p2rank_save_path}{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {p2rank_save_path} -threads 8\"\n",
    "    else:\n",
    "        os.system(f\"mkdir -p {main_path}p2rank/\")\n",
    "        print(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {main_path}p2rank/ -threads 8\"\n",
    "    os.system(cmd)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f\"Checking existing p2rank files at {B}{p2rank_save_path}{S}:\")\n",
    "    _p2rank_dirset = set(os.listdir(f\"{p2rank_save_path}\"))\n",
    "    _log = []\n",
    "    for _pdb in pdb_code_list:\n",
    "        if _pdb+\"_protein.pdb_predictions.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, prediction\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "        if _pdb+\"_protein.pdb_residues.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, residues\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "    if len(_log) == 0:\n",
    "        qrint(f\"Existing p2rank files at {B}{save_files_path}p2rank/{S} will be used.\")\n",
    "        \n",
    "    else:\n",
    "        with open(f\"{main_path}log_absent_p2rank_pdbs.txt\", \"w\") as f:\n",
    "            f.writelines(_log)\n",
    "        qrint(f\"{len(_log)} files related to {len(_drop_p2rank)} proteins not found in {B}{save_files_path}p2rank/{S}.\")\n",
    "        qrint(f\"Information of abasent files saved to {B}{main_path}log_absent_p2rank_pdbs.txt{S}.\")\n",
    "        qrint(f\"Reprocessing {R}protein_list_absent.ds{S} for absent files:\")\n",
    "        ds2 = f\"{main_path}log_absent_p2rank_pdbs.txt\"\n",
    "        with open(ds, \"r\") as infile:\n",
    "            with open(ds2, \"w\") as out:\n",
    "                for _line in infile.readlines():\n",
    "                    if re.match(r\".+\\/(....)_protein.pdb\", _line).groups()[0] in _drop_p2rank:\n",
    "                        out.write(_line)\n",
    "        qrint(f\"Successfully processed {R}protein_list_absent.ds{S} at {B}{main_path}protein_list_absent.ds{S}\")\n",
    "        \n",
    "        qrint(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds2} -o {main_path}p2rank/ -threads 8\"\n",
    "        os.system(f\"mkdir -p {main_path}p2rank\")\n",
    "        os.system(cmd)\n",
    "        import shutil\n",
    "        qrint(f\"Copying generated files to {B}{save_files_path}p2rank/{S}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/\"):\n",
    "            if _f != \"params.txt\" and _f != \"visualizations\" and _f != \"run.log\":\n",
    "                shutil.copy(f\"{main_path}p2rank/{_f}\", f\"{p2rank_save_path}{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/\"):\n",
    "            if _f != \"data\":\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/{_f}\", f\"{p2rank_save_path}visualizations/{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/data\"):\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/data/{_f}\", f\"{p2rank_save_path}visualizations/data/{_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce0876",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get ligand infomation: <font color=\"red\">ligand_df</font> (& <font color=\"red\">ligand_atom_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d67dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mligand_df\u001b[0m & \u001b[1;31mligand_atom_id_dict\u001b[0m processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/ligand_df.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_df\u001b[0m.\n",
      "Loading \u001b[1;31mligand_atom_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/ligand_atom_id_dict.pkl\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_atom_id_dict\u001b[0m.\n",
      "CPU times: user 6.14 ms, sys: 3.14 ms, total: 9.28 ms\n",
      "Wall time: 44.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}ligand_df{S} & {R}ligand_atom_id_dict{S} processing\" if (cfg_mode==\"nciyes\") else f\"{R}ligand_df{S} processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "\n",
    "ligand_atom_id_dict = {} if (cfg_mode==\"nciyes\") else None\n",
    "    \n",
    "parser = PDBParser()\n",
    "if not cfg_use_saved_files:\n",
    "    print(f\"Processing {R}ligand_df{S}:\")\n",
    "    if cfg_mode == \"nciyes\":\n",
    "        ligand_name_list = list(ligand_df[\"ligand_name_ncifile\"])\n",
    "    ligand_pdb_list = list(ligand_df[\"pdb_code\"])\n",
    "    ligand_fpath_list = list(ligand_df[\"ligand_fpath\"])\n",
    "    canonique_smiles = []\n",
    "    error_list = []\n",
    "    for _fpath, _pdb in tqdm(zip(ligand_fpath_list, ligand_pdb_list), total=len(ligand_fpath_list)):\n",
    "        if \".sdf\" in _fpath:\n",
    "            try:\n",
    "                canonique_smiles.append(get_canonical_smiles(Chem.MolToSmiles(Chem.MolFromMolFile(_fpath))))\n",
    "            except Exception as e:\n",
    "                canonique_smiles.append(f\"ERROR - {str(e)}\")\n",
    "                error_list.append(_fpath)\n",
    "                continue            \n",
    "        elif \".pdb\" in _fpath:\n",
    "            try:\n",
    "                canonique_smiles.append(get_canonical_smiles(Chem.MolToSmiles(Chem.MolFromPDBFile(_fpath))))\n",
    "            except Exception as e:\n",
    "                canonique_smiles.append(f\"ERROR - {str(e)}\")\n",
    "                error_list.append(_fpath)\n",
    "                continue\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            structure = parser.get_structure(\"pdb\", _fpath)[0]\n",
    "            _atom_ids = []\n",
    "            for chain in structure:\n",
    "                for residue in chain:\n",
    "                    for atom in residue.get_atoms():\n",
    "                        atom_id = atom.get_name()\n",
    "                        _atom_ids.append(atom_id)\n",
    "            ligand_atom_id_dict[_pdb] = {_atom:i for (_atom, i) in zip(_atom_ids, range(len(_atom_ids)))}\n",
    "    ligand_df[\"canonique_smiles\"] = canonique_smiles\n",
    "    qrint(f\"Successfully processed {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_save_files:\n",
    "        # Saving ligand_df\n",
    "        ligand_df.to_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "        qrint(f\"Successfully saved {R}ligand_df{S} to {B}{save_files_path}ligand_df.csv{S}.\")\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ligand_atom_id_dict, f)\n",
    "            qrint(f\"Successfully saved {R}ligand_atom_id_dict{S} to {B}{save_files_path}ligand_atom_id_dict.pkl{S}.\")\n",
    "\n",
    "else: # cfg_use_saved_files = True\n",
    "    qrint(f\"Loading {R}ligand_df{S} from {B}{save_files_path}ligand_df.csv{S}:\")\n",
    "    ligand_df = pd.read_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "    qrint(f\"Successfully loaded {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_mode == \"nciyes\":\n",
    "        qrint(f\"Loading {R}ligand_atom_id_dict{S} from {B}{save_files_path}ligand_atom_id_dict.pkl{S}:\")\n",
    "        with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"rb\") as f:\n",
    "            ligand_atom_id_dict = pickle.load(f)\n",
    "        qrint(f\"Successfully loaded {R}ligand_atom_id_dict{S}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f99137",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_df_without_smiles = ligand_df[ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_without_smiles.to_csv(f\"{save_files_path}MDropped_1.Ligands_{len(ligand_df_without_smiles)} - SMILES Generation Error.csv\")\n",
    "ligand_df_with_smiles = ligand_df[~ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_with_smiles.to_csv(f\"{save_files_path}ligand_df_with_SMILES.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ae143-5c65-4397-844c-1b92189d6224",
   "metadata": {},
   "source": [
    "### Get <font color = \"red\"> info </font> for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5671fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31minfo\u001b[0m for dataset processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31minfo\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/info.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31minfo\u001b[0m. Length: \u001b[1;34m4\u001b[0m\n",
      "CPU times: user 6.14 ms, sys: 800 µs, total: 6.94 ms\n",
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info{S} for dataset processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}info{S}:\")\n",
    "    info = []\n",
    "    error_info = []\n",
    "    for i,line in tqdm(ligand_df_with_smiles.iterrows(), total=ligand_df_with_smiles.shape[0]):\n",
    "        ligand_name = line[\"ligand_name_file\"]\n",
    "        pdb_code = line[\"pdb_code\"]\n",
    "        pdb_fname = line[\"pdb_code\"] + \"_protein\"\n",
    "        canonique_smiles = line[\"canonique_smiles\"]\n",
    "        ligand_fpath = line[\"ligand_fpath\"]\n",
    "        ligand_ftype = os.path.splitext(os.path.split(ligand_fpath)[1])[1]\n",
    "        release_year = line[\"release_year\"]\n",
    "        affinity = line[\"affinity\"]\n",
    "        if cfg_save_files or cfg_use_saved_files:\n",
    "            p2rankFile = f\"{p2rank_save_path}{pdb_fname}.pdb_predictions.csv\"\n",
    "        else:\n",
    "            p2rankFile = f\"{main_path}p2rank/{pdb_fname}.pdb_predictions.csv\"\n",
    "                    \n",
    "        pocket_df = pd.read_csv(p2rankFile)\n",
    "        pocket_df.columns = pocket_df.columns.str.strip()\n",
    "        pocket_coms = pocket_df[[\"center_x\", \"center_y\", \"center_z\"]].values\n",
    "        if len(pocket_coms) == 0:\n",
    "            error_info.append([pdb_code, \"p2rank file error\", ligand_name, ligand_fpath, release_year])\n",
    "            continue\n",
    "        if cfg_right_pocket_by_minor_distance:\n",
    "            if ligand_ftype == \".sdf\":\n",
    "                coord = Chem.MolFromMolFile(ligand_fpath).GetConformer().GetPositions()\n",
    "            elif ligand_ftype == \".pdb\":\n",
    "                coord = Chem.MolFromPDBFile(ligand_fpath).GetConformer().GetPositions()\n",
    "            coord = coord.sum(axis=0)/len(coord)\n",
    "            right_ith_pocket = ((pocket_coms-coord)**2).sum(axis=1).argmin()\n",
    "        for ith_pocket, _pocket_com in enumerate(pocket_coms):\n",
    "            _pocket_com = \",\".join([str(a.round(3)) for a in _pocket_com])\n",
    "            if cfg_right_pocket_by_minor_distance:\n",
    "                right_pocket_by_distance = (ith_pocket == right_ith_pocket)\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, affinity, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year, right_pocket_by_distance])\n",
    "            else:\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year])\n",
    "\n",
    "    info = pd.DataFrame(info, columns = [\"ligand_name\", \"pdb_code\", \"canonique_smiles\", \n",
    "                                        \"ligand_fpath\", \"ligand_ftype\", \"affinity\", \"pocket_name\", \"pocket_com\", \"release_year\", \"right_pocket_by_distance\"])\n",
    "    error_info = pd.DataFrame(error_info, columns = [\"pdb_code\", \"info\", \"ligand_name\", \"ligand_fpath\", \"release_year\"])\n",
    "    qrint(f\"Successfully processed {R}info{S}.\")\n",
    "    \n",
    "    # Remove recorded bad info\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "    info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    if cfg_save_files:\n",
    "        info.to_csv(f\"{save_files_path}info.csv\")\n",
    "        qrint(f\"Successfully saved {R}info{S} to {B}{save_files_path}info.csv{S}.\")\n",
    "        error_info.to_csv(f\"{save_files_path}MDropped_2.PDBs - Info Generation Error.csv\")\n",
    "        qrint(f\"Successfully saved {R}error_info{S} to {B}{save_files_path}MDropped_2.PDBs - Info Generation Error.csv{S}.\")\n",
    "\n",
    "else:\n",
    "    qrint(f\"Loading {R}info{S} from {B}{save_files_path}info.csv{S}:\")\n",
    "    info = pd.read_csv(f\"{save_files_path}info.csv\", index_col = 0)\n",
    "    \n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_pocket.csv{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_protein.txt{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\"):    \n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv{S}.\")\n",
    "    \n",
    "    info = info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    \n",
    "    qrint(f\"Successfully loaded {R}info{S}. Length: {B}{len(info)}{S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d21cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3883313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch_geometric.data import Dataset\n",
    "import rdkit.Chem as Chem    # conda install rdkit -c rdkit if import failure.\n",
    "from feature_utils import extract_torchdrug_feature_from_mol\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    from utils import construct_data_from_graph_gvp\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_utils import sx_construct_data_from_graph_gvp\n",
    "    from sx_feature_utils import sx_extract_torchdrug_feature_from_mol\n",
    "    from sx_new_utils import sx_ligand_dedocking\n",
    "    from sx_new_utils import sx_get_nci_matrix_by_dict\n",
    "elif cfg_mode == \"frag\":\n",
    "    from utils import construct_data_from_graph_gvp\n",
    "    nci_df = None\n",
    "\n",
    "\n",
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, proteinMode=0, compoundMode=1,\n",
    "                pocket_radius=20, shake_nodes=None, \n",
    "                transform=None, pre_transform=None, pre_filter=None, generate_3D_conf = False,\n",
    "                protein_res_id_dict=None, nci_df=None, ligand_atom_id_dict=None, cfg_mode=None,\n",
    "                right_pocket_by_distance=True,\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        print(self.processed_paths)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.nci_df=nci_df\n",
    "        self.protein_res_id_dict = protein_res_id_dict\n",
    "        self.ligand_atom_id_dict = ligand_atom_id_dict\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "        self.generate_3D_conf = generate_3D_conf\n",
    "        self.cfg_mode = cfg_mode if cfg_mode else \"tankbind\"\n",
    "        self.right_pocket_by_distance=right_pocket_by_distance\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt', 'protein.pt']\n",
    "\n",
    "    def process(self):\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        canonique_smiles = line['canonique_smiles']\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if type(pocket_com) == str else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line['use_whole_protein'] if \"use_whole_protein\" in line.index else False\n",
    "        protein_name = line['pdb_code']\n",
    "        pocket_name = line['pocket_name']\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = self.protein_dict[protein_name]\n",
    "        ligand_fpath = line['ligand_fpath']\n",
    "        ligand_ftype = line['ligand_ftype']\n",
    "        ligand_name = line['ligand_name']\n",
    "        affinity = line['affinity']\n",
    "        \n",
    "        if self.right_pocket_by_distance:\n",
    "            right_pocket_by_distance = line['right_pocket_by_distance']\n",
    "        \n",
    "        if ligand_ftype == \".pdb\":\n",
    "            mol = Chem.MolFromPDBFile(ligand_fpath)\n",
    "        elif ligand_ftype == \".sdf\":\n",
    "            mol = Chem.MolFromMolFile(ligand_fpath)\n",
    "        mol.Compute2DCoords()  \n",
    "        \n",
    "        if self.cfg_mode == \"tankbind\":\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "            try:\n",
    "                data, input_node_list, keepNode = construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                      protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                      coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                      pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                      use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            try:\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"nciyes\":\n",
    "            protein_res_ids = self.protein_res_id_dict[protein_name]\n",
    "            if (len(protein_res_ids.keys())-1) != protein_res_ids[list(protein_res_ids.keys())[-1]]:\n",
    "                return protein_name+\" ERROR_I : protein_res_ids length error.\"\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = sx_extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True, generate_3D_conf=False)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "                # y is distance map, instead of contact map.\n",
    "            try:\n",
    "                data, input_node_list, keepNode = sx_construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                    protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                    pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                    use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            kept_res_ids = [_id for (_id, _keep) in zip(protein_res_ids, keepNode) if _keep]\n",
    "\n",
    "            try:\n",
    "                atom_ids = self.ligand_atom_id_dict[protein_name]\n",
    "                #data.nci_sequence = torch.Tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, res_full_id, atom_ids, self.nci_df).flatten())\n",
    "                data.nci_sequence = torch.tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, kept_res_ids, atom_ids, self.nci_df).flatten())\n",
    "                data.pair_shape = (len(kept_res_ids), len(atom_ids))\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"frag\":\n",
    "            pass\n",
    "            #TODO: write your codes here. Refer to \"if self.cfg_mode==\"tankband\" above.\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e6972-8dca-4750-9ca3-f1cae581c58e",
   "metadata": {},
   "source": [
    "#### data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b384d-6b29-40a5-baf7-8ea893682cde",
   "metadata": {},
   "source": [
    "### datasets generation or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b66002d0-9eb2-486c-877a-1dd368d3f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_use_saved_files = False\n",
    "cfg_save_files=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f329c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mdataset\u001b[0m generation or load\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mFalse\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mRun\u001b[0m codes but results \u001b[1;34mwon't be saved\u001b[0m.\n",
      "\u001b[1;31minfo split\u001b[0m for dataset processing\n",
      "\u001b[1;31mTimesplit\u001b[0m: for dataset processing, the results will always be saved.\n",
      "Checking timesplit files：\n",
      "Found timesplit files. \u001b[1;31mTimesplit\u001b[0m will be applied.\n",
      "Processing \u001b[1;31mdataset\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/train/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/train/processed/protein.pt']\n",
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/val/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/val/processed/protein.pt']\n",
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/test/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/test/processed/protein.pt']\n",
      "Successfully processed \u001b[1;31mdataset\u001b[0ms.\n",
      "-- \u001b[1;31mtrain set\u001b[0m : 4\n",
      "-- \u001b[1;31mval set\u001b[0m   : 0\n",
      "-- \u001b[1;31mtest set\u001b[0m  : 0\n",
      "-- \u001b[1;31mtest2 set\u001b[0m : 0\n",
      "CPU times: user 40.7 ms, sys: 20.6 ms, total: 61.2 ms\n",
      "Wall time: 474 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}dataset{S} generation or load\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info split{S} for dataset processing\")\n",
    "\n",
    "\n",
    "if cfg_timesplit: # cfg_timesplit = True\n",
    "    qrint(f\"{R}Timesplit{S}: for dataset processing, the results will always be saved.\")\n",
    "    qrint(f\"Checking timesplit files：\")\n",
    "    if not os.path.exists(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_test.txt\"):\n",
    "        qrint(f\"Timesplit files not found. {R}Length split strategy{S} will be applied.\")\n",
    "        cfg_timesplit = False\n",
    "    else:\n",
    "        qrint(f\"Found timesplit files. {R}Timesplit{S} will be applied.\")\n",
    "        with open(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _train = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _val = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_test.txt\", \"r\") as f:\n",
    "            _test = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "            \n",
    "        dataset_path = f\"{save_files_path}dataset/\"\n",
    "        qrint(f\"Processing {R}dataset{S}:\")\n",
    "        if not cfg_use_saved_files:\n",
    "            os.system(f\"rm -r {dataset_path}\")\n",
    "            for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "                os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "            info_test2 = info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))]\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=info_test2, protein_dict=protein_dict, \n",
    "                                     protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) if len(info_test2) \\\n",
    "                        else None        \n",
    "        else:\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))], \n",
    "                                     protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) \\\n",
    "                        if os.path.exists(f\"{dataset_path}test2/processed/data.pt\") else None\n",
    "        \n",
    "        train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info[info.pdb_code.isin(_train)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info[info.pdb_code.isin(_val)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info[info.pdb_code.isin(_test)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "            \n",
    "if not cfg_timesplit:\n",
    "    test2_set = None\n",
    "    train_part, val_part, test_part = cfg_split_strategy[0], cfg_split_strategy[1], cfg_split_strategy[2]\n",
    "    train_val_split, val_test_split = int(train_part * len(info)), int((train_part+val_part) * len(info))\n",
    "    qrint(f\"{R}Length% split strategy{S}: {train_val_split} - {val_test_split-train_val_split} - {len(info)-val_test_split}. For dataset processing, the results will always be saved.\")\n",
    "    dataset_path = f\"{save_files_path}dataset/\"\n",
    "    qrint(f\"Processing {R}dataset{S}:\")\n",
    "    if not cfg_use_saved_files:\n",
    "        os.system(f\"rm -r {dataset_path}\")\n",
    "        for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "            os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "    train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info.iloc[0:train_val_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info.iloc[train_val_split:val_test_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info.iloc[val_test_split:], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    \n",
    "    \n",
    "qrint(f\"Successfully processed {R}dataset{S}s.\")\n",
    "qrint(f\"-- {R}train set{S} : {len(train_set)}\")\n",
    "qrint(f\"-- {R}val set{S}   : {len(val_set)}\")\n",
    "qrint(f\"-- {R}test set{S}  : {len(test_set)}\")\n",
    "qrint(f\"-- {R}test2 set{S} : {len(test2_set) if test2_set else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa6733c-1c42-4514-931f-cf06f8121ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdata(data, fpath = \"./trash.txt\", cfg_mode=cfg_mode):\n",
    "    if isinstance(data, str) or isinstance(data, list):\n",
    "        with open(fpath, \"a\") as f:\n",
    "            f.write(f\"{data} error!\\n\")        \n",
    "    elif cfg_mode==\"nciyes\":\n",
    "        device = data.y_batch.device\n",
    "        samples = []\n",
    "        shapes = []\n",
    "        ff = True\n",
    "\n",
    "        # check nci record in right_pocket\n",
    "        for i, (_right, _shape) in enumerate(zip(data.right_pocket_by_distance, data.pair_shape)):\n",
    "            if _right == True:\n",
    "                samples.append(i)\n",
    "                shapes.append(_shape)\n",
    "            samples = torch.tensor(samples).to(device)\n",
    "        index = torch.isin(data.y_batch, samples)\n",
    "        ss = data.nci_sequence[index]\n",
    "        if len(samples) and not (ss.sum()):\n",
    "            with open(fpath, \"a\") as f:\n",
    "                f.write(f\"{data.dataname} : its right pocket has no NCI record!\\n\")\n",
    "def checkdata2(data, fpath = \"./trash2.txt\", cfg_mode=cfg_mode):\n",
    "    if isinstance(data, str) or isinstance(data, list):\n",
    "        with open(fpath, \"a\") as f:\n",
    "            f.write(f\"{data} error!\\n\")        \n",
    "    elif cfg_mode==\"nciyes\":\n",
    "        device = data.y_batch.device\n",
    "        samples = []\n",
    "        shapes = []\n",
    "        ff = True\n",
    "\n",
    "        # check nci record in right_pocket\n",
    "        for i, _right in enumerate(data.right_pocket_by_distance):\n",
    "            if _right == True:\n",
    "                samples.append(i)\n",
    "            samples = torch.tensor(samples).to(device)\n",
    "        index = torch.isin(data.y_batch, samples)\n",
    "        ss = data.nci_sequence[index]\n",
    "        if len(samples) and not (ss.sum()):\n",
    "            with open(fpath, \"a\") as f:\n",
    "                f.write(f\"{data.dataname} : its right pocket has no NCI record!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f467456f-53ab-4403-937f-308200f1d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_checkdata=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ce96b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ebf690-95c2-464d-a4d6-db72b6d50e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f66a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:00:54   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:3\"\n",
    "if cfg_mode == \"tankbind\":\n",
    "    from model import get_model\n",
    "    model = get_model(0, logging, device)\n",
    "    modelFile = \"../saved_models/self_dock.pt\"\n",
    "    model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "    \n",
    "elif cfg_mode == \"nciyes\":\n",
    "    from sx_model import sx_get_model\n",
    "    model = sx_get_model(0, logging, device, nciyes=True, margin=1, margin_weight=1, nci_weight=0, output_classes=2,\n",
    "                         class_weight=torch.tensor([1,1],dtype=torch.float32))\n",
    "    IaBNetFile = \"../saved_models/self_dock.pt\"\n",
    "    model.IaBNet.load_state_dict(torch.load(IaBNetFile, map_location=device))\n",
    "    \n",
    "elif cfg_mode == \"frag\":\n",
    "    from model_frag import get_model\n",
    "    model = get_model(0, logging, device)\n",
    "    modelFile = \"../saved_models/self_dock.pt\" #?\n",
    "    model.load_state_dict(torch.load(modelFile, map_location=device))#?\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0a0001-48e1-482f-a39c-304b937cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "if True:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--max_epoch\", type=int, default=20)\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=3)\n",
    "    parser.add_argument(\"--compound_name\", \n",
    "            choices=['PDL1','HPK1_3','HPK1_4','KRAS_G12D','MAT2A','SOS1','ALK', 'BRAF', 'BTK', 'CDK4', 'EGFR', 'FGFR1', 'JAK2', 'NTRK1', 'VEGFR2','PRMT5'])\n",
    "    parser.add_argument(\"--data_path\", type=str, default='data')\n",
    "    parser.add_argument(\"--init_model\", type=str, default=\"../saved_models/self_dock.pt\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=save_model_path)\n",
    "    parser.add_argument(\"--embedding_dir\", type=str, default='embedding')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--dropout_rate\", type=float, default=0)\n",
    "    parser.add_argument(\"--iter\", type=int, default=1)\n",
    "    parser.add_argument(\"--model_mode\", choices=['init', 'Halfbind', 'Tankbind'], default='Tankbind')\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d330515-674e-40bc-af35-f5c1d475c6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6265cb2dde4c3c8662f0ea04a8bba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [1/20], Loss: 14.4363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e1704cea884cb389fdc67e54d4bb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [2/20], Loss: 7.1349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5067ae79a54caca3675e4c1ad2a21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [3/20], Loss: 7.6094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961e4b633874c3b984223ebb727b7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [4/20], Loss: 5.1153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b642b44eacb74bf8b0975803eb9d1f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [5/20], Loss: 3.0080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbde8eb1b1cd4d2180a52190c6fc4dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [6/20], Loss: 2.1192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29963e9dcd474ca862a0ae9217975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [7/20], Loss: 2.3901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e6f2ebb364a39a98360a11d7fd28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [8/20], Loss: 2.5101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269198913e12418da1e1b2898923396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [9/20], Loss: 0.3307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1b76bc44bb4d0895551c0f63eabade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [10/20], Loss: 0.3783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c068b0f86a004f5990fd9398bccf0f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [11/20], Loss: 0.7120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b950375ca747e7af0f4c392ab464b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [12/20], Loss: 0.3932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5856850cc4d440386c77a661844cef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN----> Epoch [13/20], Loss: 0.4029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd0a53323a84b498a24afc2d60bbe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "train_data_loader = DataLoader(train_set, batch_size=args.batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=True, num_workers=3)\n",
    "val_data_loader = DataLoader(val_set, batch_size=args.batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=3) if len(val_set) else None\n",
    "test_data_loader = DataLoader(test_set, batch_size=args.batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1) if len(test_set) else None\n",
    "test2_data_loader = DataLoader(test2_set, batch_size=args.batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1) if test2_set else None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "list_val_metric = []\n",
    "\n",
    "#from Collections import defaultdict\n",
    "\n",
    "nci_dict = {}\n",
    "\n",
    "######## start train\n",
    "if True:\n",
    "    for _epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        _list_loss = []\n",
    "        _list_nci = []\n",
    "        errors, error_file = [], f\"{main_path}train_{_epoch}_errors.txt\"\n",
    "        for i,data in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "            if cfg_checkdata:\n",
    "                checkdata(data)\n",
    "                if isinstance(data, str) or isinstance(data, list):\n",
    "                    errors.append(data)\n",
    "                    with open(error_file, \"a\") as f:\n",
    "                        f.write(\"\\n\"+str(data)+\"\\n\")    \n",
    "                    continue\n",
    "            data = data.to(device)\n",
    "            if cfg_mode == \"tankbind\": # Results : affinity_pred_dictbb\n",
    "                y_pred, affinity_pred = model(data)\n",
    "            elif cfg_mode == \"nciyes\": # Results : many\n",
    "                data = data.to(device)\n",
    "                y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                            data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _list_loss.append(loss.detach().cpu())\n",
    "                _list_nci.append(nci_pred.detach().cpu())\n",
    "        \n",
    "        nci_dict[f\"train_{_epoch}\"] = _list_nci\n",
    "        \n",
    "        losst = torch.cat([torch.tensor([i]) for i in _list_loss])\n",
    "        losst = torch.mean(losst, dim=0)\n",
    "        \n",
    "        print('TRAIN----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst.item()))\n",
    "        model.eval()\n",
    "        \n",
    "        if val_data_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                _list_loss_va = []\n",
    "                _list_nci = []\n",
    "                for data in tqdm(val_data_loader):\n",
    "                    data = data.to(device)\n",
    "                    if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                    elif cfg_mode == \"nciyes\": # Results : many\n",
    "                        y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                        loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                    data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                        _list_loss_va.append(loss)\n",
    "                        _list_nci.append(nci_pred)\n",
    "                losst_va = torch.cat([torch.tensor([i]) for i in _list_loss_va])\n",
    "                losst_va = torch.mean(losst_va, dim=0)\n",
    "                \n",
    "                nci_dict[f\"val_{_epoch}\"] = _list_nci\n",
    "                print('VALID----> Epoch [{}/{}], Loss: {:.4f}' .format(_epoch+1, args.max_epoch, losst_va.item()))\n",
    "                state = {'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch': _epoch}\n",
    "                model_dir = args.model_dir + '/%s/lr%s-batchsize%s-%s' % (args.iter, args.lr, args.batch_size, args.model_mode)\n",
    "                if not os.path.exists(model_dir):\n",
    "                    os.system(f\"mkdir -p {model_dir}\")\n",
    "                epoch_model_dir = '%s/epoch-%s' % (model_dir, _epoch + 1)\n",
    "                torch.save(state, epoch_model_dir)\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        ## TODO 定义一个score用来评估\n",
    "        \n",
    "        if test_data_loader is not None:\n",
    "            _list_loss_te = []\n",
    "            _list_nci = []\n",
    "            for data in tqdm(test_data_loader):\n",
    "                data = data.to(device)\n",
    "                if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        data = data.to(device)\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                elif cfg_mode == \"nciyes\": # Results : many\n",
    "                    y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                    loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                    _list_loss_te.append(loss)\n",
    "                    _list_nci.append(nci_pred)\n",
    "            \n",
    "            nci_dict[f\"test_{_epoch}\"] = _list_nci\n",
    "            losst_te = torch.cat([torch.tensor([i]) for i in _list_loss_te])\n",
    "            losst_te = torch.mean(losst_te, dim=0)\n",
    "            print('TEST----> loss: {}' .format(losst_te))\n",
    "        \n",
    "        if test2_data_loader is not None:\n",
    "            _list_loss_te_2 = []\n",
    "            for data in tqdm(test2_data_loader):\n",
    "                if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "                        data = data.to(device)\n",
    "                        y_pred, affinity_pred = model(data)\n",
    "                elif cfg_mode == \"nciyes\": # Results : many\n",
    "                    y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "                    loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                                data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "                    _list_loss_te_2.append(loss)\n",
    "            \n",
    "            losst_te_2 = torch.cat([torch.tensor([i]) for i in _list_loss_te_2])\n",
    "            losst_te_2 = torch.mean(losst_te_2, dim=0)\n",
    "            print('TEST2----> loss: {}' .format(losst_te_2))\n",
    "\n",
    "   \n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d50091-168f-4f3f-be9d-3164d76f480d",
   "metadata": {},
   "source": [
    "🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fe936-012c-452d-b659-b22ccebd6d09",
   "metadata": {},
   "source": [
    "㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090a42f-2ebd-46ad-b6e7-53b6bc6dcabb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f481-f689-47e2-b00f-3ad1ddf46d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if cfg_mode == \"tankbind\":\n",
    "    \n",
    "elif cfg_mode == \"nciyes\":\n",
    "    with open(f\"{main_path}errors.pkl\", \"wb\") as f:\n",
    "        pickle.dump(errors, f)\n",
    "    with open(f\"{main_path}y_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_true_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_true_dict, f)\n",
    "    with open(f\"{main_path}affinity_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(affinity_pred_dict, f)\n",
    "    with open(f\"{main_path}data_name_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_name_dict, f)    \n",
    "    with open(f\"{main_path}dis_map_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dis_map_dict, f)\n",
    "    with open(f\"{main_path}loss_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(loss_dict, f)\n",
    "    with open(f\"{main_path}right_pocket_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(right_pocket_dict, f)\n",
    "elif cfg_mode == \"frag\":\n",
    "    #TODO: write your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa32fcf-5871-4540-99a4-f617933f24d2",
   "metadata": {},
   "source": [
    "|CODE|EPOCH|TP|TN|FP|FN|Preci.%|Recall%|Notes\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "|NCIYes-test00-08150437|val_0|#25175|4490395|1414547|557|5.58|97.83|weights: 1, 500|\n",
    "|NCIYes-test00-08150549|val_0|9325|2553748|398723|3541|22.85|72.47|weights: 1, 100|\n",
    "|NCIYes-test00-08150616|val_0|0|2952471|0|12866|-|-|weights: 1, 50|\n",
    "|NCIYes-test00-08150700|val_0|10390|2496756|455715|2476|22.29|80.75|weights: 1, 80|\n",
    "|NCIYes-test00-08150700|val_0|18514|5197212|707730|7218|2.54|80.75|weights: 1, 80|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a37cb-2a2d-4ecb-aeb3-66ba560dd83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tankbind_py38]",
   "language": "python",
   "name": "conda-env-tankbind_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3642610297c2db0c2e515b5d6934e6fa60bcae4d2e973dd2c51cba19538092f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
