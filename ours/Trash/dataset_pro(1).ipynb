{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cecdf-0db2-416a-bc6f-ee88317c31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " class MyDataset_VS(Dataset):\n",
    "        def __init__(self, root, df_tr=None, df_te=None, df_va=None, df_te2=None, protein_dict=None, proteinMode=0, compoundMode=1,\n",
    "                    pocket_radius=20, shake_nodes=None, \n",
    "                    transform=None, pre_transform=None, pre_filter=None, generate_3D_conf = False,\n",
    "                    protein_res_id_dict=None, nci_df=None, ligand_atom_id_dict=None, cfg_mode=None,\n",
    "                    right_pocket_by_distance=True,\n",
    "                    ):\n",
    "            self.trainindx = 0\n",
    "            self.testindx = 0\n",
    "            self.valindx = 0\n",
    "            self.test2indx = 0\n",
    "            self.test_df = df_te\n",
    "            self.train_df = df_tr\n",
    "            self.val_df = df_va\n",
    "            self.test2_df = df_te2\n",
    "            self.testindx = len(self.test_df) \n",
    "            self.expected_test_count = len(self.test_df) \n",
    "            self.trainindx = len(self.train_df) +  self.testindx \n",
    "            self.expected_train_count = len(self.train_df) \n",
    "            self.valindx = len(self.val_df) + self.trainindx\n",
    "            self.expected_val_count = len(self.val_df)\n",
    "            self.test2indx = len(self.test2_df) + self.valindx \n",
    "            self.expected_test2_count = len(self.test2_df)\n",
    "            self.protein_dict = protein_dict\n",
    "            super().__init__(root, transform, pre_transform, pre_filter)\n",
    "            print(self.processed_paths)\n",
    "            self.data = torch.load(self.processed_paths[0])\n",
    "            self.protein_dict = torch.load(self.processed_paths[1])\n",
    "            self.nci_df=nci_df\n",
    "            self.protein_res_id_dict = protein_res_id_dict\n",
    "            self.ligand_atom_id_dict = ligand_atom_id_dict\n",
    "            self.proteinMode = proteinMode\n",
    "            self.pocket_radius = pocket_radius\n",
    "            self.compoundMode = compoundMode\n",
    "            self.shake_nodes = shake_nodes\n",
    "            self.generate_3D_conf = generate_3D_conf\n",
    "            self.cfg_mode = cfg_mode if cfg_mode else \"tankbind\"\n",
    "            self.right_pocket_by_distance=right_pocket_by_distance\n",
    "        @property\n",
    "        def processed_file_names(self):\n",
    "            return ['data.pt', 'protein.pt']\n",
    "\n",
    "        def process(self):\n",
    "            _data_total = pd.concat([self.test_df, self.train_df, self.val_df, self.test2_df]) \n",
    "            torch.save(_data_total, self.processed_paths[0])\n",
    "            torch.save(self.protein_dict, self.processed_paths[1])\n",
    "\n",
    "        def len(self):\n",
    "            return self.expected_test_count + self.expected_train_count \\\n",
    "                + self.expected_val_count + self.expected_test2_count\n",
    "\n",
    "        def get_idx_split(self):\n",
    "            return self.expected_test_count, \\\n",
    "                self.expected_test_count + self.expected_train_count, \\\n",
    "                self.expected_test_count + self.expected_train_count + self.expected_val_count, \\\n",
    "                self.expected_test_count + self.expected_train_count + self.expected_val_count + self.expected_test2_count\n",
    "\n",
    "        def get(self, idx):\n",
    "            line = self.data.iloc[idx]\n",
    "            canonique_smiles = line['canonique_smiles']\n",
    "            pocket_com = line['pocket_com']\n",
    "            pocket_com = np.array(pocket_com.split(\",\")).astype(float) if type(pocket_com) == str else pocket_com\n",
    "            pocket_com = pocket_com.reshape((1, 3))\n",
    "            use_whole_protein = line['use_whole_protein'] if \"use_whole_protein\" in line.index else False\n",
    "            protein_name = line['pdb_code']\n",
    "            pocket_name = line['pocket_name']\n",
    "            protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = self.protein_dict[protein_name]\n",
    "            ligand_fpath = line['ligand_fpath']\n",
    "            ligand_ftype = line['ligand_ftype']\n",
    "            ligand_name = line['ligand_name']\n",
    "            affinity = line['affinity']\n",
    "            if self.right_pocket_by_distance:\n",
    "                right_pocket_by_distance = line['right_pocket_by_distance']\n",
    "            if ligand_ftype == \".pdb\":\n",
    "                mol = Chem.MolFromPDBFile(ligand_fpath)\n",
    "            elif ligand_ftype == \".sdf\":\n",
    "                mol = Chem.MolFromMolFile(ligand_fpath)\n",
    "            mol.Compute2DCoords()  \n",
    "            if self.cfg_mode == \"tankbind\":\n",
    "                try:\n",
    "                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_II : \"+str(e)\n",
    "                try:\n",
    "                    data, input_node_list, keepNode = construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                        protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                        pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                        use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                    data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_III : \"+str(e)\n",
    "                try:\n",
    "                    data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                    data.affinity = affinity\n",
    "                    data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_IV : \"+str(e)\n",
    "            elif self.cfg_mode == \"nciyes\":\n",
    "                protein_res_ids = self.protein_res_id_dict[protein_name]\n",
    "                if (len(protein_res_ids.keys())-1) != protein_res_ids[list(protein_res_ids.keys())[-1]]:\n",
    "                    return protein_name+\" ERROR_I : protein_res_ids length error.\"\n",
    "                try:\n",
    "                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = sx_extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True, generate_3D_conf=False)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_II : \"+str(e)\n",
    "                    # y is distance map, instead of contact map.\n",
    "                try:\n",
    "                    data, input_node_list, keepNode = sx_construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                        protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                        pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                        use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_III : \"+str(e)\n",
    "                data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "                kept_res_ids = [_id for (_id, _keep) in zip(protein_res_ids, keepNode) if _keep]\n",
    "\n",
    "                try:\n",
    "                    atom_ids = self.ligand_atom_id_dict[protein_name]\n",
    "                    #data.nci_sequence = torch.Tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, res_full_id, atom_ids, self.nci_df).flatten())\n",
    "                    data.nci_sequence = torch.tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, kept_res_ids, atom_ids, self.nci_df).flatten())\n",
    "                    data.pair_shape = (len(kept_res_ids), len(atom_ids))\n",
    "                    data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                    data.affinity = affinity\n",
    "                    data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_IV : \"+str(e)\n",
    "            elif self.cfg_mode == \"frag\":\n",
    "                try:\n",
    "                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_II : \"+str(e)\n",
    "                try:\n",
    "                    data, input_node_list, keepNode = construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                        protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                        coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                        pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                        use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                    data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_III : \"+str(e)\n",
    "                try:\n",
    "                    data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                    data.affinity = affinity\n",
    "                    data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "                except Exception as e:\n",
    "                    return protein_name+\" ERROR_IV : \"+str(e)\n",
    "                #TODO: write your codes here. Refer to \"if self.cfg_mode==\"tankband\" above.\n",
    "\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98173d-7ff2-409a-96cf-9175905be4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_use_saved_files = False # If you have already generated all the data required.\n",
    "cfg_save_files = True # If you want to save generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321901f7-0d9a-4042-b5cf-5d03f5116ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qrint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#time\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mqrint\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     qrint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m generation or load\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qrint' is not defined"
     ]
    }
   ],
   "source": [
    "#time\n",
    "if True:\n",
    "    qrint(\"\\n\", r=False)\n",
    "    qrint(f\"{R}dataset{S} generation or load\")\n",
    "    qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "    qrint(\"\\n\", r=False)\n",
    "    qrint(f\"{R}info split{S} for dataset processing\")\n",
    "\n",
    "\n",
    "    if cfg_timesplit: # cfg_timesplit = True\n",
    "        qrint(f\"{R}Timesplit{S}: for dataset processing, the results will always be saved.\")\n",
    "        qrint(f\"Checking timesplit files：\")\n",
    "        if not os.path.exists(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_test.txt\"):\n",
    "            qrint(f\"Timesplit files not found. {R}Length split strategy{S} will be applied.\")\n",
    "            cfg_timesplit = False\n",
    "        else:\n",
    "            qrint(f\"Found timesplit files. {R}Timesplit{S} will be applied.\")\n",
    "            with open(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\", \"r\") as f:\n",
    "                _train = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "            with open(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\", \"r\") as f:\n",
    "                _val = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "            with open(f\"{save_files_path}timesplit_test.txt\", \"r\") as f:\n",
    "                _test = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "                \n",
    "            dataset_path = f\"{save_files_path}dataset/\"\n",
    "            qrint(f\"Processing {R}dataset{S}:\")\n",
    "            if not cfg_use_saved_files:\n",
    "                os.system(f\"rm -r {dataset_path}\")\n",
    "                for _s in [\"data\"]:\n",
    "                    os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "            info_test2 = info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))]\n",
    "            _total_set = MyDataset_VS(f\"{dataset_path}data/\", df_tr=info[info.pdb_code.isin(_train)],df_te=info[info.pdb_code.isin(_test)],df_va=info[info.pdb_code.isin(_val)], df_te2=info_test2, \\\n",
    "                                    protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "            _te, _tr, _va, _te2 = _total_set.get_idx_split()\n",
    "            test_set = _total_set[:_te]\n",
    "            train_set = _total_set[_te:_tr]\n",
    "            val_set = _total_set[_tr:_va]\n",
    "            test2_set = _total_set[_va:_te2]\n",
    "                \n",
    "    if not cfg_timesplit:\n",
    "        test2_set = None\n",
    "        train_part, val_part, test_part = cfg_split_strategy[0], cfg_split_strategy[1], cfg_split_strategy[2]\n",
    "        train_val_split, val_test_split = int(train_part * len(info)), int((train_part+val_part) * len(info))\n",
    "        qrint(f\"{R}Length% split strategy{S}: {train_val_split} - {val_test_split-train_val_split} - {len(info)-val_test_split}. For dataset processing, the results will always be saved.\")\n",
    "        dataset_path = f\"{save_files_path}dataset/\"\n",
    "        qrint(f\"Processing {R}dataset{S}:\")\n",
    "        if not cfg_use_saved_files:\n",
    "            os.system(f\"rm -r {dataset_path}\")\n",
    "            for _s in [\"data\"]:\n",
    "                os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "        _total_set = MyDataset_VS(f\"{dataset_path}data/\", df_tr=info.iloc[0:train_val_split], df_te=info.iloc[val_test_split:], df_va=info.iloc[train_val_split:val_test_split], df_te2=info_test2, \\\n",
    "                                    protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        _te, _tr, _va, _te2 = _total_set.get_idx_split()\n",
    "        test_set = _total_set[:_te]\n",
    "        train_set = _total_set[_te:_tr]\n",
    "        val_set = _total_set[_tr:_va]\n",
    "        test2_set = _total_set[_va:_te2]\n",
    "        \n",
    "    qrint(f\"Successfully processed {R}dataset{S}s.\")\n",
    "    qrint(f\"-- {R}train set{S} : {len(train_set)}\")\n",
    "    qrint(f\"-- {R}val set{S}   : {len(val_set)}\")\n",
    "    qrint(f\"-- {R}test set{S}  : {len(test_set)}\")\n",
    "    qrint(f\"-- {R}test2 set{S} : {len(test2_set) if test2_set else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeae1e0-5648-4bb7-8918-3ef85912e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_aff_score(self, aff_pred, aff_true):\n",
    "        return torch.sqrt((((aff_pred - aff_true)**2).sum(axis=-1)).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
