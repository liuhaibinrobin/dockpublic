{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23142276",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sx.nciyes.main.<font color=red>new</font>.ipynb\n",
    "\n",
    "Notebook for training and inference with updated NCI data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20a9fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07:18:46'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec45038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration and Initilization\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf3ac7",
   "metadata": {},
   "source": [
    "1st run:\n",
    "- set \"cfg_save_files = True\" and \"cfg_use_saved_files = False\" to process data and saved them.\n",
    "\n",
    "otherwise:\n",
    "- set \"cfg_save_files = False\" and \"cfg_use_saved_files = True\" to skip processing and load saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62c21caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to change settings below.\n",
    "cfg_use_saved_files = True # If you have already generated all the data required.\n",
    "cfg_save_files = False # If you want to save generated data.\n",
    "cfg_frag = False\n",
    "cfg_checkdata = True\n",
    "cfg_split_strategy = (1,0,0)\n",
    "mode_ls = [\"tankbind\", \"nciyes\", \"frag\"]\n",
    "\n",
    "cfg_mode = \"nciyes\" # \"frag\n",
    "cfg_data_version = \"v8.26\" #\"v9.1\"\n",
    "\n",
    "\n",
    "cfg_timesplit = True # If timesplit is applicated for splitting the dataset.\n",
    "\n",
    "cfg_custom_dir_name = \"Demo\" # Prefix in the name of the output folder.\n",
    "cfg_train = True # If you want to train the model.\n",
    "cfg_distinguish_by_timestamp = True # If true, a timestamp is added to your output dir name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476eabd",
   "metadata": {},
   "source": [
    "### Other Configuration and Initilization\n",
    "Under normal condition, you needn't modify this chapiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6443014-d883-4654-8db5-17a42526f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under normal condition, you should not modify the codes below.\n",
    "input_path = f\"./Inputs/{cfg_mode}/{cfg_data_version}/\"\n",
    "output_path = f\"./Outputs/{cfg_mode}/\"\n",
    "save_files_path = f\"./Inputs/Savedfiles/{cfg_mode}.{cfg_data_version}/\"\n",
    "p2rank_save_path = f\"./Inputs/Savedfiles/p2rank/\"\n",
    "p2rank_path = \"../p2rank_2.3/prank\"\n",
    "ds_path = \"../../../\" # Related path used for ds files.\n",
    "save_model_path = f\"./Inputs/Savedfiles/{cfg_mode}.model/\" #TODO: write your paths here or just keep this.\n",
    "pdb_df_fname = f\"Data.{cfg_data_version}.PDBs.csv\"\n",
    "ligand_df_fname = f\"Data.{cfg_data_version}.Ligands.csv\"\n",
    "pdb_df_fpath = f\"{input_path}{pdb_df_fname}\"\n",
    "ligand_df_fpath = f\"{input_path}{ligand_df_fname}\"\n",
    "datainfo = f\"{input_path}Datainfo.txt\"\n",
    "p2rank = f\"bash {p2rank_path}\"\n",
    "\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_fname = f\"Data.{cfg_data_version}.NCIs.csv\"\n",
    "    nci_df_fpath = f\"{input_path}{nci_fname}\"\n",
    "    \n",
    "# Under normal condition, you should not modify the codes below.\n",
    "cfg_jupyter = True # If you're using jupyter notebook. # old\n",
    "cfg_right_pocket_by_minor_distance = True # If the right pocket is chosen by calculating distance between ligand center and pocket center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9110acb0-dd78-451e-811b-4a7a134ac29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Inputs/nciyes/v8.26/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46bd5d4c-1b4f-4264-b511-12f5e12ea65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to \u001b[1;34m./Outputs/nciyes/Demo_22-08-31_0723/\u001b[0m.\n",
      "\n",
      "Data version: v8.26\n",
      "PDBs: 1, Ligands: 1, NCIs: 20\n",
      "\n",
      "\n",
      "Loaded \u001b[1;31mpdb_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.PDBs.csv\u001b[0m with length 1\n",
      "Loaded \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.Ligands.csv\u001b[0m with length 1\n",
      "Loaded \u001b[1;31mnci_df\u001b[0m from \u001b[1;34m./Inputs/nciyes/v8.26/Data.v8.26.NCIs.csv\u001b[0m with length 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from Bio.PDB import PDBParser\n",
    "import re\n",
    "sys.path.insert(0, \"../tankbind/\")\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    pass\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_feature_utils import sx_get_protein_feature, get_clean_res_list\n",
    "    from feature_utils import get_canonical_smiles\n",
    "elif cfg_mode == \"frag\":\n",
    "    pass\n",
    "    #TODO: if you want to import some functions here.\n",
    "\n",
    "if cfg_jupyter:\n",
    "    from tqdm.notebook import tqdm\n",
    "    R, B, S = \"\\033[1;31m\", \"\\033[1;34m\", \"\\033[0m\" \n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    R, B, S = \"\", \"\", \"\" \n",
    "\n",
    "_dirname = \"\"\n",
    "_dirname = (_dirname + cfg_custom_dir_name) if cfg_custom_dir_name else (time.strftime(\"%y-%m-%d_%H%M\"))\n",
    "_dirname = (_dirname + \"_\" + time.strftime(\"%y-%m-%d_%H%M\")) if (cfg_custom_dir_name and cfg_distinguish_by_timestamp) else _dirname\n",
    "main_path = f\"{output_path}{_dirname}/\"\n",
    "log_fpath = f\"{main_path}log.txt\"\n",
    "if os.path.exists(log_fpath):\n",
    "    os.system(f\"rm -r {log_fpath}\")\n",
    "if os.path.exists(main_path):\n",
    "    os.system(f\"rm -r {main_path}\")\n",
    "for _path in [save_files_path, save_model_path, main_path]:\n",
    "    os.system(f\"mkdir -p {_path}\")\n",
    "\n",
    "def qrint(target, jupyter= cfg_jupyter, log=log_fpath, R=R, B=B, S=S, r=True):\n",
    "    if r:\n",
    "        print(target)\n",
    "    if jupyter:\n",
    "        target = target.replace(R,\"\").replace(B,\"\").replace(S,\"\")\n",
    "    with open(log_fpath, \"a\") as f:\n",
    "        if target != \"\\n\":\n",
    "            f.write(time.strftime(\"[%m-%d %H:%M:%S] \")+target.replace(\"\\n\", \"                 \\n\")+\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "if cfg_mode == \"nci_yes\":\n",
    "    def get_full_id_old(full_id_ls: list, resname):\n",
    "        chain_id = full_id_ls[2]\n",
    "        res_id = full_id_ls[3][1]\n",
    "        return chain_id + \"_\" + str(res_id)+\"_\"+resname\n",
    "\n",
    "def saveconfig(cfg_use_saved_files, cfg_save_files, B=B, R=R, S=S):\n",
    "    qrint(f\"{R}cfg_use_saved_files{S}={B}{cfg_use_saved_files}{S}, {R}cfg_save_files{S}={B}{cfg_save_files}{S}.\")\n",
    "    if cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "    elif not cfg_use_saved_files and cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes and {B}save{S} results.\")\n",
    "    elif not cfg_use_saved_files and not cfg_save_files:\n",
    "        return(f\"{B}Run{S} codes but results {B}won't be saved{S}.\")\n",
    "    else:\n",
    "        return(f\"{R}CONFIG WARNING{R}: {B}Skip{S} codes and {B}use saved files{S}.\")\n",
    "        \n",
    "qrint(f\"Results will be saved to {B}{main_path}{S}.\\n\")\n",
    "\n",
    "with open(datainfo,\"r\") as f:\n",
    "    _ls = [k.replace(\"\\n\",\"\") for k in f.readlines()]\n",
    "    for _l in _ls:\n",
    "        qrint(_l)\n",
    "    qrint(\"\\n\")\n",
    "    \n",
    "pdb_df = pd.read_csv(pdb_df_fpath, index_col=0)\n",
    "ligand_df = pd.read_csv(ligand_df_fpath, index_col=0)\n",
    "pdb_code_list = list(pdb_df[\"pdb_code\"])\n",
    "pdb_fpath_list = list(pdb_df[\"pdb_fpath\"])\n",
    "qrint(f\"Loaded {R}pdb_df{S} from {B}{pdb_df_fpath}{S} with length {len(pdb_df)}\")\n",
    "qrint(f\"Loaded {R}ligand_df{S} from {B}{ligand_df_fpath}{S} with length {len(ligand_df)}\")\n",
    "if cfg_mode == \"nciyes\":\n",
    "    nci_df = pd.read_csv(nci_df_fpath, index_col=0)\n",
    "    qrint(f\"Loaded {R}nci_df{S} from {B}{nci_df_fpath}{S} with length {len(nci_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62774b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb78b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get protein features: <font color=\"red\">protein_dict</font> (& <font color=\"red\">protein_res_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4e6c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/protein_dicts/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1e236b3c394aa9802ba3cc592024a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded \u001b[1;31mprotein_dict\u001b[0m and \u001b[1;31mprotein_res_id_dict\u001b[0m.\n",
      "CPU times: user 32.4 ms, sys: 15.7 ms, total: 48.1 ms\n",
      "Wall time: 210 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}protein_dict{S} and {R}protein_res_id_dict{S}\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "protein_dict = {}\n",
    "protein_res_id_dict = {} if cfg_mode==\"nciyes\" else None\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Processing {R}protein_dict{S}\")\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    protein_dicts = [{} for n in range(11)] # 0,1,2,3,4,5,6,7,8,9,x\n",
    "    #protein_res_id_dict = [{} for n in range(11)]  #0,1,2,3,4,5,6,7,8,9,x\n",
    "    \n",
    "    for i, (_pname, _fpath) in tqdm(enumerate(zip(pdb_code_list, pdb_fpath_list)), total=len(pdb_fpath_list)):\n",
    "        s = parser.get_structure(_pname, _fpath)\n",
    "        res_list = list(s.get_residues())\n",
    "        clean_res_list = get_clean_res_list(res_list, ensure_ca_exist=True)\n",
    "        clean_res_full_id_list = [get_full_id_old(x.full_id, x.get_resname()) for x in clean_res_list] if (cfg_mode == \"nciyes\") else None\n",
    "\n",
    "        if (cfg_mode == \"tankbind\"):\n",
    "            _protein_dict = get_protein_feature(clean_res_list)\n",
    "        elif (cfg_mode == \"nciyes\"):\n",
    "            _protein_dict, _protein_res_id_dict = sx_get_protein_feature(clean_res_list, clean_res_full_id_list)\n",
    "        elif (cfg_mode == \"frag\"):\n",
    "            #TODO:write your function here\n",
    "            _protein_dict = get_protein_feature(clean_res_list)\n",
    "            \n",
    "        ind = _pname[0]\n",
    "        ind = int(ind) if str.isdigit(ind) else 10\n",
    "\n",
    "        protein_dicts[ind][_pname] = _protein_dict\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            protein_res_id_dict[_pname] = _protein_res_id_dict\n",
    "            \n",
    "    for _d in protein_dicts:\n",
    "        protein_dict.update(_d)\n",
    "    protein_dicts = None\n",
    "    \n",
    "    \n",
    "    if cfg_save_files:\n",
    "        os.system(f\"mkdir -p {save_files_path}protein_dicts/\")\n",
    "        qrint(f\"Saving {R}protein_dict{S} and {R}protein_res_id_dict{S}:\" if (cfg_mode == \"nciyes\") else f\"Saving {R}protein_dict{S}:\")\n",
    "        for i in tqdm(range(11), total=11):\n",
    "            with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_dicts[i], f)\n",
    "        if (cfg_mode == \"nciyes\"):\n",
    "            with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"wb\") as f:\n",
    "                pickle.dump(protein_res_id_dict, f)\n",
    "    qrint(f\"Successfully processed and saved {R}protein_dict{S} and {R}protein_res_id_dict{S} to {B}{save_files_path}protein_dicts/{S}.\" if (cfg_mode == \"nciyes\") \n",
    "          else f\"Successfully processed and saved {R}protein_dict{S} to {B}{save_files_path}protein_dicts/{S}.\")\n",
    "    \n",
    "else:\n",
    "    qrint(f\"Loading {R}protein_dict{S} and {R}protein_res_id_dict{S} from {B}{save_files_path}protein_dicts/{S}\" if (cfg_mode == \"nciyes\") \n",
    "         else f\"Loading {R}protein_dict{S} from {B}{save_files_path}protein_dicts/{S}\")\n",
    "    for i in tqdm(range(11), total=11):\n",
    "        with open(f\"{save_files_path}protein_dicts/dict_{str(i)}.pkl\",\"rb\") as f:\n",
    "            protein_dict.update(pickle.load(f))\n",
    "    with open(f\"{save_files_path}protein_dicts/res_dict.pkl\",\"rb\") as f:\n",
    "        protein_res_id_dict.update(pickle.load(f))\n",
    "    qrint(f\"Successfully loaded {R}protein_dict{S} and {R}protein_res_id_dict{S}.\" if (cfg_mode == \"nciyes\")\n",
    "         else f\"Successfully loaded {R}protein_dict{S}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ef2ed",
   "metadata": {},
   "source": [
    "### Segmentation of proteins by <font color=red>p2rank</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a6ff8-f905-4a7f-989f-3c796ba8b254",
   "metadata": {},
   "source": [
    "#### Generate or load <font color=\"red\">.ds</font> file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efdf01dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34m.ds\u001b[0m file generation\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mFalse\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mTrue\u001b[0m.\n",
      "\u001b[1;34mRun\u001b[0m codes and \u001b[1;34msave\u001b[0m results.\n",
      "Processing \u001b[1;31mprotein_list.ds\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './Outputs/nciyes/Demo_22-08-31_0723/protein_list.ds': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c012ed5969844839eaa82848bf89793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed \u001b[1;31mprotein_list.ds\u001b[0m at \u001b[1;34m./Outputs/nciyes/Demo_22-08-31_0723/protein_list.ds\u001b[0m\n",
      "Successfully saved \u001b[1;31mprotein_list.ds\u001b[0m to \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/protein_list.ds\u001b[0m\n",
      "CPU times: user 35.6 ms, sys: 3.81 ms, total: 39.4 ms\n",
      "Wall time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = False\n",
    "cfg_save_files = True\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}.ds{S} file generation\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    import shutil\n",
    "    qrint(f\"Processing {R}protein_list.ds{S}:\")\n",
    "    ds = f\"{main_path}protein_list.ds\"\n",
    "    os.system(f\"rm {ds}\")\n",
    "    with open(ds, \"w\") as out:\n",
    "        for _fpath in tqdm(pdb_fpath_list, total=len(pdb_fpath_list)):\n",
    "            out.write(f\"{ds_path}{_fpath}\\n\")\n",
    "    qrint(f\"Successfully processed {R}protein_list.ds{S} at {B}{main_path}protein_list.ds{S}\")\n",
    "    if cfg_save_files:\n",
    "        shutil.copy(ds, f\"{save_files_path}protein_list.ds\")\n",
    "        qrint(f\"Successfully saved {R}protein_list.ds{S} to {B}{save_files_path}protein_list.ds{S}\")\n",
    "else:\n",
    "    qrint(f\"Using existing {R}protein_list.ds{S} file at {B}{save_files_path}protein_list.ds{S}.\")\n",
    "    ds = f\"{save_files_path}protein_list.ds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac633ba3-4336-40e1-a238-679468c231d5",
   "metadata": {},
   "source": [
    "#### Generate or check <font color=\"red\">p2rank results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2054fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mP2RANK\u001b[0m: \u001b[1;34mp2rank\u001b[0m file generation or check\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Checking existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/p2rank/\u001b[0m:\n",
      "Existing p2rank files at \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/p2rank/\u001b[0m will be used.\n",
      "CPU times: user 10.9 ms, sys: 12 ms, total: 22.8 ms\n",
      "Wall time: 60.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfg_use_saved_files = True\n",
    "cfg_save_files = False\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}P2RANK{S}: {B}p2rank{S} file generation or check\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "_drop_p2rank = set()\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    if cfg_save_files:\n",
    "        #os.mkdir(f\"{save_files_path}p2rank\")\n",
    "        print(f\"Running p2rank with output dir {B}{p2rank_save_path}{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {p2rank_save_path} -threads 8\"\n",
    "    else:\n",
    "        os.system(f\"mkdir -p {main_path}p2rank/\")\n",
    "        print(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds} -o {main_path}p2rank/ -threads 8\"\n",
    "    os.system(cmd)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f\"Checking existing p2rank files at {B}{p2rank_save_path}{S}:\")\n",
    "    _p2rank_dirset = set(os.listdir(f\"{p2rank_save_path}\"))\n",
    "    _log = []\n",
    "    for _pdb in pdb_code_list:\n",
    "        if _pdb+\"_protein.pdb_predictions.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, prediction\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "        if _pdb+\"_protein.pdb_residues.csv\" not in _p2rank_dirset:\n",
    "            _log.append(f\"{_pdb}, residues\\n\")\n",
    "            _drop_p2rank.add(_pdb)\n",
    "    if len(_log) == 0:\n",
    "        qrint(f\"Existing p2rank files at {B}{save_files_path}p2rank/{S} will be used.\")\n",
    "        \n",
    "    else:\n",
    "        with open(f\"{main_path}log_absent_p2rank_pdbs.txt\", \"w\") as f:\n",
    "            f.writelines(_log)\n",
    "        qrint(f\"{len(_log)} files related to {len(_drop_p2rank)} proteins not found in {B}{save_files_path}p2rank/{S}.\")\n",
    "        qrint(f\"Information of abasent files saved to {B}{main_path}log_absent_p2rank_pdbs.txt{S}.\")\n",
    "        qrint(f\"Reprocessing {R}protein_list_absent.ds{S} for absent files:\")\n",
    "        ds2 = f\"{main_path}log_absent_p2rank_pdbs.txt\"\n",
    "        with open(ds, \"r\") as infile:\n",
    "            with open(ds2, \"w\") as out:\n",
    "                for _line in infile.readlines():\n",
    "                    if re.match(r\".+\\/(....)_protein.pdb\", _line).groups()[0] in _drop_p2rank:\n",
    "                        out.write(_line)\n",
    "        qrint(f\"Successfully processed {R}protein_list_absent.ds{S} at {B}{main_path}protein_list_absent.ds{S}\")\n",
    "        \n",
    "        qrint(f\"Running p2rank with output dir {B}{main_path}p2rank/{S}\")\n",
    "        cmd = f\"{p2rank} predict {ds2} -o {main_path}p2rank/ -threads 8\"\n",
    "        os.system(f\"mkdir -p {main_path}p2rank\")\n",
    "        os.system(cmd)\n",
    "        import shutil\n",
    "        qrint(f\"Copying generated files to {B}{save_files_path}p2rank/{S}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/\"):\n",
    "            if _f != \"params.txt\" and _f != \"visualizations\" and _f != \"run.log\":\n",
    "                shutil.copy(f\"{main_path}p2rank/{_f}\", f\"{p2rank_save_path}{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/\"):\n",
    "            if _f != \"data\":\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/{_f}\", f\"{p2rank_save_path}visualizations/{_f}\")\n",
    "        for _f in os.listdir(f\"{main_path}p2rank/visualizations/data\"):\n",
    "                shutil.copy(f\"{main_path}p2rank/visualizations/data/{_f}\", f\"{p2rank_save_path}visualizations/data/{_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce0876",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get ligand infomation: <font color=\"red\">ligand_df</font> (& <font color=\"red\">ligand_atom_id_dict</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84d67dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mligand_df\u001b[0m & \u001b[1;31mligand_atom_id_dict\u001b[0m processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31mligand_df\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/ligand_df.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_df\u001b[0m.\n",
      "Loading \u001b[1;31mligand_atom_id_dict\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/ligand_atom_id_dict.pkl\u001b[0m:\n",
      "Successfully loaded \u001b[1;31mligand_atom_id_dict\u001b[0m.\n",
      "CPU times: user 7.06 ms, sys: 2.47 ms, total: 9.53 ms\n",
      "Wall time: 76.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}ligand_df{S} & {R}ligand_atom_id_dict{S} processing\" if (cfg_mode==\"nciyes\") else f\"{R}ligand_df{S} processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "\n",
    "ligand_atom_id_dict = {} if (cfg_mode==\"nciyes\") else None\n",
    "    \n",
    "parser = PDBParser()\n",
    "if not cfg_use_saved_files:\n",
    "    print(f\"Processing {R}ligand_df{S}:\")\n",
    "    if cfg_mode == \"nciyes\":\n",
    "        ligand_name_list = list(ligand_df[\"ligand_name_ncifile\"])\n",
    "    ligand_pdb_list = list(ligand_df[\"pdb_code\"])\n",
    "    ligand_fpath_list = list(ligand_df[\"ligand_fpath\"])\n",
    "    canonique_smiles = []\n",
    "    error_list = []\n",
    "    for _fpath, _pdb in tqdm(zip(ligand_fpath_list, ligand_pdb_list), total=len(ligand_fpath_list)):\n",
    "        try:\n",
    "            canonique_smiles.append(get_canonical_smiles(Chem.MolToSmiles(Chem.MolFromPDBFile(_fpath))))\n",
    "        except Exception as e:\n",
    "            canonique_smiles.append(f\"ERROR - {str(e)}\")\n",
    "            error_list.append(_fpath)\n",
    "            continue\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            structure = parser.get_structure(\"pdb\", _fpath)[0]\n",
    "            _atom_ids = []\n",
    "            for chain in structure:\n",
    "                for residue in chain:\n",
    "                    for atom in residue.get_atoms():\n",
    "                        atom_id = atom.get_name()\n",
    "                        _atom_ids.append(atom_id)\n",
    "            ligand_atom_id_dict[_pdb] = {_atom:i for (_atom, i) in zip(_atom_ids, range(len(_atom_ids)))}\n",
    "    ligand_df[\"canonique_smiles\"] = canonique_smiles\n",
    "    qrint(f\"Successfully processed {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_save_files:\n",
    "        # Saving ligand_df\n",
    "        ligand_df.to_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "        qrint(f\"Successfully saved {R}ligand_df{S} to {B}{save_files_path}ligand_df.csv{S}.\")\n",
    "        if cfg_mode == \"nciyes\":\n",
    "            with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"wb\") as f:\n",
    "                pickle.dump(ligand_atom_id_dict, f)\n",
    "            qrint(f\"Successfully saved {R}ligand_atom_id_dict{S} to {B}{save_files_path}ligand_atom_id_dict.pkl{S}.\")\n",
    "\n",
    "else: # cfg_use_saved_files = True\n",
    "    qrint(f\"Loading {R}ligand_df{S} from {B}{save_files_path}ligand_df.csv{S}:\")\n",
    "    ligand_df = pd.read_csv(f\"{save_files_path}ligand_df.csv\")\n",
    "    qrint(f\"Successfully loaded {R}ligand_df{S}.\")\n",
    "    \n",
    "    if cfg_mode == \"nciyes\":\n",
    "        qrint(f\"Loading {R}ligand_atom_id_dict{S} from {B}{save_files_path}ligand_atom_id_dict.pkl{S}:\")\n",
    "        with open(f\"{save_files_path}ligand_atom_id_dict.pkl\", \"rb\") as f:\n",
    "            ligand_atom_id_dict = pickle.load(f)\n",
    "        qrint(f\"Successfully loaded {R}ligand_atom_id_dict{S}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86f99137",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_df_without_smiles = ligand_df[ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_without_smiles.to_csv(f\"{save_files_path}MDropped_1.Ligands_{len(ligand_df_without_smiles)} - SMILES Generation Error.csv\")\n",
    "ligand_df_with_smiles = ligand_df[~ligand_df.canonique_smiles.str.contains(\"ERROR\")]\n",
    "ligand_df_with_smiles.to_csv(f\"{save_files_path}ligand_df_with_SMILES.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ae143-5c65-4397-844c-1b92189d6224",
   "metadata": {},
   "source": [
    "### Get <font color = \"red\"> info </font> for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5671fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31minfo\u001b[0m for dataset processing\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "Loading \u001b[1;31minfo\u001b[0m from \u001b[1;34m./Inputs/Savedfiles/nciyes.v8.26/info.csv\u001b[0m:\n",
      "Successfully loaded \u001b[1;31minfo\u001b[0m. Length: \u001b[1;34m4\u001b[0m\n",
      "CPU times: user 9.83 ms, sys: 1.22 ms, total: 11 ms\n",
      "Wall time: 70.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info{S} for dataset processing\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "\n",
    "if not cfg_use_saved_files:\n",
    "    qrint(f\"Processing {R}info{S}:\")\n",
    "    info = []\n",
    "    error_info = []\n",
    "    for i,line in tqdm(ligand_df_with_smiles.iterrows(), total=ligand_df_with_smiles.shape[0]):\n",
    "        ligand_name = line[\"ligand_name_ncifile\"]\n",
    "        pdb_code = line[\"pdb_code\"]\n",
    "        pdb_fname = line[\"pdb_code\"] + \"_protein\"\n",
    "        canonique_smiles = line[\"canonique_smiles\"]\n",
    "        ligand_fpath = line[\"ligand_fpath\"]\n",
    "        ligand_ftype = os.path.splitext(os.path.split(ligand_fpath)[1])[1]\n",
    "        release_year = line[\"release_year\"]\n",
    "        affinity = line[\"affinity\"]\n",
    "        if cfg_save_files or cfg_use_saved_files:\n",
    "            p2rankFile = f\"{p2rank_save_path}{pdb_fname}.pdb_predictions.csv\"\n",
    "        else:\n",
    "            p2rankFile = f\"{main_path}p2rank/{pdb_fname}.pdb_predictions.csv\"\n",
    "                    \n",
    "        pocket_df = pd.read_csv(p2rankFile)\n",
    "        pocket_df.columns = pocket_df.columns.str.strip()\n",
    "        pocket_coms = pocket_df[[\"center_x\", \"center_y\", \"center_z\"]].values\n",
    "        if len(pocket_coms) == 0:\n",
    "            error_info.append([pdb_code, \"p2rank file error\", ligand_name, ligand_fpath, release_year])\n",
    "            continue\n",
    "        if cfg_right_pocket_by_minor_distance:\n",
    "            coord = Chem.MolFromPDBFile(ligand_fpath).GetConformer().GetPositions()\n",
    "            coord = coord.sum(axis=0)/len(coord)\n",
    "            right_ith_pocket = ((pocket_coms-coord)**2).sum(axis=1).argmin()\n",
    "        for ith_pocket, _pocket_com in enumerate(pocket_coms):\n",
    "            _pocket_com = \",\".join([str(a.round(3)) for a in _pocket_com])\n",
    "            if cfg_right_pocket_by_minor_distance:\n",
    "                right_pocket_by_distance = (ith_pocket == right_ith_pocket)\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, affinity, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year, right_pocket_by_distance])\n",
    "            else:\n",
    "                info.append([ligand_name, pdb_code, canonique_smiles, ligand_fpath, ligand_ftype, f\"pocket_{ith_pocket+1}\", _pocket_com, release_year])\n",
    "            '''\n",
    "            if False:\n",
    "                #except Exception as e:\n",
    "                _pdb = line[\"pdb_code\"]\n",
    "                ligand_name = line[\"ligand_name_ncifile\"]\n",
    "                pdb_code = line[\"pdb_code\"]\n",
    "                pdb_fname = line[\"pdb_code\"] + \"_protein\"\n",
    "                canonique_smiles = line[\"canonique_smiles\"]\n",
    "                ligand_fpath = line[\"ligand_fpath\"]\n",
    "                ligand_ftype = os.path.splitext(os.path.split(ligand_fpath)[1])[1]\n",
    "                pocket_df = pd.read_csv(p2rankFile)\n",
    "                pocket_df.columns = pocket_df.columns.str.strip()\n",
    "                pocket_coms = pocket_df[[\"center_x\", \"center_y\", \"center_z\"]].values\n",
    "                affinity = line[\"affinity\"]\n",
    "                release_year = line[\"release_year\"]\n",
    "                #print(f\"Error with {_pdb}: {str(e)} : {ligand_name},{pdb_fname},{canonique_smiles},{ligand_fpath},{ligand_ftype},{pocket_coms},{affinity}\")\n",
    "                error_info.append([_pdb, str(e), ligand_name, pdb_fname, canonique_smiles, ligand_fpath, ligand_ftype, pocket_coms, affinity, release_year])\n",
    "            '''\n",
    "    info = pd.DataFrame(info, columns = [\"ligand_name\", \"pdb_code\", \"canonique_smiles\", \n",
    "                                        \"ligand_fpath\", \"ligand_ftype\", \"affinity\", \"pocket_name\", \"pocket_com\", \"release_year\", \"right_pocket_by_distance\"])\n",
    "    error_info = pd.DataFrame(error_info, columns = [\"pdb_code\", \"info\", \"ligand_name\", \"ligand_fpath\", \"release_year\"])\n",
    "    qrint(f\"Successfully processed {R}info{S}.\")\n",
    "    \n",
    "    # Remove recorded bad info (可能没用，可以不管）\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "    info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    if cfg_save_files:\n",
    "        info.to_csv(f\"{save_files_path}info.csv\")\n",
    "        qrint(f\"Successfully saved {R}info{S} to {B}{save_files_path}info.csv{S}.\")\n",
    "        error_info.to_csv(f\"{save_files_path}MDropped_2.PDBs - Info Generation Error.csv\")\n",
    "        qrint(f\"Successfully saved {R}error_info{S} to {B}{save_files_path}MDropped_2.PDBs - Info Generation Error.csv{S}.\")\n",
    "\n",
    "else:\n",
    "    qrint(f\"Loading {R}info{S} from {B}{save_files_path}info.csv{S}:\")\n",
    "    info = pd.read_csv(f\"{save_files_path}info.csv\", index_col = 0)\n",
    "    \n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\"):\n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Dropped_pocket.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_pocket.csv{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\"):\n",
    "        with open(f\"{save_files_path}MDropped_InModel - Dropped_protein.txt\", \"r\") as f:\n",
    "            _drop_proteins = f.readlines()\n",
    "        _drop_proteins = [_dp.replace(\"\\n\", \"\") for _dp in _drop_proteins]\n",
    "        info = info[~info[\"pdb_code\"].isin(_drop_proteins)]\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Dropped_protein.txt{S}.\")\n",
    "    if os.path.exists(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\"):    \n",
    "        _drop_pocket = pd.read_csv(f\"{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv\")\n",
    "        for i in range(len(_drop_pocket)):\n",
    "            line = _drop_pocket.iloc[i]\n",
    "            _d_pdb, _d_ligname, _d_pocket = line[\"pdb_code\"], line[\"ligand_name\"], line[\"pocket\"]\n",
    "            info = info.drop(info[(info.pdb_code == _d_pdb) & (info.ligand_name == _d_ligname) & (info.pocket_name == _d_pocket)].index)\n",
    "        qrint(f\"Removed bad input from {B}{save_files_path}MDropped_InModel - Incoherent Res Name in old NCI files.csv{S}.\")\n",
    "    \n",
    "    info = info.reset_index()\n",
    "    if \"index\" in info.columns:\n",
    "        del info[\"index\"]\n",
    "    \n",
    "    qrint(f\"Successfully loaded {R}info{S}. Length: {B}{len(info)}{S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d21cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3883313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch_geometric.data import Dataset\n",
    "from sx_utils import sx_construct_data_from_graph_gvp\n",
    "import rdkit.Chem as Chem    # conda install rdkit -c rdkit if import failure.\n",
    "from feature_utils import extract_torchdrug_feature_from_mol\n",
    "\n",
    "if cfg_mode == \"tankbind\":\n",
    "    from feature_utils import construct_data_from_graph_gvp\n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_feature_utils import sx_extract_torchdrug_feature_from_mol\n",
    "    from sx_new_utils import sx_ligand_dedocking\n",
    "    from sx_new_utils import sx_get_nci_matrix_by_dict\n",
    "elif cfg_mode == \"frag\":\n",
    "    pass\n",
    "    #TODO: import your functions here.\n",
    "\n",
    "#TODO: move all these import to the beginning of this notebook when all codes are finished.\n",
    "\n",
    "class MyDataset_VS(Dataset):\n",
    "    def __init__(self, root, data=None, protein_dict=None, proteinMode=0, compoundMode=1,\n",
    "                pocket_radius=20, shake_nodes=None, \n",
    "                transform=None, pre_transform=None, pre_filter=None, generate_3D_conf = False,\n",
    "                protein_res_id_dict=None, nci_df=None, ligand_atom_id_dict=None, cfg_mode=None,\n",
    "                right_pocket_by_distance=True,\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.protein_dict = protein_dict\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        print(self.processed_paths)\n",
    "        self.data = torch.load(self.processed_paths[0])\n",
    "        self.protein_dict = torch.load(self.processed_paths[1])\n",
    "        self.nci_df=nci_df\n",
    "        self.protein_res_id_dict = protein_res_id_dict\n",
    "        self.ligand_atom_id_dict = ligand_atom_id_dict\n",
    "        self.proteinMode = proteinMode\n",
    "        self.pocket_radius = pocket_radius\n",
    "        self.compoundMode = compoundMode\n",
    "        self.shake_nodes = shake_nodes\n",
    "        self.generate_3D_conf = generate_3D_conf\n",
    "        self.cfg_mode = cfg_mode if cfg_mode else \"tankbind\"\n",
    "        self.right_pocket_by_distance=right_pocket_by_distance\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt', 'protein.pt']\n",
    "\n",
    "    def process(self):\n",
    "        torch.save(self.data, self.processed_paths[0])\n",
    "        torch.save(self.protein_dict, self.processed_paths[1])\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        line = self.data.iloc[idx]\n",
    "        canonique_smiles = line['canonique_smiles']\n",
    "        pocket_com = line['pocket_com']\n",
    "        pocket_com = np.array(pocket_com.split(\",\")).astype(float) if type(pocket_com) == str else pocket_com\n",
    "        pocket_com = pocket_com.reshape((1, 3))\n",
    "        use_whole_protein = line['use_whole_protein'] if \"use_whole_protein\" in line.index else False\n",
    "        protein_name = line['pdb_code']\n",
    "        pocket_name = line['pocket_name']\n",
    "        protein_node_xyz, protein_seq, protein_node_s, protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v = self.protein_dict[protein_name]\n",
    "        ligand_fpath = line['ligand_fpath']\n",
    "        ligand_ftype = line['ligand_ftype']\n",
    "        ligand_name = line['ligand_name']\n",
    "        affinity = line['affinity']\n",
    "        \n",
    "        if self.right_pocket_by_distance:\n",
    "            right_pocket_by_distance = line['right_pocket_by_distance']\n",
    "        \n",
    "        if ligand_ftype == \".pdb\":\n",
    "            mol = Chem.MolFromPDBFile(ligand_fpath)\n",
    "        elif ligand_ftype == \".sdf\":\n",
    "            mol = Chem.MolFromMolFile(ligand_fpath)\n",
    "        mol.Compute2DCoords()  \n",
    "        \n",
    "        if self.cfg_mode == \"tankbind\":\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "            try:\n",
    "                data, input_node_list, keepNode = construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                      protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                      coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                      pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                      use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "                data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            try:\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"nciyes\":\n",
    "            protein_res_ids = self.protein_res_id_dict[protein_name]\n",
    "            if (len(protein_res_ids.keys())-1) != protein_res_ids[list(protein_res_ids.keys())[-1]]:\n",
    "                return protein_name+\" ERROR_I : protein_res_ids length error.\"\n",
    "            try:\n",
    "                coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list, pair_dis_distribution = sx_extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True, generate_3D_conf=False)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_II : \"+str(e)\n",
    "                # y is distance map, instead of contact map.\n",
    "            try:\n",
    "                data, input_node_list, keepNode = sx_construct_data_from_graph_gvp(protein_node_xyz, protein_seq, protein_node_s, \n",
    "                                    protein_node_v, protein_edge_index, protein_edge_s, protein_edge_v,\n",
    "                                    coords, compound_node_features, input_atom_edge_list, input_atom_edge_attr_list,\n",
    "                                    pocket_radius=self.pocket_radius, use_whole_protein=use_whole_protein, includeDisMap=True,\n",
    "                                    use_compound_com_as_pocket=False, chosen_pocket_com=pocket_com, compoundMode=self.compoundMode)\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_III : \"+str(e)\n",
    "            data.compound_pair = pair_dis_distribution.reshape(-1, 16)\n",
    "            kept_res_ids = [_id for (_id, _keep) in zip(protein_res_ids, keepNode) if _keep]\n",
    "\n",
    "            try:\n",
    "                atom_ids = self.ligand_atom_id_dict[protein_name]\n",
    "                #data.nci_sequence = torch.Tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, res_full_id, atom_ids, self.nci_df).flatten())\n",
    "                data.nci_sequence = torch.tensor(sx_get_nci_matrix_by_dict(protein_name, ligand_name, kept_res_ids, atom_ids, self.nci_df).flatten())\n",
    "                data.pair_shape = (len(kept_res_ids), len(atom_ids))\n",
    "                data.right_pocket_by_distance = right_pocket_by_distance\n",
    "                data.affinity = affinity\n",
    "                data.dataname = protein_name + \"_\" + ligand_name + \"_\" + pocket_name\n",
    "            except Exception as e:\n",
    "                return protein_name+\" ERROR_IV : \"+str(e)\n",
    "        \n",
    "        elif self.cfg_mode == \"frag\":\n",
    "            pass\n",
    "            #TODO: write your codes here. Refer to \"if self.cfg_mode==\"tankband\" above.\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e6972-8dca-4750-9ca3-f1cae581c58e",
   "metadata": {},
   "source": [
    "#### data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b384d-6b29-40a5-baf7-8ea893682cde",
   "metadata": {},
   "source": [
    "### datasets generation or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f329c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mdataset\u001b[0m generation or load\n",
      "\u001b[1;31mcfg_use_saved_files\u001b[0m=\u001b[1;34mTrue\u001b[0m, \u001b[1;31mcfg_save_files\u001b[0m=\u001b[1;34mFalse\u001b[0m.\n",
      "\u001b[1;34mSkip\u001b[0m codes and \u001b[1;34muse saved files\u001b[0m.\n",
      "\u001b[1;31minfo split\u001b[0m for dataset processing\n",
      "\u001b[1;31mLength% split strategy\u001b[0m: 4 - 0 - 0. For dataset processing, the results will always be saved.\n",
      "Processing \u001b[1;31mdataset\u001b[0m:\n",
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/train/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/train/processed/protein.pt']\n",
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/val/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/val/processed/protein.pt']\n",
      "['Inputs/Savedfiles/nciyes.v8.26/dataset/test/processed/data.pt', 'Inputs/Savedfiles/nciyes.v8.26/dataset/test/processed/protein.pt']\n",
      "Successfully processed \u001b[1;31mdataset\u001b[0ms.\n",
      "-- \u001b[1;31mtrain set\u001b[0m : 4\n",
      "-- \u001b[1;31mval set\u001b[0m   : 0\n",
      "-- \u001b[1;31mtest set\u001b[0m  : 0\n",
      "-- \u001b[1;31mtest2 set\u001b[0m : 0\n",
      "CPU times: user 13 ms, sys: 19.1 ms, total: 32.2 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}dataset{S} generation or load\")\n",
    "qrint(saveconfig(cfg_use_saved_files, cfg_save_files))\n",
    "qrint(\"\\n\", r=False)\n",
    "qrint(f\"{R}info split{S} for dataset processing\")\n",
    "\n",
    "\n",
    "if cfg_timesplit: # cfg_timesplit = True\n",
    "    qrint(f\"{R}Timesplit{S}: for dataset processing, the results will always be saved.\")\n",
    "    qrint(f\"Checking timesplit files：\")\n",
    "    if not os.path.exists(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\") or not os.path.exists(f\"{save_files_path}timesplit_test.txt\"):\n",
    "        qrint(f\"Timesplit files not found. {R}Length split strategy{S} will be applied.\")\n",
    "        cfg_timesplit = False\n",
    "    else:\n",
    "        qrint(f\"Found timesplit files. {R}Timesplit{S} will be applied.\")\n",
    "        with open(f\"{save_files_path}timesplit_train_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _train = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_val_no_lig_overlap.txt\", \"r\") as f:\n",
    "            _val = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "        with open(f\"{save_files_path}timesplit_test.txt\", \"r\") as f:\n",
    "            _test = [_t.replace(\"\\n\", \"\") for _t in f.readlines()]\n",
    "            \n",
    "        dataset_path = f\"{save_files_path}dataset/\"\n",
    "        qrint(f\"Processing {R}dataset{S}:\")\n",
    "        if not cfg_use_saved_files:\n",
    "            os.system(f\"rm -r {dataset_path}\")\n",
    "            for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "                os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "            info_test2 = info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))]\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=test2_info, protein_dict=protein_dict, \n",
    "                                     protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) if len(info_test2) \\\n",
    "                        else None        \n",
    "        else:\n",
    "            test2_set = MyDataset_VS(f\"{dataset_path}test2/\", data=info[~(info.pdb_code.isin(_train)|info.pdb_code.isin(_val)|info.pdb_code.isin(_test))], \n",
    "                                     protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode) \\\n",
    "                        if os.path.exists(f\"{dataset_path}test2/processed/data.pt\") else None\n",
    "        \n",
    "        train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info[info.pdb_code.isin(_train)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info[info.pdb_code.isin(_val)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "        test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info[info.pdb_code.isin(_test)], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "            \n",
    "if not cfg_timesplit:\n",
    "    test2_set = None\n",
    "    train_part, val_part, test_part = cfg_split_strategy[0], cfg_split_strategy[1], cfg_split_strategy[2]\n",
    "    train_val_split, val_test_split = int(train_part * len(info)), int((train_part+val_part) * len(info))\n",
    "    qrint(f\"{R}Length% split strategy{S}: {train_val_split} - {val_test_split-train_val_split} - {len(info)-val_test_split}. For dataset processing, the results will always be saved.\")\n",
    "    dataset_path = f\"{save_files_path}dataset/\"\n",
    "    qrint(f\"Processing {R}dataset{S}:\")\n",
    "    if not cfg_use_saved_files:\n",
    "        os.system(f\"rm -r {dataset_path}\")\n",
    "        for _s in [\"train\", \"val\", \"test\", \"test2\"]:\n",
    "            os.system(f\"mkdir -p {dataset_path}{_s}\")\n",
    "    train_set = MyDataset_VS(f\"{dataset_path}train/\", data=info.iloc[0:train_val_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    val_set = MyDataset_VS(f\"{dataset_path}val/\", data=info.iloc[train_val_split:val_test_split], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    test_set = MyDataset_VS(f\"{dataset_path}test/\", data=info.iloc[val_test_split:], protein_dict=protein_dict, protein_res_id_dict=protein_res_id_dict, nci_df=nci_df, ligand_atom_id_dict=ligand_atom_id_dict, cfg_mode=cfg_mode)\n",
    "    \n",
    "qrint(f\"Successfully processed {R}dataset{S}s.\")\n",
    "qrint(f\"-- {R}train set{S} : {len(train_set)}\")\n",
    "qrint(f\"-- {R}val set{S}   : {len(val_set)}\")\n",
    "qrint(f\"-- {R}test set{S}  : {len(test_set)}\")\n",
    "qrint(f\"-- {R}test2 set{S} : {len(test2_set) if test2_set else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8fa6733c-1c42-4514-931f-cf06f8121ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdata(data, fpath = \"./trash.txt\", cfg_mode=cfg_mode):\n",
    "    if isinstance(data, str) or isinstance(data, list):\n",
    "        with open(fpath, \"a\") as f:\n",
    "            f.write(f\"{data} error!\\n\")        \n",
    "    elif cfg_mode==\"nciyes\":\n",
    "        device = data.y_batch.device\n",
    "        samples = []\n",
    "        shapes = []\n",
    "        ff = True\n",
    "\n",
    "        # check nci record in right_pocket\n",
    "        for i, (_right, _shape) in enumerate(zip(data.right_pocket_by_distance, data.pair_shape)):\n",
    "            if _right == True:\n",
    "                samples.append(i)\n",
    "                shapes.append(_shape)\n",
    "            samples = torch.tensor(samples).to(device)\n",
    "        index = torch.isin(data.y_batch, samples)\n",
    "        ss = data.nci_sequence[index]\n",
    "        if len(samples) and not (ss.sum()):\n",
    "            with open(fpath, \"a\") as f:\n",
    "                f.write(f\"{data.dataname} : its right pocket has no NCI record!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ce96b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4ebf690-95c2-464d-a4d6-db72b6d50e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "device = 'cuda:3'\n",
    "num_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9f66a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:43:05   5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\n"
     ]
    }
   ],
   "source": [
    "if cfg_mode == \"tankbind\":\n",
    "    from model import get_model\n",
    "    model = get_model(0, logging, device)\n",
    "    modelFile = \"../saved_models/self_dock.pt\"\n",
    "    model.load_state_dict(torch.load(modelFile, map_location=device))\n",
    "    \n",
    "if cfg_mode == \"nciyes\":\n",
    "    from sx_model import sx_get_model\n",
    "    model = sx_get_model(0, logging, device, nciyes=True, margin=1, margin_weight=1, nci_weight=1, output_classes=2,\n",
    "                         class_weight=torch.tensor([1,1],dtype=torch.float32))\n",
    "    IaBNetFile = \"../saved_models/self_dock.pt\"\n",
    "    model.IaBNet.load_state_dict(torch.load(IaBNetFile, map_location=device))\n",
    "    \n",
    "elif cfg_mode == \"frag\":\n",
    "    #TODO: WRITE YOUR CODE HERE\n",
    "    #from your_model_file import your_model\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_ = model.eval()\n",
    "\n",
    "train_data_loader = DataLoader(train_set, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1)\n",
    "val_data_loader = DataLoader(val_set, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1)\n",
    "test_data_loader = DataLoader(test_set, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1)\n",
    "test2_data_loader = DataLoader(test2_set, batch_size=batch_size, follow_batch=['x', 'y', 'compound_pair'], shuffle=False, num_workers=1) if test2_set else None\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29bebf2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2d09444dd42ca819f4739ebb61910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Save results by epoch_name(defaultdict(list) with key assigned to the name of epoch. example: train_0, val_1)\n",
    "\n",
    "affinity_pred_dict = defaultdict(list)\n",
    "y_pred_dict = defaultdict(list)\n",
    "nci_pred_dict = defaultdict(list)\n",
    "nci_true_dict = defaultdict(list)\n",
    "dis_map_dict = defaultdict(list)\n",
    "loss_dict = defaultdict(list)\n",
    "right_pocket_dict = defaultdict(list)\n",
    "data_name_dict =defaultdict(list)\n",
    "y_batch_dict = defaultdict(list)\n",
    "\n",
    "for _epoch in range(num_epoch):\n",
    "    epoch_name = f\"train_{_epoch}\"\n",
    "    errors, error_file = [], f\"{main_path}train_{_epoch}_errors.txt\"\n",
    "    for i,data in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "        if cfg_checkdata:\n",
    "            checkdata(data)\n",
    "            if isinstance(data, str) or isinstance(data, list):\n",
    "                errors.append(data)\n",
    "                with open(error_file, \"a\") as f:\n",
    "                    f.write(\"\\n\"+str(data)+\"\\n\")    \n",
    "                continue\n",
    "        data = data.to(device)\n",
    "        if cfg_mode == \"tankbind\": # Results : affinity_pred_dict\n",
    "            data = data.to(device)\n",
    "            y_pred, affinity_pred = model(data)\n",
    "            affinity_pred_dict[epoch_name].append(affinity_pred.detach().cpu())\n",
    "\n",
    "\n",
    "        elif cfg_mode == \"nciyes\": # Results : many\n",
    "            y_pred, affinity_pred, nci_pred = model(data, i)\n",
    "            affinity_pred_dict[epoch_name].append(affinity_pred.detach().cpu())\n",
    "            y_pred_dict[epoch_name].append(y_pred.detach().cpu())\n",
    "            nci_pred_dict[epoch_name].append(nci_pred.detach().cpu())\n",
    "            nci_true_dict[epoch_name].append(data.nci_sequence.detach().cpu())\n",
    "            dis_map_dict[epoch_name].append(data.dis_map.detach().cpu())\n",
    "            right_pocket_dict[epoch_name].append(data.right_pocket_by_distance)\n",
    "            data_name_dict[epoch_name].append(data.dataname)\n",
    "            y_batch_dict[epoch_name].append(data.y_batch)\n",
    "            loss = model.calculate_loss(affinity_pred, y_pred, nci_pred, data.affinity, data.dis_map,\n",
    "                                        data.nci_sequence, data.right_pocket_by_distance, i, data.y_batch, data.pair_shape)\n",
    "            optimizer.zero_grad()\n",
    "            if loss is not None:\n",
    "                with open(error_file, \"a\") as f:\n",
    "                    f.write(str(i)+\", \")\n",
    "                loss_dict[epoch_name].append(float(loss))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "b                optimizer.step()\n",
    "            else:\n",
    "                with open(f\"error_data_{i}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(data, f)\n",
    "                loss_dict[epoch_name].append(\"Error\")\n",
    "                print(f\"Error with data: {data.dataname}\")\n",
    "\n",
    "        elif cfg_mode == \"frag\":\n",
    "            pass\n",
    "    #TODO: write your codes here.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d50091-168f-4f3f-be9d-3164d76f480d",
   "metadata": {},
   "source": [
    "🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏🙏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fe936-012c-452d-b659-b22ccebd6d09",
   "metadata": {},
   "source": [
    "㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️㊗️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090a42f-2ebd-46ad-b6e7-53b6bc6dcabb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f481-f689-47e2-b00f-3ad1ddf46d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if cfg_mode == \"tankbind\":\n",
    "    \n",
    "elif cfg_mode == \"nciyes\":\n",
    "    with open(f\"{main_path}errors.pkl\", \"wb\") as f:\n",
    "        pickle.dump(errors, f)\n",
    "    with open(f\"{main_path}y_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(y_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_pred_dict, f)\n",
    "    with open(f\"{main_path}nci_true_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nci_true_dict, f)\n",
    "    with open(f\"{main_path}affinity_pred_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(affinity_pred_dict, f)\n",
    "    with open(f\"{main_path}data_name_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_name_dict, f)    \n",
    "    with open(f\"{main_path}dis_map_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dis_map_dict, f)\n",
    "    with open(f\"{main_path}loss_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(loss_dict, f)\n",
    "    with open(f\"{main_path}right_pocket_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(right_pocket_dict, f)\n",
    "elif cfg_mode == \"frag\":\n",
    "    #TODO: write your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa32fcf-5871-4540-99a4-f617933f24d2",
   "metadata": {},
   "source": [
    "|CODE|EPOCH|TP|TN|FP|FN|Preci.%|Recall%|Notes\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "|NCIYes-test00-08150437|val_0|#25175|4490395|1414547|557|5.58|97.83|weights: 1, 500|\n",
    "|NCIYes-test00-08150549|val_0|9325|2553748|398723|3541|22.85|72.47|weights: 1, 100|\n",
    "|NCIYes-test00-08150616|val_0|0|2952471|0|12866|-|-|weights: 1, 50|\n",
    "|NCIYes-test00-08150700|val_0|10390|2496756|455715|2476|22.29|80.75|weights: 1, 80|\n",
    "|NCIYes-test00-08150700|val_0|18514|5197212|707730|7218|2.54|80.75|weights: 1, 80|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a37cb-2a2d-4ecb-aeb3-66ba560dd83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tankbind_py38]",
   "language": "python",
   "name": "conda-env-tankbind_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3642610297c2db0c2e515b5d6934e6fa60bcae4d2e973dd2c51cba19538092f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
