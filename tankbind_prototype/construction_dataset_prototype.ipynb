{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `重要`\n",
    "在克隆本 git 项目后，如果未按照现有方式挂载 `dataspace` 数据卷，则需要按情况将每一个代码块的 `processed=True` 修改为 `processed=False`，然后运行，以将处理后的数据保存至某个文件夹（原为`dataspace/Prototype`），并在`tankbind_prototype/datasets` 下生成训练、测试用数据集。\n",
    "若成功挂载 `dataspace`，请跳到本脚本的 「<font color='gold'>「从这里开始」</font>数据集构建」，并以该处开始构建数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "\n",
    "### Preperation | 路径准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "pdbbind_path = \"/home/jovyan/dataspace/PDBbind2020/all_pdbbind\"\n",
    "proto_path = \"/home/jovyan/dataspace/Prototype\"\n",
    "p2rank = \"bash /home/jovyan/TankBind/p2rank_2.3/prank\"\n",
    "tankbind_src_folder_path = \"../tankbind_prototype/\"\n",
    "import sys\n",
    "sys.path.insert(0, tankbind_src_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取表格和 `pdb` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    session_table = pd.read_csv(\"/home/jovyan/dataspace/Liyou/jingsen.csv\")\n",
    "\n",
    "    sample_id = list(range(len(session_table)))\n",
    "    sample_id = [\"ID_\"+str(_) for _ in sample_id]\n",
    "    session_table['sample_id'] = sample_id\n",
    "    \n",
    "    pdb_in_session = session_table.pdb_id.unique()\n",
    "    pdb_in_pdbbind = os.listdir(pdbbind_path)\n",
    "    for _ in tqdm(pdb_in_session):\n",
    "        if _.lower() not in pdb_in_pdbbind:\n",
    "            print(_, end=\" \")\n",
    "    # All PDBs in session table could be found in pdbbind2020\n",
    "    pdb_in_session = list(pdb_in_sessio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 遍历 PDBBind2020 数据库的 ligand 文件，为读取最佳口袋坐标做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    from feature_utils import read_mol\n",
    "    from rdkit import RDLogger\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    pdb_list = []\n",
    "    problem_list = []\n",
    "    for pdb in tqdm(pdb_in_session):\n",
    "        pdb = pdb.lower()\n",
    "        sdf_fileName = f\"{pdbbind_path}/{pdb}/{pdb}_ligand.sdf\"\n",
    "        mol2_fileName = f\"{pdbbind_path}/{pdb}/{pdb}_ligand.mol2\"\n",
    "        mol, problem = read_mol(sdf_fileName, mol2_fileName)\n",
    "        if problem:\n",
    "            problem_list.append(pdb)\n",
    "            continue\n",
    "        pdb_list.append(pdb)\n",
    "    print(problem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`problem_list = ['2foy', '2fou', '2fov', '3zp9', '1h07', '3dwb', '1a7x', '1qpf', '4qxs', '2ovz', '1hv5', '1dmt', '3fuc', '4g8l', '1tlp', '4dt6', '3bwf', '3ntp', '2e94', '4lkm', '3mi2', '3gep', '3ggj', '1bzy', '4bxk', '2oc2', '3kck', '3djx', '2pll', '5aqk', '5ipc', '5kly', '5hlm', '4l4l', '4jbs', '5kyk', '3bbb', '4e67', '3fvh', '4lkl', '4r07', '4loi', '3hik', '4e9d', '4e9c', '5km1', '5kma', '4k10']`\n",
    "\n",
    "针对这些蛋白，将读取文件坐标找到最佳口袋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备表格\n",
    "最终结果已导出为 `multiton_table.csv`，故前面的步骤都跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    session_table['session_au'] = session_table['assay_id'].astype(str) + \"_\" + session_table['uniprot_id']\n",
    "    session_count = session_table[['session_au', 'kekule_smiles', 'pdb_id']].groupby('session_au').nunique()\n",
    "    session_count.reset_index(inplace=True)\n",
    "\n",
    "    singleton_session = list(session_count[session_count.kekule_smiles==1].session_au)\n",
    "    multiton_session = list(session_count[session_count.kekule_smiles!=1].session_au)\n",
    "\n",
    "    ms_dict ={_:\"singleton\" for _ in singleton_session}\n",
    "    ms_dict.update({_:\"multiton\" for _ in multiton_session})\n",
    "\n",
    "    session_table['session_au_usability'] = session_table.session_au.apply(lambda x: ms_dict[x])\n",
    "    \n",
    "    \n",
    "    from feature_utils import generate_sdf_from_smiles_using_rdkit\n",
    "    multiton_table = session_table[session_table.session_au_usability==\"multiton\"]\n",
    "    multiton_table\n",
    "\n",
    "    kekule_smiles = list(multiton_table.kekule_smiles.unique())\n",
    "    len(kekule_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成 `smiles_name` 词典，以免直接用 `kekule_smiles` 命名文件导致的问题。\n",
    "\n",
    "读取 `smiles_name` 词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    import torch\n",
    "    smiles_dict = {}\n",
    "    for i, smiles in enumerate(kekule_smiles):\n",
    "        smiles_dict[smiles] = f\"smiles_{i}\"\n",
    "    import torch\n",
    "    torch.save(smiles_dict, \"/home/jovyan/dataspace/Prototype/dicts/FULL_smiles_dict.pt\")\n",
    "else:\n",
    "    smiles_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/FULL_smiles_dict.pt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 `kekule_smiles` 生成小分子文件。只需要执行一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    error_smiles = []\n",
    "    for i, smiles in tqdm(enumerate(kekule_smiles), total=len(kekule_smiles)):\n",
    "        try:\n",
    "            smiles_name = smiles_dict[smiles]\n",
    "            generate_sdf_from_smiles_using_rdkit(smiles=smiles, rdkitMolFile=f\"/home/jovyan/dataspace/Prototype/rdkit_generated_sdfs/{smiles_name}.sdf\", shift_dis=0)\n",
    "        except:\n",
    "            error_smiles.append((smiles, smiles_dict[smiles]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清理错误的样本，并保存最后的 `multiton_table`。过程只需要执行一次。\n",
    "```\n",
    "error_smiles = [\"smiles_12096\",  \"smiles_12097\", \"smiles_12098\", \"smiles_37527\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89335/2615119571.py:42: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  multiton_table = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/multiton_table_with_smiles_name.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    smiles_files = os.listdir(\"/home/jovyan/dataspace/Prototype/rdkit_generated_sdfs\")\n",
    "    smiles_files = [_.replace(\".sdf\", \"\") for _ in smiles_files]\n",
    "\n",
    "    error_smiles = []\n",
    "    for _ in tqdm(kekule_smiles):\n",
    "        if smiles_dict[_] not in smiles_files:\n",
    "            error_smiles.append((_, smiles_dict[_]))\n",
    "\n",
    "    real_error_smiles = []\n",
    "    print(\"LENGTH\", len(error_smiles))\n",
    "    a = 0\n",
    "    for i, error in tqdm(enumerate(error_smiles), total=len(error_smiles)):\n",
    "        smiles, smiles_name = error\n",
    "        try:\n",
    "            generate_sdf_from_smiles_using_rdkit(smiles=smiles, rdkitMolFile=f\"/home/jovyan/dataspace/Prototype/rdkit_generated_sdfs/{smiles_name}.sdf\", shift_dis=0)\n",
    "        except:\n",
    "            a += 1\n",
    "            print(smiles_name, end=\" \")\n",
    "            real_error_smiles.append((smiles, smiles_dict[smiles]))\n",
    "    print(\"\\nLENGTH\", a)\n",
    "\n",
    "\n",
    "    all_smiles_file = os.listdir(\"/home/jovyan/dataspace/Prototype/rdkit_generated_sdfs\")\n",
    "    all_smiles_file = [_.replace(\".sdf\", \"\") for _ in all_smiles_file]\n",
    "\n",
    "\n",
    "    for _ in tqdm(kekule_smiles):\n",
    "        if smiles_dict[_] not in all_smiles_file:\n",
    "            print(smiles_dict[_])\n",
    "\n",
    "\n",
    "    multiton_table['kekule_smiles_name'] = multiton_table.kekule_smiles.apply(lambda x: smiles_dict[x])\n",
    "\n",
    "\n",
    "    error_smiles = [\"smiles_12096\",  \"smiles_12097\", \"smiles_12098\", \"smiles_37527\"]\n",
    "\n",
    "\n",
    "    multiton_table = multiton_table[~multiton_table.kekule_smiles_name.isin(error_smiles)]\n",
    "    multiton_table.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_multiton_table_with_smiles_name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 `smiles_dict.pt`，读取 `multiton_table.csv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90545/2114790264.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  multiton_table = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/multiton_table_with_smiles_name.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    smiles_dict = torch.load(\"/home/jovyan/dataspace/Prototype/FULL_smiles_dict.pt\")\n",
    "    multiton_table = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_multiton_table_with_smiles_name.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 蛋白质：口袋切分与特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 `p2rank` 生成口袋「略」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if False:\n",
    "    p2rank_prediction_folder = \"/home/jovyan/dataspace/Prototype/p2rank_results\"\n",
    "    os.system(f\"mkdir -p {p2rank_prediction_folder}\")\n",
    "    ds = f\"{p2rank_prediction_folder}/protein_list.ds\"\n",
    "    with open(ds, \"w\") as out:\n",
    "        for pdb in ##TODO: protein_names\n",
    "            out.write(f\"../{TODO: foldernames}/{pdb}_protein.pdb\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查已有的 `p2rank` 结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 `multiton_table.csv` 得到当前的 `pdb_list`。\n",
    "\n",
    "检查是否所有的 `pdb` 口袋都能在 `p2rank` 输出文件夹中找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/9092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2RANK YES.\n"
     ]
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    pdb_list = [_.lower() for _ in list(multiton_table.pdb_id.unique())]\n",
    "    p2rank_prediction_folder = \"/home/jovyan/dataspace/Prototype/p2rank_results\"\n",
    "\n",
    "    _ = os.listdir(p2rank_prediction_folder)\n",
    "    pdb_without_p2rank_result = []\n",
    "    for pdb in tqdm(pdb_list):\n",
    "        if f\"{pdb.lower()}_protein.pdb_predictions.csv\" in _:\n",
    "            continue\n",
    "        else:\n",
    "            pdb_without_p2rank_result.append(pdb)\n",
    "            print(pdb, end=\", \")\n",
    "    if pdb_without_p2rank_result == []:\n",
    "        print(\"P2RANK YES.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  准备 `protein_features`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/9092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    tankbind_data_path = f\"/home/jovyan/dataspace/Prototype/tankbind_data\"\n",
    "    os.system(f\"mkdir -p {tankbind_data_path}\")\n",
    "    pdb_list = [_.lower() for _ in list(multiton_table.pdb_id.unique())]\n",
    "    d_list = []\n",
    "\n",
    "    for name in tqdm(pdb_list):\n",
    "        p2rankFile = f\"{p2rank_prediction_folder}/{name}_protein.pdb_predictions.csv\"\n",
    "        d = pd.read_csv(p2rankFile)\n",
    "        d.columns = d.columns.str.strip()\n",
    "        d_list.append(d.assign(name=name))\n",
    "    d = pd.concat(d_list).reset_index(drop=True)\n",
    "    d.reset_index(drop=True).to_feather(f\"{tankbind_data_path}/p2rank_result.feather\")\n",
    "\n",
    "    tankbind_data_path = f\"/home/jovyan/dataspace/Prototype/tankbind_data\"\n",
    "    d = pd.read_feather(f\"{tankbind_data_path}/p2rank_result.feather\")\n",
    "\n",
    "    pockets_dict = {}\n",
    "    for name in tqdm(pdb_list):\n",
    "        pockets_dict[name] = d[d.name == name].reset_index(drop=True)\n",
    "        \n",
    "    torch.save(pockets_dict, \"/home/jovyan/dataspace/Prototype/dicts/FULL_pockets_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    from feature_utils import get_protein_feature\n",
    "\n",
    "    input_ = []\n",
    "    protein_embedding_folder = f\"{tankbind_data_path}/gvp_protein_embedding\"\n",
    "    os.system(f\"mkdir -p {protein_embedding_folder}\")\n",
    "    for pdb in tqdm(pdb_list):\n",
    "        proteinFile = f\"{pdbbind_path}/{pdb}/{pdb}_protein.pdb\"\n",
    "        toFile = f\"{protein_embedding_folder}/{pdb}.pt\"\n",
    "        x = (pdb, proteinFile, toFile)\n",
    "        input_.append(x)\n",
    "        \n",
    "\n",
    "    from Bio.PDB import PDBParser\n",
    "    from feature_utils import get_clean_res_list\n",
    "    import torch\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    def batch_run(x):\n",
    "        protein_dict = {}\n",
    "        pdb, proteinFile, toFile = x\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        s = parser.get_structure(pdb, proteinFile)\n",
    "        res_list = get_clean_res_list(s.get_residues(), verbose=False, ensure_ca_exist=True)\n",
    "        protein_dict[pdb] = get_protein_feature(res_list)\n",
    "        torch.save(protein_dict, toFile)\n",
    "        \n",
    "\n",
    "    import mlcrate as mlc\n",
    "    import os\n",
    "    pool = mlc.SuperPool(64)\n",
    "    pool.pool.restart()\n",
    "    _ = pool.map(batch_run,input_)\n",
    "    pool.exit()\n",
    "\n",
    "\n",
    "    import torch\n",
    "    protein_dict = {}\n",
    "    for pdb in tqdm(name_list):\n",
    "        protein_dict.update(torch.load(f\"{protein_embedding_folder}/{pdb}.pt\"))\n",
    "        \n",
    "        \n",
    "    torch.save(protein_dict, \"/home/jovyan/dataspace/Prototype/dicts/FULL_protein_dict.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 「折叠」读取已保存的 `protein_dict` 和 `pockets_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    tankbind_data_path = f\"/home/jovyan/dataspace/Prototype/tankbind_data\"\n",
    "    protein_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/FULL_protein_dict.pt\")\n",
    "    pockets_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/FULL_pockets_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 「折叠」小分子：特征构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/113099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `Na`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `Li`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smiles_5544 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `K`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `Ag`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `As`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n",
      "/opt/conda/envs/tankbind_py38/lib/python3.8/site-packages/torchdrug/data/feature.py:37: UserWarning: Unknown value `Te`\n",
      "  warnings.warn(\"Unknown value `%s`\" % x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smiles_39785 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_39786 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_39787 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_39788 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_40760 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_40762 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_40763 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_40764 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_40766 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
      "smiles_43789 max(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n"
     ]
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    from feature_utils import extract_torchdrug_feature_from_mol\n",
    "    from feature_utils import read_mol\n",
    "\n",
    "    compound_dict = {}\n",
    "    skip_smiles_list = []\n",
    "    for smiles_name in tqdm(multiton_table.kekule_smiles_name.unique()):\n",
    "        mol, _ = read_mol(f\"/home/jovyan/dataspace/Prototype/rdkit_generated_sdfs/{smiles_name}.sdf\", None)\n",
    "        # extract features from sdf.\n",
    "        try:\n",
    "            compound_dict[smiles_name] = extract_torchdrug_feature_from_mol(mol, has_LAS_mask=True)  # self-dock set has_LAS_mask to true\n",
    "        except Exception as e:\n",
    "            print(smiles_name, e)\n",
    "            skip_smiles_list.append(smiles_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "skip_smiles_list = ['smiles_5544', 'smiles_39785', 'smiles_39786', 'smiles_39787', 'smiles_39788', 'smiles_40760', 'smiles_40762', 'smiles_40763', 'smiles_40764', 'smiles_40766', 'smiles_43789']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    torch.save(compound_dict, f\"/home/jovyan/dataspace/Prototype/dicts/FULL_compound_torchdrug_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    multiton_table = multiton_table[~multiton_table.kekule_smiles_name.isin(skip_smiles_list)]\n",
    "    multiton_table.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_multiton_table_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90545/3151936896.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  multiton_table = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/multiton_table_v2.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    multiton_table = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_multiton_table_v2.csv\", index_col=0)\n",
    "    multiton_table['session_aus'] = multiton_table['session_au'] + \"_\" + multiton_table['kekule_smiles']\n",
    "    data = multiton_table.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表格处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入分组文件以进行 `train-ood-iid-val` 分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    drugood_split = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/non_drugood_split2.csv\")\n",
    "    drugood_split=drugood_split.fillna(\"-\")\n",
    "    drugood_split.split_tag = drugood_split.split_tag.apply(lambda x: \"train\" if x==\"-\" else x)\n",
    "    drugood_split[\"session_aus\"] = drugood_split['assay_id'].astype(str) + \"_\" + drugood_split['uniprot_id'] + \"_\" + drugood_split['kekule_smiles']\n",
    "    drugood_split=drugood_split[['session_aus', 'split_tag']]\n",
    "    \n",
    "    data = pd.merge(left=multiton_table, right=drugood_split, how=\"left\", on=\"session_aus\")\n",
    "    data.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 `raw_info.csv` 为 `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    data = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理`data`生成 `info` 并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/695316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5886176\n"
     ]
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    info = []\n",
    "    err_pdb_list=[]\n",
    "    for i, line in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        \n",
    "        sample_id = line['sample_id']\n",
    "        pdb_id = line['pdb_id']\n",
    "        uniprot_id = line['uniprot_id']\n",
    "        assay_id = line['assay_id']\n",
    "        smiles_name = line['kekule_smiles_name']\n",
    "        smiles = line['kekule_smiles']\n",
    "        session_au = line['session_au']\n",
    "        session_aus = line['session_aus']\n",
    "        \n",
    "        value = line['value']\n",
    "        docking_score = line['docking_score']\n",
    "        mmgbsa_binding_energy = line['mmgbsa_binding_energy']\n",
    "        qed = line[\"qed\"]\n",
    "        \n",
    "        split_tag = line['split_tag']\n",
    "        \n",
    "        protein_name = pdb_id.lower()\n",
    "        try:\n",
    "            pocket = pockets_dict[protein_name].head(20)\n",
    "        except:\n",
    "            err_pdb_list.append(pdb)\n",
    "            continue\n",
    "        pocket.columns = pocket.columns.str.strip()\n",
    "        pocket_coms = pocket[['center_x', 'center_y', 'center_z']].values\n",
    "\n",
    "        # protein center as a block.\n",
    "        protein_com = protein_dict[protein_name][0].numpy().mean(axis=0).astype(float).reshape(1, 3)\n",
    "        info.append([sample_id, pdb_id, uniprot_id, assay_id, smiles_name, smiles, session_au, session_aus, protein_com, protein_name+\"_c\", \n",
    "                     value, docking_score, mmgbsa_binding_energy, qed, split_tag])\n",
    "        \n",
    "        for idx, pocket_line in pocket.iterrows():\n",
    "            pdb_idx = f\"{protein_name}_{idx}\"\n",
    "            info.append([sample_id, pdb_id, uniprot_id, assay_id, smiles_name, smiles, session_au, session_aus, pocket_coms[idx].reshape(1, 3), pdb_idx, \n",
    "                         value, docking_score, mmgbsa_binding_energy, qed, split_tag])\n",
    "            \n",
    "    info = pd.DataFrame(\n",
    "        info, \n",
    "        columns=['sample_id', 'pdb_id', 'uniprot_id', 'assay_id', 'smiles_name', 'kekule_smiles', 'session_au', 'session_aus', 'pocket_com', 'pocket_name', \n",
    "                 'value', 'docking_score','mmgbsa_binding_energy', 'qed', 'split_tag'])\n",
    "    print(len(info))\n",
    "\n",
    "\n",
    "    len(err_pdb_list)\n",
    "    # 0\n",
    "\n",
    "\n",
    "    info.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info_all_pockets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 `info_all_pockets` 为 `info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    info = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info_all_pockets.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用此前生成的 `ligand_coor_dict` 来指定口袋，生成 `pdb_id_to_pocket_name_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    ligand_coor_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/ARC_ligand_coor_dict.pt\")\n",
    "    \n",
    "    \n",
    "    pdb_id_to_pocket_name_dict ={}\n",
    "    error_pdb_id_list = []\n",
    "    for PDB_id in tqdm(ligand_coor_dict, total=len(ligand_coor_dict)):\n",
    "        try:\n",
    "            pdb_id = PDB_id.lower()\n",
    "            ligand_center = ligand_coor_dict[PDB_id][0]\n",
    "            p2rankFile = f\"/home/jovyan/dataspace/Prototype/p2rank_results/{pdb_id}_protein.pdb_predictions.csv\"\n",
    "            pocket = pd.read_csv(p2rankFile)\n",
    "            pocket.columns = pocket.columns.str.strip()\n",
    "            pocket_coms = pocket[['center_x', 'center_y', 'center_z']].values\n",
    "            pocket_coms_with_center = np.concatenate([pocket_coms.mean(axis=0).reshape(1,3), pocket_coms])\n",
    "            pocket_names = [pdb_id + \"_c\"] + [pdb_id+\"_\"+str(i) for i in range(len(pocket_coms))]\n",
    "            right_ith = ((pocket_coms_with_center-ligand_center)**2).sum(axis=1).argmin()\n",
    "            right_name = pocket_names[right_ith]\n",
    "            pdb_id_to_pocket_name_dict[PDB_id] = right_name\n",
    "        except:\n",
    "            print(PDB_id, end = \" \")\n",
    "            error_pdb_id_list.append(PDB_id)\n",
    "            pdb_id_to_pocket_name_dict[PDB_id] = \"ERROR\"\n",
    "            \n",
    "    torch.save(pdb_id_to_pocket_name_dict, \"/home/jovyan/dataspace/Prototype/dicts/FULL_pdb_id_to_pocket_name_dict.pt\")\n",
    "    torch.save(error_pdb_id_list, \"/home/jovyan/dataspace/Prototype/dicts/FULL_pdb_id_to_pocket_name_error_list.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取`pdb_id_to_pocket_name_dict`，并结合 `info_all_pockets` 构造出仅保留单个 `pocket` 的 `info_core_pocket`。\n",
    "错误的蛋白因为已标注为 `ERROR`，会因为没有名为 `ERROR` 的口袋而被略去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    pdb_id_to_pocket_name_dict = torch.load(\"/home/jovyan/dataspace/Prototype/pdb_id_to_pocket_name_dict.pt\")\n",
    "    info['p2rank_core_pocket'] = info.apply(lambda x: pdb_id_to_pocket_name_dict[x.pdb_id] == x.pocket_name, axis=1)\n",
    "    \n",
    "    info.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info_all_pockets_noted.csv\")\n",
    "    \n",
    "    info = info[info.p2rank_core_pocket==True] \n",
    "    info.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULLRAW_info_core_pocket.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取 `info_core_pocket`，该表格被用于构建最终数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    info = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/info_core_pocket.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对 `val / test` 组的数据，额外提供一个根据 docking_score 预先指定口袋的 info_val_test_best_docking_score\n",
    "\n",
    "这一步在后续构建两个数据集时也进行了，所以可以不管这一格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    from pandas import DataFrame\n",
    "    info_val_test = info[info.split_tag!=\"train\"]\n",
    "    info_val_test = info_val_test.sort_values('docking_score').groupby('session_aus').apply(DataFrame.head, n=1).reset_index(drop=True)\n",
    "    info_val_test.to_csv(\"/home/jovyan/dataspace/Prototype/tables/info_extra_for_val_and_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对 info 和 info_val_test 进行额外的 session_au_usability_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    info_count = info[['session_au', 'kekule_smiles', 'pdb_id']].groupby('session_au').nunique()\n",
    "    info_count.reset_index(inplace=True)\n",
    "    info_singleton = list(info_count[info_count.kekule_smiles==1].session_au)\n",
    "    info_multiton = list(info_count[info_count.kekule_smiles!=1].session_au)\n",
    "    ms_dict ={_:\"singleton\" for _ in info_singleton}\n",
    "    ms_dict.update({_:\"multiton\" for _ in info_multiton})\n",
    "    info['session_au_usability'] = info.session_au.apply(lambda x: ms_dict[x])\n",
    "    info = info[info.session_au_usability==\"multiton\"]\n",
    "    info.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULL_info_core_pocket_multiton.csv\")\n",
    "    info.to_pickle(\"/home/jovyan/dataspace/Prototype/tables/FULL_info_core_pocket_multiton.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = True\n",
    "if not processed:\n",
    "    info_count = info_val_test[['session_au', 'kekule_smiles', 'pdb_id']].groupby('session_au').nunique()\n",
    "    info_count.reset_index(inplace=True)\n",
    "    info_singleton = list(info_count[info_count.kekule_smiles==1].session_au)\n",
    "    info_multiton = list(info_count[info_count.kekule_smiles!=1].session_au)\n",
    "    ms_dict ={_:\"singleton\" for _ in info_singleton}\n",
    "    ms_dict.update({_:\"multiton\" for _ in info_multiton})\n",
    "    info_val_test['session_au_usability'] = info_val_test.session_au.apply(lambda x: ms_dict[x])\n",
    "    info_val_test = info_val_test[info_val_test.session_au_usability==\"multiton\"]\n",
    "    info_val_test.to_csv(\"/home/jovyan/dataspace/Prototype/tables/info_extra_for_val_and_test_multiton.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='gold'>「从这里开始」</font>数据集构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import torch\n",
    "pdbbind_path = \"/home/jovyan/dataspace/PDBbind2020/all_pdbbind\"\n",
    "proto_path = \"/home/jovyan/dataspace/Prototype\"\n",
    "p2rank = \"bash /home/jovyan/TankBind/p2rank_2.3/prank\"\n",
    "tankbind_src_folder_path = \"../tankbind_prototype/\"\n",
    "import sys\n",
    "sys.path.insert(0, tankbind_src_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tankbind_data_path = f\"/home/jovyan/dataspace/Prototype/tankbind_data\"\n",
    "protein_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/FULL_protein_dict.pt\")\n",
    "compound_dict = torch.load(\"/home/jovyan/dataspace/Prototype/dicts/FULL_compound_torchdrug_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取 `info` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_pickle(\"/home/jovyan/dataspace/Prototype/tables/FULL_info_core_pocket_multiton.pkl\")\n",
    "#session type 使用 同assay同pdbid 或者  同assay同uniprot\n",
    "info[\"session_ap\"] = info['assay_id'].astype(str) + \"_\" + info['pdb_id']\n",
    "info[\"session_aps\"]=info[\"session_ap\"]+\"_\"+info[\"kekule_smiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/main_tankbind/dataset_prototype/full/dataset/processed/data.pt', '/home/jovyan/main_tankbind/dataset_prototype/full/dataset/processed/protein.pt', '/home/jovyan/main_tankbind/dataset_prototype/full/dataset/processed/compound.pt']\n"
     ]
    }
   ],
   "source": [
    "from data_prototype import TankBindDataSet_prototype\n",
    "\n",
    "toFileFull = f\"/home/jovyan/main_tankbind/dataset_prototype/full/dataset\"\n",
    "os.system(f\"rm -rf {toFileFull}\")\n",
    "os.system(f\"mkdir -p {toFileFull}\")\n",
    "dataset = TankBindDataSet_prototype(toFileFull, data=info, protein_dict=protein_dict, compound_dict=compound_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/691468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = []\n",
    "t_dict = {}\n",
    "data = dataset.data\n",
    "\n",
    "for i, line in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    d = dataset[i]\n",
    "    sample_id = line['sample_id']\n",
    "    p_length = d['node_xyz'].shape[0]\n",
    "    c_length = d['coords'].shape[0]\n",
    "    y_length = d['y'].shape[0]\n",
    "    t.append([i, sample_id, p_length, c_length, y_length])\n",
    "    t_dict[sample_id] = [i, sample_id, p_length, c_length, y_length]\n",
    "    \n",
    "torch.save(t, \"/home/jovyan/dataspace/Prototype/tables/FULL_supplementary.pt\")\n",
    "torch.save(t_dict, \"/home/jovyan/dataspace/Prototype/dicts/FULL_supplementary_dict.pt\")\n",
    "\n",
    "t = pd.DataFrame(t, columns=['index', 'sample_id', 'p_length', 'c_length', 'y_length'])\n",
    "t.to_csv(\"/home/jovyan/dataspace/Prototype/tables/FULL_supplementary.csv\")\n",
    "\n",
    "data = pd.concat([data, t[['p_length', 'c_length', 'y_length']]], axis=1)\n",
    "torch.save(data, f\"{toFileFull}/processed/data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.data\n",
    "t = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULL_t.csv\", index_col=0)\n",
    "data =pd.concat([data, t[['p_length', 'c_length', 'y_length']]], axis=1)\n",
    "torch.save(data, f\"{toFileFull}/processed/data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(\"/home/jovyan/dataspace/Prototype/tables/FULL_info_core_pocket_multiton.csv\", index_col=0)\n",
    "#session type 使用 同assay同pdbid 或者  同assay同uniprot\n",
    "info[\"session_ap\"] = info['assay_id'].astype(str) + \"_\" + info['pdb_id']\n",
    "info[\"session_aps\"]=info[\"session_ap\"]+\"_\"+info[\"kekule_smiles\"]\n",
    "info.reset_index(inplace=True, drop=True)\n",
    "info.pocket_com = info.pocket_com.apply(lambda x: np.array([float(_) for _ in x.replace(\"[[\", \"\").replace(\"]]\", \"\").split()]))\n",
    "info2 = info[info.split_tag.isin([\"test\", 'iid_val', 'ood_val'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同 `aus` 样本，不同 `pdb` 中抽取 `docking_score` 最低的一条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "info3 = info2.sort_values(by='docking_score', ascending=True).groupby('session_aus', as_index=False).first()\n",
    "info3.head(5)\n",
    "info3.to_pickle(\"/home/jovyan/dataspace/Prototype/tables/EXTRA_info_sole_pdb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_pdbs = [_.lower() for _ in info2.pdb_id.unique()]\n",
    "small_smiles = list(info2.smiles_name.unique())\n",
    "\n",
    "protein_dict2 = {_:protein_dict[_] for _ in small_pdbs}\n",
    "compound_dict2 = {_:compound_dict[_] for _ in small_smiles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/main_tankbind/dataset_prototype/extra_val_test_reduced_0130/processed/data.pt', '/home/jovyan/main_tankbind/dataset_prototype/extra_val_test_reduced_0130/processed/protein.pt', '/home/jovyan/main_tankbind/dataset_prototype/extra_val_test_reduced_0130/processed/compound.pt']\n"
     ]
    }
   ],
   "source": [
    "toFileExtra = f\"/home/jovyan/main_tankbind/dataset_prototype/extra_val_test_reduced_0130\"\n",
    "os.system(f\"rm -rf {toFileExtra}\")\n",
    "os.system(f\"mkdir -p {toFileExtra}\")\n",
    "\n",
    "from data_prototype import TankBindDataSet_prototype\n",
    "dataset = TankBindDataSet_prototype(toFileExtra, data=info3, protein_dict=protein_dict2, compound_dict=compound_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/32631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = []\n",
    "t_dict = {}\n",
    "data = dataset.data\n",
    "\n",
    "for i, line in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    d = dataset[i]\n",
    "    sample_id = line['sample_id']\n",
    "    p_length = d['node_xyz'].shape[0]\n",
    "    c_length = d['coords'].shape[0]\n",
    "    y_length = d['y'].shape[0]\n",
    "    t.append([i, sample_id, p_length, c_length, y_length])\n",
    "    t_dict[sample_id] = [i, sample_id, p_length, c_length, y_length]\n",
    "    \n",
    "torch.save(t, \"/home/jovyan/dataspace/Prototype/tables/EXTRA_supplementary.pt\")\n",
    "torch.save(t_dict, \"/home/jovyan/dataspace/Prototype/dicts/EXTRA_supplementary_dict.pt\")\n",
    "\n",
    "t = pd.DataFrame(t, columns=['index', 'sample_id', 'p_length', 'c_length', 'y_length'])\n",
    "t.to_csv(\"/home/jovyan/dataspace/Prototype/tables/EXTRA_supplementary.csv\")\n",
    "\n",
    "data = pd.concat([data, t[['p_length', 'c_length', 'y_length']]], axis=1)\n",
    "torch.save(data, f\"{toFileExtra}/processed/data.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3642610297c2db0c2e515b5d6934e6fa60bcae4d2e973dd2c51cba19538092f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
